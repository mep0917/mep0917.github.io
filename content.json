{"meta":{"title":"Hexo","subtitle":null,"description":null,"author":"John Doe","url":"http://yoursite.com","root":"/"},"pages":[{"title":"about","date":"2019-09-09T16:04:59.000Z","updated":"2019-10-23T02:17:47.360Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-09-09T16:05:51.000Z","updated":"2019-09-09T16:05:51.140Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"再见了!  同程","slug":"My-blog-83","date":"2019-09-15T15:15:19.000Z","updated":"2019-10-23T01:16:05.994Z","comments":true,"path":"2019/09/15/My-blog-83/","link":"","permalink":"http://yoursite.com/2019/09/15/My-blog-83/","excerpt":"","text":"现在是晚上已经十一点了,诺大的办公室现在就剩我自己了,突然写这篇文章是因为,自己前两天提交了离职单,下个月就离开了。其实自己也算是个多愁善感的人,一个人独处总是容易想很多事情,世界本来就是变化的,谁也不知道明天会发生什么变故,哈哈,我的文采也就这样了,煽不了情,还有最后一个月,努力站好最后一班岗,这个博客当然不会断的,最后附一张丑图,不好意思丑脸挡住了 哈哈 下班! ![](C:\\Users\\lenovo\\Pictures\\Camera Roll\\微信图片_20191023091537.jpg)","categories":[],"tags":[]},{"title":"关于海量存储系统Fastdfs","slug":"My-blog-82","date":"2019-09-11T13:39:36.000Z","updated":"2019-10-22T13:39:03.933Z","comments":true,"path":"2019/09/11/My-blog-82/","link":"","permalink":"http://yoursite.com/2019/09/11/My-blog-82/","excerpt":"","text":"FastDFS是一个开源的分布式文件系统，它对文件进行管理，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等。可以说它就是为互联网而生，为大数据而生的。 FastDFS服务端有两个角色：跟踪器（tracker）和存储节点（storage）。跟踪器主要做调度工作，在访问上起负载均衡的作用。 存储节点存储文件，完成文件管理的所有功能：存储、同步和提供存取接口，FastDFS同时对文件的meta data进行管理。跟踪器和存储节点都可以由多台服务器构成。跟踪器和存储节点中的服务器均可以随时增加或下线而不会影响线上服务。其中跟踪器中的所有服务器都是对等的，可以根据服务器的压力情况随时增加或减少。 1 解决海量存储，同时存储容量扩展方便。 2 解决文件内容重复,如果用户上传的文件重复(文件指纹一样)，那么系统只有存储一份数据，值得一提的是，这项技术目前被广泛应用在网盘中。 3 结合Nginx提高网站读取图片的效率。 其排重原理为： FastDFS的storage server每次上传均计算文件的hash值，然后从FastDHT服务器上进行查找比对，如果没有返回，则写入hash，并将文件保存；如果有返回，则建立一个新的文件链接（软链），不保存文件。","categories":[],"tags":[]},{"title":"很久之前就想写的docker  来了","slug":"My-blog-81","date":"2019-09-05T11:34:27.000Z","updated":"2019-10-22T13:35:23.938Z","comments":true,"path":"2019/09/05/My-blog-81/","link":"","permalink":"http://yoursite.com/2019/09/05/My-blog-81/","excerpt":"","text":"Docker 是一个开源的应用容器引擎，让开发者可以打包他们的应用以及依赖包到一个可移植的容器中，然后发布到任何流行的 Linux 机器上，也可以实现虚拟化。容器是完全使用沙箱机制，相互之间不会有任何接口。 Docker与虚拟机比较作为一种轻量级的虚拟化方式，Docker在运行应用上跟传统的虚拟机方式相比具有显著优势： Docker容器很快，启动和停止可以在秒级实现，这相比传统的虚拟机方式要快得多。 Docker容器对系统资源需求很少，一台主机上可以同时运行数千个Docker容器。 Docker通过类似Git的操作来方便用户获取、分发和更新应用镜像，指令简明，学习成本较低。 Docker通过Dockerfile配置文件来支持灵活的自动化创建和部署机制，提高工作效率。 Docker构架Docker使用C/S架构，Client 通过接口与Server进程通信实现容器的构建，运行和发布。client和server可以运行在同一台集群，也可以通过跨主机实现远程通信。 Docker优势更高效的利用系统资源 由于容器不需要进行硬件虚拟以及运行完整操作系统等额外开销，Docker 对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。 更快速的启动时间 传统的虚拟机技术启动应用服务往往需要数分钟，而 Docker 容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间。大大的节约了开发、测试、部署的时间。 一致的运行环境 开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些 bug 并未在开发过程中被发现。而 Docker 的镜像提供了除内核外完整的运行时环境，确保了应用运行环境一致性，从而不会再出现 「这段代码在我机器上没问题啊」 这类问题。 持续交付和部署 对开发和运维（DevOps）人员来说，最希望的就是一次创建或配置，可以在任意地方正常运行。 使用 Docker 可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过 Dockerfile 来进行镜像构建，并结合 持续集成(Continuous Integration) 系统进行集成测试，而运维人员则可以直接在生产环境中快速部署该镜像，甚至结合 持续部署(Continuous Delivery/Deployment) 系统进行自动部署。 而且使用 Dockerfile 使镜像构建透明化，不仅仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好的生产环境中部署该镜像。 更轻松的迁移 由于 Docker 确保了执行环境的一致性，使得应用的迁移更加容易。Docker 可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，甚至是笔记本，其运行结果是一致的。因此用户可以很轻易的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。 更轻松的维护和扩展 Docker 使用的分层存储以及镜像的技术，使得应用重复部分的复用更为容易，也使得应用的维护更新更加简单，基于基础镜像进一步扩展镜像也变得非常简单。此外，Docker 团队同各个开源项目团队一起维护了一大批高质量的 官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。 Docker核心概念镜像(image) Docker 镜像（Image）就是一个只读的模板。例如：一个镜像可以包含一个完整的操作系统环境，里面仅安装了 Apache 或用户需要的其它应用程序。镜像可以用来创建 Docker 容器，一个镜像可以创建很多容器。Docker 提供了一个很简单的机制来创建镜像或者更新现有的镜像，用户甚至可以直接从其他人那里下载一个已经做好的镜像来直接使用。 仓库(repository) 仓库（Repository）是集中存放镜像文件的场所。有时候会把仓库和仓库注册服务器（Registry）混为一谈，并不严格区分。实际上，仓库注册服务器上往往存放着多个仓库，每个仓库中又包含了多个镜像，每个镜像有不同的标签（tag）。 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。最大的公开仓库是 Docker Hub，存放了数量庞大的镜像供用户下载。国内的公开仓库包括 时速云 、网易云 等，可以提供大陆用户更稳定快速的访问。当然，用户也可以在本地网络内创建一个私有仓库。 当用户创建了自己的镜像之后就可以使用 push 命令将它上传到公有或者私有仓库，这样下次在另外一台机器上使用这个镜像时候，只需要从仓库上 pull 下来就可以了。 Docker 仓库的概念跟 Git 类似，注册服务器可以理解为 GitHub 这样的托管服务。 容器(container) Docker 利用容器（Container）来运行应用。容器是从镜像创建的运行实例。它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台。可以把容器看做是一个简易版的 Linux 环境（包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。 容器的定义和镜像几乎一模一样，也是一堆层的统一视角，唯一区别在于容器的最上面那一层是可读可写的。 Docker隔离性Docker容器本质上是宿主机上的进程。Docker 通过namespace实现了资源隔离，通过cgroups实现资源限制，通过写时复制机制(Copy-on-write)实现了高效的文件操作。 Linux内核实现namespace的主要目的之一就是实现轻量级虚拟化(容器)服务。在同一个namespace下的进程可以感知彼此的变化，而对外界的进程一无所知。这样就可以让容器中的进程产生错觉，仿佛自己置身于一个独立的系统环境中，以达到独立和隔离的目的。 一般,Docker容器需要并且Linux内核也提供了这6种资源的namespace的隔离： UTS : 主机与域名 IPC : 信号量、消息队列和共享内存 PID : 进程编号 NETWORK : 网络设备、网络栈、端口等 Mount : 挂载点(文件系统) User : 用户和用户组 Pid namespace 用户进程是lxc-start进程的子进程，不同用户的进程就是通过pid namespace隔离开的，且不同namespace中可以有相同PID,具有以下特征： 1、每个namespace中的pid是有自己的pid=1的进程(类似/sbin/init进程） 2、每个namespace中的进程只能影响自己的同一个namespace或子namespace中的进程 3、因为/proc包含正在运行的进程，因此而container中的pseudo-filesystem的/proc目录只能看到自己namespace中的进程 4、因为namespace允许嵌套，父进程可以影响子namespace进程，所以子namespace的进程可以在父namespace中看到，但是具有不同的pid Net namespace 有了pid namespace ,每个namespace中的pid能够相互隔离，但是网络端口还是共享host的端口。网络隔离是通过netnamespace实现的， 每个net namespace有独立的network devices, IP address, IP routing tables, /proc/net目录，这样每个container的网络就能隔离开来， LXC在此基础上有5种网络类型，docker默认采用veth的方式将container中的虚拟网卡同host上的一个docker bridge连接在一起 IPC namespace Container中进程交互还是采用linux常见的进程间交互方法(interprocess communication-IPC)包括常见的信号量、消息队列、内存共享。然而同VM不同， container的进程交互实际是host具有相同pid namespace中的进程间交互，因此需要在IPC资源申请时加入namespace信息，每个IPC资源有一个唯一的32bit ID Mnt namespace 类似chroot ,将一个进程放到一个特定的目录执行，mnt namespace允许不同namespace的进程看到的文件结构不同，这样每个namespace中的进程所看到的文件目录被隔离开了，同chroot不同，每个namespace中的container在/proc/mounts的信息只包含所在namespace 的mount point Uts namespace UTS(UNIX Time sharing System）namespace允许每个container拥有独立地hostname和domain name,使其在网络上被视作一个独立的节点而非Host上的一个进程 User namespace 每个container可以有不同的user和group id ,也就是说可以container内部的用户在container内部执行程序而非 Host上的用户","categories":[],"tags":[]},{"title":"Vue框架为何引人热爱  今天来说说","slug":"My-blog-80","date":"2019-09-02T14:30:44.000Z","updated":"2019-10-22T13:32:57.563Z","comments":true,"path":"2019/09/02/My-blog-80/","link":"","permalink":"http://yoursite.com/2019/09/02/My-blog-80/","excerpt":"","text":"前后端只通过 JSON 来交流，组件化、工程化不需要依赖后端去实现。 可以通过Vue.js来实现组件化工程化；有哪些好处或弊端？现在的发展趋势是否往这个方面发展 前端优化技术 cdn加速 网站上静态资源即css、js全都使用cdn分发，图片亦然。具体来说，CDN就是采用更多的缓存服务器（CDN边缘节点），布放在用户访问相对集中的地区或网络中。当用户访问网站时，利用全局负载技术，将用户的访问指向距离最近的缓存服务器上，由缓存服务器响应用户请求 2.使用Gzip压缩网页 3 减少 HTTP请求数，如果可以的话，尽可能的将外部的脚本、样式进行合并，多个合为一个。另外， CSS、 Javascript、Image 都可以用相应的工具进行压缩，压缩后往往能省下不少空间，如何压缩以及合并外部脚本和样式请参照这篇文章 利用grunt插件来压缩js和css文件用来减少http请求，提高页面效率 4 避免空的src和href 当link标签的href属性为空、script标签的src属性为空的时候，浏览器渲染的时候会把当前页面的URL作为它们的属性值，从而把页面的内容加载进来作为它们的值。所以要避免犯这样的疏忽。 5 把CSS放到顶部 网页上的资源加载时从上网下顺序加载的，所以css放在页面的顶部能够优先渲染页面，让用户感觉页面加载很快。 6 把JS放到底部 加载js时会对后续的资源造成阻塞，必须得等js加载完才去加载后续的文件 ，所以就把js放在页面底部最后加载。 7 可缓存的AJAX 异步请求同样的造成用户等待，所以使用ajax请求时，要主动告诉浏览器如果该请求有缓存就去请求缓存内容。如下代码片段, cache:true就是显式的要求如果当前请求有缓存的话，直接使用缓存 8 减少作用域链查找，这一点在循环中是尤其需要注意的问题。如果在循环中需要访问非本作用域下的变量时请在遍历之前用局部变量缓存该变量，并在遍历结束后再重写那个变量，这一点对全局变量尤其重要，因为全局变量处于作用域链的最顶端，访问时的查找次数是最多的。 9 生成纯静态页，也就是把动态内容事先生成好，这样在前端就避免请求后端数据，加快了页面访问速度 使用场景 利弊切记一定要举例子,如下：前后端要不要分，怎么分，是由具体业务决定的。 需要搜索引擎带流量的，必须由服务器端渲染。 需要用户登录且不能由搜索引擎抓取，前后端分离是鼓励的。 需要App和后端交互，必须分离。 但是分了就表示架构合理？不一定。设计一套合理／可升级／客户端友好的API也不容易。 要想做好前后端分离，前端开发要了解后端架构，后端开发要虚心学习前端技术，双方如果互相鄙视，分了也白搭 为什么选择Vue.jsMVVM 是Model-View-ViewModel 的缩写，它是一种基于前端开发的架构模式，其核心是提供对View 和 ViewModel 的双向数据绑定，这使得ViewModel 的状态改变可以自动传递给 View，即所谓的数据双向绑定。 Vue.js 是一个提供了 MVVM 风格的双向数据绑定的 Javascript 库，专注于View 层。它的核心是 MVVM 中的 VM，也就是 ViewModel。 ViewModel负责连接 View 和 Model，保证视图和数据的一致性，这种轻量级的架构让前端开发更加高效、便捷。 强调前端为什么要用vue.js 为什么要用工程化相对 HTML4 , HTML5 最大的亮点是它为移动设备提供了一些非常有用的功能，使得 HTML5 具备了开发App的能力, HTML5开发App 最大的好处就是跨平台、快速迭代和上线，节省人力成本和提高效率，因此很多企业开始对传统的App进行改造，逐渐用H5代替Native，到2015年的时候，市面上大多数App 或多或少嵌入都了H5 的页面。 Vue.js 和 jquery的区别 强调没有最好的，只有最适合的jQuery是使用选择器（$）选取DOM对象，对其进行赋值、取值、事件绑定等操作，其实和原生的HTML的区别只在于可以更方便的选取和操作DOM对象，而数据和界面是在一起的。比如需要获取label标签的内容：$(“lable”).val();,它还是依赖DOM元素的值。 Vue则是通过Vue对象将数据和View完全分离开来了。对数据进行操作不再需要引用相应的DOM对象，可以说数据和View是分离的，他们通过Vue对象这个vm实现相互的绑定。这就是传说中的MVVM。 混合式开发近几年，混合模式移动应用的概念甚嚣尘上，受到了一些中小型企业的青睐，究其原因，混合模式开发可以比传统移动开发节约大量的开发成本和人力成本。 Hybrid App（混合模式移动应用）是指介于web-app、native-app这两者之间的app，兼具“Native App良好用户交互体验的优势”和“Web App跨平台开发的优势”。 说白了，如果走传统移动开发路线，公司业务覆盖多端，那么每个平台势必要请一个专属开发人员，安卓要请一个前端开发，ios同理，那么人力成本则进行了翻倍，同时，如果多端使用不同的代码，当有功能上的修改或者维护时，成本也是不可想象的。试想如果开发者编写一套代码，可编译到iOS、Android、H5、小程序等多个平台，这绝对是省时省力的良好方案。 具体操作：https://v3u.cn/a_id_82 mvvm 和 mvc 区别？mvc 和 mvvm 其实区别并不大。都是一种设计思想。主要就是 mvc 中 Controller 演变成 mvvm 中的 viewModel。mvvm 主要解决了 mvc 中大量的 DOM 操作使页面渲染性能降低，加载速度变慢，影响用户体验。和当 Model 频繁发生变化，开发者需要主动更新到 View 。 vue 的优点是什么？低耦合。视图（View）可以独立于 Model 变化和修改，一个 ViewModel 可以绑定到不同的”View”上，当 View 变化的时候 Model 可以不变，当 Model 变化的时候 View 也可以不变。 可重用性。你可以把一些视图逻辑放在一个 ViewModel 里面，让很多 view 重用这段视图逻辑。 独立开发。开发人员可以专注于业务逻辑和数据的开发（ViewModel），设计人员可以专注于页面设计，使用 Expression Blend 可以很容易设计界面并生成 xml 代码。 可测试。界面素来是比较难于测试的，而现在测试可以针对 ViewModel 来写。 vue生命周期的理解？总共分为 8 个阶段创建前/后，载入前/后，更新前/后，销毁前/后。 创建前/后： 在 beforeCreate 阶段，vue 实例的挂载元素 el 还没有。 载入前/后：在 beforeMount 阶段，vue 实例的$el 和 data 都初始化了，但还是挂载之前为虚拟的 dom 节点，data.message 还未替换。在 mounted 阶段，vue 实例挂载完成，data.message 成功渲染。 更新前/后：当 data 变化时，会触发 beforeUpdate 和 updated 方法。 销毁前/后：在执行 destroy 方法后，对 data 的改变不会再触发周期函数，说明此时 vue 实例已经解除了事件监听以及和 dom 的绑定，但是 dom 结构依然存在 组件之间的传值？12345678910111213141516171819202122232425262728293031323334//父组件通过标签上面定义传值&lt;template&gt; &lt;Main :obj=&quot;data&quot;&gt;&lt;/Main&gt;&lt;/template&gt;&lt;script&gt; //引入子组件 import Main form &quot;./main&quot; exprot default&#123; name:&quot;parent&quot;, data()&#123; return &#123; data:&quot;我要向子组件传递数据&quot; &#125; &#125;, //初始化组件 components:&#123; Main &#125; &#125;&lt;/script&gt;//子组件通过props方法接受数据&lt;template&gt; &lt;div&gt;&#123;&#123;data&#125;&#125;&lt;/div&gt;&lt;/template&gt;&lt;script&gt; exprot default&#123; name:&quot;son&quot;, //接受父组件传值 props:[&quot;obj&quot;] &#125;&lt;/script&gt; 路由之间跳转？声明式（标签跳转）router-link 编程式（ js 跳转） router.push(‘index’) 懒加载（按需加载路由）webpack 中提供了 require.ensure()来实现按需加载。以前引入路由是通过 import 这样的方式引入，改为 const 定义的方式进行引入。 不进行页面按需加载引入方式： import home from ‘../../common/home.vue’ 进行页面按需加载的引入方式： const home = r =&gt; require.ensure( [], () =&gt; r (require(‘../../common/home.vue’))) vuex 是什么？怎么使用？哪种功能场景使用它？vue 框架中状态管理。在 main.js 引入 store，注入。新建了一个目录 store，….. export 。场景有：单页应用中，组件之间的状态。音乐播放、登录状态、加入购物车 1234567891011// 新建 store.jsimport vue from &apos;vue&apos;import vuex form &apos;vuex&apos;vue.use(vuex)export default new vuex.store(&#123; //...code&#125;)//main.jsimport store from &apos;./store&apos;... vue 的双向绑定的原理是什么vue.js 是采用数据劫持结合发布者-订阅者模式的方式，通过 Object.defineProperty()来劫持各个属性的 setter，getter，在数据变动时发布消息给订阅者，触发相应的监听回调。 具体步骤： 第一步：需要 observe 的数据对象进行递归遍历，包括子属性对象的属性，都加上 setter 和 getter 这样的话，给这个对象的某个值赋值，就会触发 setter，那么就能监听到了数据变化 第二步：compile 解析模板指令，将模板中的变量替换成数据，然后初始化渲染页面视图，并将每个指令对应的节点绑定更新函数，添加监听数据的订阅者，一旦数据有变动，收到通知，更新视图 第三步：Watcher 订阅者是 Observer 和 Compile 之间通信的桥梁，主要做的事情是: 在自身实例化时往属性订阅器(dep)里面添加自己 自身必须有一个 update()方法 待属性变动 dep.notice()通知时，能调用自身的 update() 方法，并触发 Compile 中绑定的回调，则功成身退。 第四步：MVVM 作为数据绑定的入口，整合 Observer、Compile 和 Watcher 三者，通过 Observer 来监听自己的 model 数据变化，通过 Compile 来解析编译模板指令，最终利用 Watcher 搭起 Observer 和 Compile 之间的通信桥梁，达到数据变化 -&gt; 视图更新；视图交互变化(input) -&gt; 数据 model 变更的双向绑定效果。","categories":[],"tags":[]},{"title":"详解jwt以及实际应用","slug":"My-blog-79","date":"2019-08-29T13:26:01.000Z","updated":"2019-10-22T13:30:35.035Z","comments":true,"path":"2019/08/29/My-blog-79/","link":"","permalink":"http://yoursite.com/2019/08/29/My-blog-79/","excerpt":"","text":"JWT 特点 体积小，因而传输速度快 传输方式多样，可以通过URL/POST参数/HTTP头部等方式传输 严格的结构化。它自身（在 payload 中）就包含了所有与用户相关的验证消息，如用户可访问路由、访问有效期等信息，服务器无需再去连接数据库验证信息的有效性，并且 payload 支持为你的应用而定制化。 支持跨域验证，可以应用于单点登录。 JWT是Auth0提出的通过对JSON进行加密签名来实现授权验证的方案，编码之后的JWT看起来是这样的一串字符： 1eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiYWRtaW4iOnRydWV9.TJVA95OrM7E2cBab30RMHrHDcEfxjoYZgeFONFh7HgQ 由 . 分为三段，通过解码可以得到： 头部（Header） 12345// 包括类别（typ）、加密算法（alg）；&#123; &quot;alg&quot;: &quot;HS256&quot;, &quot;typ&quot;: &quot;JWT&quot;&#125; jwt的头部包含两部分信息： 声明类型，这里是jwt 声明加密的算法 通常直接使用 HMAC SHA256 载荷（payload） 载荷（payload） 载荷就是存放有效信息的地方。 签名（signature) 签名的目的：签名实际上是对头部以及载荷内容进行签名。所以，如果有人对头部以及载荷的内容解码之后进行修改，再进行编码的话，那么新的头部和载荷的签名和之前的签名就将是不一样的。而且，如果不知道服务器加密的时候用的密钥的话，得出来的签名也一定会是不一样的。 这样就能保证token不会被篡改。 最后，我们将上面拼接完的字符串用HS256算法进行加密。在加密的时候，我们还需要提供一个密钥（secret）。类似盐 这里在第三步我们得到 JWT 之后，需要将JWT存放在 client，之后的每次需要认证的请求都要把JWT发送过来。（请求时可以放到 header 的 Authorization ） 一、使用JSON Web Token的好处？ 1.性能问题。 JWT方式将用户状态分散到了客户端中，相比于session，可以明显减轻服务端的内存压力。 Session方式存储用户id的最大弊病在于Session是存储在服务器端的，所以需要占用大量服务器内存， 对于较大型应用而言可能还要保存许多的状态，一般还需借助nosql和缓存机制来实现session的存储，如果是分布式应用还需session共享。 2.单点登录。 JWT能轻松的实现单点登录，因为用户的状态已经被传送到了客户端。 token 可保存自定义信息，如用户基本信息，web服务器用key去解析token，就获取到请求用户的信息了。 我们也可以配置它以便包含用户拥有的任何权限。这意味着每个服务不需要与授权服务交互才能授权用户。 3.前后端分离。 以前的传统模式下，后台对应的客户端就是浏览器，就可以使用session+cookies的方式实现登录， 但是在前后分离的情况下，后端只负责通过暴露的RestApi提供数据，而页面的渲染、路由都由前端完成。因为rest是无状态的，因此也就不会有session记录到服务器端。 4.兼容性。 支持移动设备，支持跨程序调用，Cookie 是不允许垮域访问的，而 Token 则不存在这个问题。 5.可拓展性。 jwt是无状态的，特别适用于分布式站点的单点登录（SSO）场景。 比如有3台机器（A、B、C）组成服务器集群，若session存在机器A上，session只能保存在其中一台服务器，此时你便不能访问机器B、C，因为B、C上没有存放该Session， 而使用token就能够验证用户请求合法性，并且我再加几台机器也没事，所以可拓展性好。 6.安全性。因为有签名，所以JWT可以防止被篡改。 JWT是基于token的身份认证的方案。 json web token全称。可以保证安全传输的前提下传送一些基本的信息，以减轻对外部存储的依赖，减少了分布式组件的依赖，减少了硬件的资源。 可实现无状态、分布式的Web应用授权，jwt的安全特性保证了token的不可伪造和不可篡改。 本质上是一个独立的身份验证令牌，可以包含用户标识、用户角色和权限等信息，以及您可以存储任何其他信息（自包含）。任何人都可以轻松读取和解析，并使用密钥来验证真实性。 缺陷： 1）JWT在生成token的时候支持失效时间，但是支持的失效时间是固定的，比如说一天。 但是用户在等出的时候是随机触发的，那么我们jwt token来做这个失效是不可行的，因为jwt在初始化的时候已经定死在什么时候过期了。 采用其他方案，在redis中存储token，设置token的过期时间，每次鉴权的时候都会去延长时间 2）jwt不适合存放大量信息，信息越多token越长","categories":[],"tags":[]},{"title":"关于MongoDB的详解","slug":"My-blog-78","date":"2019-08-22T15:15:59.000Z","updated":"2019-10-22T13:29:37.682Z","comments":true,"path":"2019/08/22/My-blog-78/","link":"","permalink":"http://yoursite.com/2019/08/22/My-blog-78/","excerpt":"","text":"MongoDB 是一个介于关系数据库和非关系数据库之间的开源产品，是最接近于关系型数据库的 NoSQL 数据库。它在轻量级JSON 交换基础之上进行了扩展，即称为 BSON 的方式来描述其无结构化的数据类型。尽管如此它同样可以存储较为复杂的数据类型。它和上一篇文章讲到的Redis有异曲同工之妙。虽然两者均为 NoSQL ，但是 MongoDB 相对于 Redis 而言，MongoDB 更像是传统的数据库。早些年我们是先有了 Relation Database (关系型数据库)，然后出现了很多很复杂的query ，里面用到了很多嵌套，很多 join 操作。所以在设计数据库的时候，我们也考虑到了如何应用他们的关系，使得写 query 可以使 database 效率达到最高。后来人们发现，不是每个系统，都需要如此复杂的关系型数据库。有些简单的网站，比如博客，比如社交网站，完全可以斩断数据库之间的一切关系。这样做带来的好处是，设计数据库变得更加简单，写 query 也变得更加简单。然后，query 消耗的时间可能也会变少。因为 query 简单了，少了许多消耗资源的 join 操作，速度自然会上去。正如所说的， query 简单了，很有以前 MySQL 可以找到的东西，现在关系没了，通过 Mongo 找不到了。我们只能将几组数据都抓到本地，然后在本地做 join ，所以在这点上可能会消耗很多资源。这里我们可以发现。如何选择数据库，完全取决于你所需要处理的数据的模型，即 Data Model 。如果它们之间，关系错综复杂，千丝万缕，这个时候 MySQL 一定是首选。如果他们的关系并不是那么密切，那么， NoSQL 将会是利器。 MongoDB 和 Redis 一样均为 key-value 存储系统，它具有以下特点： 面向集合存储，易存储对象类型的数据。 模式自由。 支持动态查询。 支持完全索引，包含内部对象。 支持查询。 支持复制和故障恢复。 使用高效的二进制数据存储，包括大型对象(如视频等)。 自动处理碎片，以支持云计算层次的扩展性 支持 Python ， PHP ， Ruby ， Java ， C ， C# ， Javascript ，Perl 及 C++ 语言的驱动程序，社区中也提供了对 Erlang 及 .NET 等平台的驱动程序。 文件存储格式为 BSON (一种 JSON 的扩展)。 可通过网络访问。 MongoDB 与 MySQL 性能比较像 MySQL 一样， MongoDB 提供了丰富的远远超出了简单的键值存储中提供的功能和功能。 MongoDB 具有查询语言，功能强大的辅助索引(包括文本搜索和地理空间)，数据分析功能强大的聚合框架等。相比使用关系数据库而言，使用MongoDB ，您还可以使用如下表所示的这些功能，跨越更多样化的数据类型和数据规模。 MySQLMongoDB丰富的数据模型否是动态 Schema否是数据类型是是数据本地化否是字段更新是是易于编程否是复杂事务是否审计是是自动分片否是 MySQL 中的许多概念在 MongoDB 中具有相近的类比。本表概述了每个系统中的一些常见概念。 MySQLMongoDB表集合行文档列字段joins嵌入文档或者链接 MongoDB应用范围和限制MongoDB 的主要目标是在 key-value (键/值)存储方式(提供了高性能和高度伸缩性)以及传统的 RDBMS 系统(丰富的功能)架起一座桥梁，集两者的优势于一身。 MongoDB 适用范围如下： 网站数据： Mongo 非常适合实时的插入，更新与查询，并具备网站实时数据存储所需的复制及高度伸缩性。 缓存：由于性能很高， Mongo 也适合作为信息基础设施的缓存层。在系统重启之后，由 Mongo 搭建的持久化缓存层可以避免下层的数据源过载。 大尺寸，低价值的数据：使用传统的关系型数据库存储一些数据时可能会比较昂贵，在此之前，很多时候程序员往往会选择传统的文件进行存储。 高伸缩性的场景： Mongo 非常适合由数十或数百台服务器组成的数据库。 Mongo 的路线图中已经包含对 MapReduce 引擎的内置支持。 用于对象及 JSON 数据的存储： Mongo 的 BSON 数据格式非常适合文档化格式的存储及查询。 MongoDB 当然也会有以下场景的限制： 高度事物性的系统：例如银行或会计系统。传统的关系型数据库目前还是更适用于需要大量原子性复杂事务的应用程序。 传统的商业智能应用：针对特定问题的 BI 数据库会对产生高度优化的查询方式。对于此类应用，数据仓库可能是更合适的选择。 需要 SQL 的问题。","categories":[],"tags":[]},{"title":"redis持久化的几种方式","slug":"My-blog-77","date":"2019-08-10T12:11:49.000Z","updated":"2019-10-22T13:28:37.774Z","comments":true,"path":"2019/08/10/My-blog-77/","link":"","permalink":"http://yoursite.com/2019/08/10/My-blog-77/","excerpt":"","text":"1、快照（snapshots） 缺省情况情况下，Redis把数据快照存放在磁盘上的二进制文件中，文件名为dump.rdb。你可以配置Redis的持久化策略，例如数据集中每N秒钟有超过M次更新，就将数据写入磁盘；或者你可以手工调用命令SAVE或BGSAVE。 工作原理 Redis forks. 子进程开始将数据写到临时RDB文件中。 当子进程完成写RDB文件，用新文件替换老文件。 这种方式可以使Redis使用copy-on-write技术。 2、AOF 快照模式并不十分健壮，当系统停止，或者无意中Redis被kill掉，最后写入Redis的数据就会丢失。 这对某些应用也许不是大问题，但对于要求高可靠性的应用来说，Redis就不是一个合适的选择。Append-only文件模式是另一种选择。你可以在配置文件中打开AOF模式 3、虚拟内存方式 当你的key很小而value很大时,使用VM的效果会比较好.因为这样节约的内存比较大. 当你的key不小时,可以考虑使用一些非常方法将很大的key变成很大的value,比如你可以考虑将key,value组合成一个新的value. vm-max-threads这个参数,可以设置访问swap文件的线程数,设置最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的.可能会造成比较长时间的延迟,但是对数据完整性有很好的保证. 自己测试的时候发现用虚拟内存性能也不错。如果数据量很大，可以考虑分布式或者其他数据库。","categories":[],"tags":[]},{"title":"关于redis的详解","slug":"My-blog-76","date":"2019-08-01T13:21:42.000Z","updated":"2019-10-22T13:27:49.393Z","comments":true,"path":"2019/08/01/My-blog-76/","link":"","permalink":"http://yoursite.com/2019/08/01/My-blog-76/","excerpt":"","text":"Reids的特点Redis本质上是一个Key-Value类型的内存数据库，很像memcached，整个数据库统统加载在内存当中进行操作，定期通过异步操作把数据库数据flush到硬盘上进行保存。 因为是纯内存操作，Redis的性能非常出色，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。 Redis的出色之处不仅仅是性能，Redis最大的魅力是支持保存多种数据结构，此外单个value的最大限制是1GB，不像 memcached只能保存1MB的数据，因此Redis可以用来实现很多有用的功能。 比方说用他的List来做FIFO双向链表，实现一个轻量级的高性 能消息队列服务，用他的Set可以做高性能的tag系统等等。另外Redis也可以对存入的Key-Value设置expire时间，因此也可以被当作一 个功能加强版的memcached来用。 Redis的主要缺点是数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。 使用redis有哪些好处？1.速度快，因为数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1) 2.支持丰富数据类型，支持string，list，set，sorted set，hash 3.支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 4.丰富的特性：可用于缓存，消息，按key设置过期时间，过期后将会自动删除 为什么redis需要把所有数据放到内存中?Redis为了达到最快的读写速度将数据都读到内存中，并通过异步的方式将数据写入磁盘。所以redis具有快速和数据持久化的特征。如果不将数据放在内存中，磁盘I/O速度为严重影响redis的性能。在内存越来越便宜的今天，redis将会越来越受欢迎。 如果设置了最大使用的内存，则数据已有记录数达到内存限值后不能继续插入新值。","categories":[],"tags":[]},{"title":"mysql order by rand() 优化","slug":"My-blog-75","date":"2019-07-14T08:39:36.000Z","updated":"2019-10-22T13:25:07.414Z","comments":true,"path":"2019/07/14/My-blog-75/","link":"","permalink":"http://yoursite.com/2019/07/14/My-blog-75/","excerpt":"","text":"当时有个需求是从数据库中随机取出一条数据作为展示，我理所当然的使用了 oder by rand(),结果导致慢查询非常多从而锁表影响了效率 查了一下文档 rand()会扫描整个表，然后再随机返回一个记录。对于比较小的表，通常不大于30万行记录的表，这种写法很实用。但是如果一旦记录大于了30万行，这个处理过程就会变得非常缓慢 所以结论就是能不用rand()就不要用 最后给出一种比较实用的替代方法的主要思想： 假设id是主键 首先：SELECT MIN(id), MAX(id) FROM tablename 然后：$id=rand($min,$max); //通过rand返回刚才取到的最大id和最小id之间的一个id号。 最后：SELECT * FROM tablename WHERE id=’$id’ LIMIT 1","categories":[],"tags":[]},{"title":"使用Celery异步任务时遇到的问题","slug":"My-blog-74","date":"2019-06-30T04:39:30.000Z","updated":"2019-10-22T13:24:36.548Z","comments":true,"path":"2019/06/30/My-blog-74/","link":"","permalink":"http://yoursite.com/2019/06/30/My-blog-74/","excerpt":"","text":"遇到了celery无法启动的问题，报错：SyntaxError: invalid syntax ，这是因为我使用的python版本为最新3.7.3 ，而async已经作为关键字而存在了 在 celery 官方的提议下，建议将 async.py 文件的文件名改成 asynchronous。所以我们只需要将 celery\\backends\\async.py 改成 celery\\backends\\asynchronous.py，并且把 celery\\backends\\redis.py 中的所有 async 改成 asynchronous 另外虽然服务起来了，但是服务会不定期的假死 1报错：Celery Process &apos;Worker&apos; exited with &apos;exitcode 1&apos; [duplicate] 经过搜索可以定位到问题所在，是因为celery依赖库billiard版本过低，导致任务发生了阻塞，所以最好的解决方案就是升级billiard 执行 pip install –upgrade billiard 官方的解释是，billiard最好&gt;=3.5，所以如果不放心的话，还是指定版本号安装比较好","categories":[],"tags":[]},{"title":"使用Supervisor管理后台服务遇到的问题","slug":"My-blog-73","date":"2019-06-15T08:39:23.000Z","updated":"2019-10-22T13:23:57.132Z","comments":true,"path":"2019/06/15/My-blog-73/","link":"","permalink":"http://yoursite.com/2019/06/15/My-blog-73/","excerpt":"","text":"我想用Supervisor来监控后台的uwsgi服务，结果始终显示fatal报错，但是别的服务都可以，仔细研读文档才发现，原来Supervisor本身无法监控带有守护进程的服务，而wsgi本身的配置文件中有守护进程的属性，所以我在wsgi的配置文件中关闭了守护进程选项，直接依赖Supervisor的特性赋予uwsgi守护进程，如果uwsgi服务被kill则不需要依赖自身的守护进程拉起，而是变成依赖Supervisor拉起服务，这样就实现了Supervisor监控uwsgi服务","categories":[],"tags":[]},{"title":"可变与不可变","slug":"My-blog-41","date":"2019-06-14T09:26:21.000Z","updated":"2019-10-22T12:23:35.856Z","comments":true,"path":"2019/06/14/My-blog-41/","link":"","permalink":"http://yoursite.com/2019/06/14/My-blog-41/","excerpt":"","text":"可变类型（mutable）：列表，字典 不可变类型（unmutable）：数字，字符串，元组 这里的可变不可变，是指内存中的那块内容（value）是否可以被改变。如果是不可变类型，在对对象本身操作的时候，必须在内存中新申请一块区域(因为老区域#不可变#)。如果是可变类型，对对象操作的时候，不需要再在其他地方申请内存，只需要在此对象后面连续申请(+/-)即可，也就是它的address会保持不变，但区域会变长或者变短。 copy.copy() 浅拷贝 copy.deepcopy() 深拷贝 浅拷贝是新创建了一个跟原对象一样的类型，但是其内容是对原对象元素的引用。这个拷贝的对象本身是新的，但内容不是。拷贝序列类型对象（列表\\元组）时，默认是浅拷贝。","categories":[],"tags":[]},{"title":"支付时遇到的性能问题","slug":"My-blog-72","date":"2019-06-07T12:24:17.000Z","updated":"2019-10-22T13:23:34.527Z","comments":true,"path":"2019/06/07/My-blog-72/","link":"","permalink":"http://yoursite.com/2019/06/07/My-blog-72/","excerpt":"","text":"在支付宝支付功能上线后，发现在某些时间段内，服务器就会报警，查了一下，内存占用过高，理论上一个很普通的支付宝三方支付业务不应该会导致性能问题，仔细看了一下源代码，发现当时写支付类的时候很随意，没有考虑到每次支付都会在内存中新建实例，而python的内存管理机制导致支付结束后不会主动销毁，所以我改造了一下支付类，将其改造成了单例模式，这样就解决了支付频率过高导致内存占用过高的问题。 1234567891011121314151617from functools import wrapsdef singleton(cls): instance = None @wraps(cls) def wrap(*args,**kwargs): nonlocal instance if instance is None: instance = cls(*args,**kwargs) #args,kwargs是用于将参数传递到__init__中 return instance return wrap@singletonclass A: passa = A()a1 = A()id(a)id(a1)","categories":[],"tags":[]},{"title":"关于触发器","slug":"My-blog-71","date":"2019-05-26T11:41:11.000Z","updated":"2019-10-22T13:22:30.475Z","comments":true,"path":"2019/05/26/My-blog-71/","link":"","permalink":"http://yoursite.com/2019/05/26/My-blog-71/","excerpt":"","text":"触发程序是与表有关的命名数据库对象，当该表出现特定事件时，将激活该对象 监听：记录的增加、修改、删除。 – 创建触发器 CREATE TRIGGER trigger_name trigger_time trigger_event ON tbl_name FOR EACH ROW trigger_stmt 参数： trigger_time是触发程序的动作时间。它可以是 before 或 after，以指明触发程序是在激活它的语句之前或之后触发。 trigger_event指明了激活触发程序的语句的类型 INSERT：将新行插入表时激活触发程序 UPDATE：更改某一行时激活触发程序 DELETE：从表中删除某一行时激活触发程序 tbl_name：监听的表，必须是永久性的表，不能将触发程序与TEMPORARY表或视图关联起来。 trigger_stmt：当触发程序激活时执行的语句。执行多个语句，可使用BEGIN…END复合语句结构 – 删除 DROP TRIGGER [schema_name.]trigger_name 可以使用old和new代替旧的和新的数据 更新操作，更新前是old，更新后是new. 删除操作，只有old. 增加操作，只有new. – 注意 11. 对于具有相同触发程序动作时间和事件的给定表，不能有两个触发程序。","categories":[],"tags":[]},{"title":"悲观锁和乐观锁","slug":"My-blog-70","date":"2019-05-03T08:39:07.000Z","updated":"2019-10-22T13:21:49.773Z","comments":true,"path":"2019/05/03/My-blog-70/","link":"","permalink":"http://yoursite.com/2019/05/03/My-blog-70/","excerpt":"","text":"乐观锁乐观锁不是数据库自带的，需要我们自己去实现。乐观锁是指操作数据库时(更新操作)，想法很乐观，认为这次的操作不会导致冲突，在操作数据时，并不进行任何其他的特殊处理（也就是不加锁），而在进行更新后，再去判断是否有冲突了。 悲观锁每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁 更新丢失：最后的更新覆盖了其他事务之前的更新，而事务之间并不知道，发生更新丢失。更新丢失，可以完全避免，应用对访问的数据加锁即可。脏读：(针对未提交的数据)一个事务在更新一条记录，未提交前，第二个事务读到了第一个事务更新后的记录，那么第二个事务就读到了脏数据，会产生对第一个未提交,解决方案加锁，或者调整mysql事务隔离级别，数据库的事务隔离越严格，并发负作用越小，代价越高","categories":[],"tags":[]},{"title":"主从同步延迟问题","slug":"My-blog-69","date":"2019-04-19T08:39:01.000Z","updated":"2019-10-22T13:21:03.634Z","comments":true,"path":"2019/04/19/My-blog-69/","link":"","permalink":"http://yoursite.com/2019/04/19/My-blog-69/","excerpt":"","text":"架构方面 1.业务的持久化层的实现采用分库架构，mysql服务可平行扩展，分散压力。 2.单个库读写分离，一主多从，主写从读，分散压力。这样从库压力比主库高，保护主库。 3.服务的基础架构在业务和mysql之间加入memcache或者Redis的cache层。降低mysql的读压力。 4.不同业务的mysql物理上放在不同机器，分散压力。 5.使用比主库更好的硬件设备作为slave 总结，mysql压力小，延迟自然会变小。 硬件方面 1.采用好服务器，比如4u比2u性能明显好，2u比1u性能明显好。 2.存储用ssd或者盘阵或者san，提升随机写的性能。 3.主从间保证处在同一个交换机下面，并且是万兆环境。 总结，硬件强劲，延迟自然会变小。一句话，缩小延迟的解决方案就是花钱和花时间。 mysql主从同步加速 1、sync_binlog在slave端设置为0 2、–logs-slave-updates 从服务器从主服务器接收到的更新不记入它的二进制日志。 3、直接禁用slave端的binlog 4、slave端，如果使用的存储引擎是innodb，innodb_flush_log_at_trx_commit = 2","categories":[],"tags":[]},{"title":"mysql集群主从库读写分离","slug":"My-blog-68","date":"2019-04-05T07:28:25.000Z","updated":"2019-10-22T13:20:32.704Z","comments":true,"path":"2019/04/05/My-blog-68/","link":"","permalink":"http://yoursite.com/2019/04/05/My-blog-68/","excerpt":"","text":"高负载高并发环境下，数据业务层、数据访问层，如果还是传统的数据结构，或者只是单单靠一台服务器负载，如此多的数据库连接操作，数据库必然会崩溃，数据库如果宕机的话，后果更是不堪设想。这时候，我们会考虑如何减少数据库的连接，一方面采用优秀的代码框架，进行代码的优化，采用优秀的数据缓存技术如：redis,如果资金丰厚的话，必然会想到架设mysql服务集群，来分担主数据库的压力。今天总结一下利用MySQL主从配置，实现读写分离，减轻数据库压力。 mysql主从同步的原理很简单，从库生成两个线程，一个I/O线程，一个SQL线程；i/o线程去请求主库 的binlog（二进制日志），并将得到的binlog日志写到relay log（中继日志） 文件中；主库会生成一个 log dump 线程，用来给从库 i/o线程传binlog； SQL 线程，会读取relay log文件中的日志，并解析成具体操作，来实现主从的操作一致，而最终数据一致","categories":[],"tags":[]},{"title":"关于数据结构的详解","slug":"My-blog-67","date":"2019-03-15T10:46:50.000Z","updated":"2019-10-22T12:54:12.653Z","comments":true,"path":"2019/03/15/My-blog-67/","link":"","permalink":"http://yoursite.com/2019/03/15/My-blog-67/","excerpt":"","text":"什么是数据结构？ 简单地说，数据结构是以某种特定的布局方式存储数据的容器。这种“布局方式”决定了数据结构对于某些操作是高效的，而对于其他操作则是低效的。首先我们需要理解各种数据结构，才能在处理实际问题时选取最合适的数据结构。 为什么我们需要数据结构？ 数据是计算机科学当中最关键的实体，而数据结构则可以将数据以某种组织形式存储，因此，数据结构的价值不言而喻。 无论你以何种方式解决何种问题，你都需要处理数据——无论是涉及员工薪水、股票价格、购物清单，还是只是简单的电话簿问题。 数据需要根据不同的场景，按照特定的格式进行存储。有很多数据结构能够满足以不同格式存储数据的需求。 常见的数据结构 首先列出一些最常见的数据结构，我们将逐一说明： 数组 栈 队列 链表 树 字典树（这是一种高效的树形结构，但值得单独说明） 散列表（哈希表） 数组数组是最简单、也是使用最广泛的数据结构。栈、队列等其他数据结构均由数组演变而来。下图是一个包含元素（1，2，3和4）的简单数组，数组长度为4。 每个数据元素都关联一个正数值，我们称之为索引，它表明数组中每个元素所在的位置。大部分语言将初始索引定义为零。 以下是数组的两种类型： 一维数组（如上所示） 多维数组（数组的数组） 数组的基本操作 Insert——在指定索引位置插入一个元素 Get——返回指定索引位置的元素 Delete——删除指定索引位置的元素 Size——得到数组所有元素的数量 面试中关于数组的常见问题 寻找数组中第二小的元素 找到数组中第一个不重复出现的整数 合并两个有序数组 重新排列数组中的正值和负值 栈著名的撤销操作几乎遍布任意一个应用。但你有没有思考过它是如何工作的呢？这个问题的解决思路是按照将最后的状态排列在先的顺序，在内存中存储历史工作状态（当然，它会受限于一定的数量）。这没办法用数组实现。但有了栈，这就变得非常方便了。 可以把栈想象成一列垂直堆放的书。为了拿到中间的书，你需要移除放置在这上面的所有书。这就是LIFO（后进先出）的工作原理。 下图是包含三个数据元素（1，2和3）的栈，其中顶部的3将被最先移除： 栈的基本操作 Push——在顶部插入一个元素 Pop——返回并移除栈顶元素 isEmpty——如果栈为空，则返回true Top——返回顶部元素，但并不移除它 面试中关于栈的常见问题 使用栈计算后缀表达式 对栈的元素进行排序 判断表达式是否括号平衡 队列与栈相似，队列是另一种顺序存储元素的线性数据结构。栈与队列的最大差别在于栈是LIFO（后进先出），而队列是FIFO，即先进先出。 一个完美的队列现实例子：售票亭排队队伍。如果有新人加入，他需要到队尾去排队，而非队首——排在前面的人会先拿到票，然后离开队伍。 下图是包含四个元素（1，2，3和4）的队列，其中在顶部的1将被最先移除： 移除先入队的元素、插入新元素 队列的基本操作 Enqueue()——在队列尾部插入元素 Dequeue()——移除队列头部的元素 isEmpty()——如果队列为空，则返回true Top()——返回队列的第一个元素 面试中关于队列的常见问题 使用队列表示栈 对队列的前k个元素倒序 使用队列生成从1到n的二进制数 链表链表是另一个重要的线性数据结构，乍一看可能有点像数组，但在内存分配、内部结构以及数据插入和删除的基本操作方面均有所不同。 链表就像一个节点链，其中每个节点包含着数据和指向后续节点的指针。 链表还包含一个头指针，它指向链表的第一个元素，但当列表为空时，它指向null或无具体内容。 链表一般用于实现文件系统、哈希表和邻接表。 这是链表内部结构的展示： 链表包括以下类型： 单链表（单向） 双向链表（双向） 链表的基本操作： InsertAtEnd - 在链表的末尾插入指定元素 InsertAtHead - 在链接列表的开头/头部插入指定元素 Delete - 从链接列表中删除指定元素 DeleteAtHead - 删除链接列表的第一个元素 Search - 从链表中返回指定元素 isEmpty - 如果链表为空，则返回true 面试中关于链表的常见问题 反转链表 检测链表中的循环 返回链表倒数第N个节点 删除链表中的重复项 树树形结构是一种层级式的数据结构，由顶点（节点）和连接它们的边组成。 树类似于图，但区分树和图的重要特征是树中不存在环路。 树形结构被广泛应用于人工智能和复杂算法，它可以提供解决问题的有效存储机制。 这是一个简单树的示意图，以及树数据结构中使用的基本术语： Root - 根节点 Parent - 父节点 Child - 子节点 Leaf - 叶子节点 Sibling - 兄弟节点 以下是树形结构的主要类型： N元树 平衡树 二叉树 二叉搜索树 AVL树 红黑树 2-3树 其中，二叉树和二叉搜索树是最常用的树。 面试中关于树结构的常见问题： 求二叉树的高度 在二叉搜索树中查找第k个最大值 查找与根节点距离k的节点 在二叉树中查找给定节点的祖先节点 字典树（Trie） 字典树，也称为“前缀树”，是一种特殊的树状数据结构，对于解决字符串相关问题非常有效。它能够提供快速检索，主要用于搜索字典中的单词，在搜索引擎中自动提供建议，甚至被用于IP的路由。 这些单词以顶部到底部的方式存储，其中绿色节点“p”，“s”和“r”分别表示“top”，“thus”和“theirs”的底部。 面试中关于字典树的常见问题 计算字典树中的总单词数 打印存储在字典树中的所有单词 使用字典树对数组的元素进行排序 使用字典树从字典中形成单词 构建T9字典（字典树+ DFS ） 哈希表哈希法（Hashing）是一个用于唯一标识对象并将每个对象存储在一些预先计算的唯一索引（称为“键（key）”）中的过程。因此，对象以键值对的形式存储，这些键值对的集合被称为“字典”。可以使用键搜索每个对象。基于哈希法有很多不同的数据结构，但最常用的数据结构是哈希表。 哈希表通常使用数组实现。","categories":[],"tags":[]},{"title":"关于递归","slug":"My-blog-66","date":"2019-01-18T08:38:43.000Z","updated":"2019-10-22T12:53:12.111Z","comments":true,"path":"2019/01/18/My-blog-66/","link":"","permalink":"http://yoursite.com/2019/01/18/My-blog-66/","excerpt":"","text":"在调用一个函数的过程中，直接或间接地调用了函数本身这个就叫递归。但为了避免出现死循环，必须要有一个结束条件 在函数中调用函数本身时，相当于你让程序回到函数的第一行重新走一遍而已 1234567def foo(S, T): S = T * T - S if S &gt;= 10: W = S + T * T return W else: foo(S, T * 2)","categories":[],"tags":[]},{"title":"八大排序算法","slug":"My-blog-65","date":"2018-12-20T12:37:26.000Z","updated":"2019-10-22T12:52:22.226Z","comments":true,"path":"2018/12/20/My-blog-65/","link":"","permalink":"http://yoursite.com/2018/12/20/My-blog-65/","excerpt":"","text":"插入排序插入排序：插入排序的基本操作就是将一个数据插入到已经排好序的有序数据中，从而得到一个新的、个数加一的有序数据，算法适用于少量数据的排序；首先将第一个作为已经排好序的，然后每次从后的取出插入到前面并排序； 时间复杂度：O(n²) 空间复杂度：O(1) 稳定性：稳定 1234567def insert_sort(ilist): for i in range(len(ilist)): for j in range(i): if ilist[i] &lt; ilist[j]: ilist.insert(j, ilist.pop(i)) break return ilist 冒泡排序冒泡排序：它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成 时间复杂度：O(n²) 空间复杂度：O(1) 稳定性：稳定 12345678910def bubble_sort(blist): count = len(blist) for i in range(0, count): for j in range(i + 1, count): if blist[i] &gt; blist[j]: blist[i], blist[j] = blist[j], blist[i] return blistblist = bubble_sort([4,5,6,7,3,2,6,9,8])print(blist) 快排快速排序：通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列 时间复杂度：O(nlog₂n) 空间复杂度：O(nlog₂n) 稳定性：不稳定 12345678910def quick_sort(qlist): if qlist == []: return [] else: qfirst = qlist[0] qless = quick_sort([l for l in qlist[1:] if l &lt; qfirst]) qmore = quick_sort([m for m in qlist[1:] if m &gt;= qfirst]) return qless + [qfirst] + qmoreqlist = quick_sort([4,5,6,7,3,2,6,9,8]) 选择排序选择排序：第1趟，在待排序记录r1 ~ r[n]中选出最小的记录，将它与r1交换；第2趟，在待排序记录r2 ~ r[n]中选出最小的记录，将它与r2交换；以此类推，第i趟在待排序记录r[i] ~ r[n]中选出最小的记录，将它与r[i]交换，使有序序列不断增长直到全部排序完毕 时间复杂度：O(n²) 空间复杂度：O(1) 稳定性：不稳定 12345678910def select_sort(slist): for i in range(len(slist)): x = i for j in range(i, len(slist)): if slist[j] &lt; slist[x]: x = j slist[i], slist[x] = slist[x], slist[i] return slistslist = select_sort([4,5,6,7,3,2,6,9,8]) 归并排序归并排序：采用分治法（Divide and Conquer）的一个非常典型的应用。将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序。若将两个有序表合并成一个有序表，称为二路归并 时间复杂度：O(nlog₂n) 空间复杂度：O(1) 稳定性：稳定 1234567891011121314151617181920212223def merge_sort(array): def merge_arr(arr_l, arr_r): array = [] while len(arr_l) and len(arr_r): if arr_l[0] &lt;= arr_r[0]: array.append(arr_l.pop(0)) elif arr_l[0] &gt; arr_r[0]: array.append(arr_r.pop(0)) if len(arr_l) != 0: array += arr_l elif len(arr_r) != 0: array += arr_r return array def recursive(array): if len(array) == 1: return array mid = len(array) // 2 arr_l = recursive(array[:mid]) arr_r = recursive(array[mid:]) return merge_arr(arr_l, arr_r) return recursive(array)","categories":[],"tags":[]},{"title":"Git相关知识","slug":"My-blog-64","date":"2018-09-19T08:38:29.000Z","updated":"2019-10-22T12:51:09.894Z","comments":true,"path":"2018/09/19/My-blog-64/","link":"","permalink":"http://yoursite.com/2018/09/19/My-blog-64/","excerpt":"","text":"常用命令git init 在本地新建一个repo,进入一个项目目录,执行git init,会初始化一个repo,并在当前文件夹下创建一个.git文件夹. git clone 获取一个url对应的远程Git repo, 创建一个local copy. 一般的格式是git clone [url]. clone下来的repo会以url最后一个斜线后面的名称命名,创建一个文件夹,如果想要指定特定的名称,可以git clone [url] newname指定. git status 查询repo的状态. git status -s: -s表示short, -s的输出标记会有两列,第一列是对staging区域而言,第二列是对working目录而言. git log show commit history of a branch. git log –oneline –number: 每条log只显示一行,显示number条. git log –oneline –graph:可以图形化地表示出分支合并历史. git log branchname可以显示特定分支的log. git log –oneline branch1 ^branch2,可以查看在分支1,却不在分支2中的提交.^表示排除这个分支(Window下可能要给^branch2加上引号). git log –decorate会显示出tag信息. git log –author=[author name] 可以指定作者的提交历史. git log –since –before –until –after 根据提交时间筛选log. –no-merges可以将merge的commits排除在外. git log –grep 根据commit信息过滤log: git log –grep=keywords 默认情况下, git log –grep –author是OR的关系,即满足一条即被返回,如果你想让它们是AND的关系,可以加上–all-match的option. git log -S: filter by introduced diff. 比如: git log -SmethodName (注意S和后面的词之间没有等号分隔). git log -p: show patch introduced at each commit. 每一个提交都是一个快照(snapshot),Git会把每次提交的diff计算出来,作为一个patch显示给你看. 另一种方法是git show [SHA]. git log –stat: show diffstat of changes introduced at each commit. 同样是用来看改动的相对信息的,–stat比-p的输出更简单一些. git add 在提交之前,Git有一个暂存区(staging area),可以放入新添加的文件或者加入新的改动. commit时提交的改动是上一次加入到staging area中的改动,而不是我们disk上的改动. git add . 会递归地添加当前工作目录中的所有文件. git diff 不加参数的git diff: show diff of unstaged changes. 此命令比较的是工作目录中当前文件和暂存区域快照之间的差异,也就是修改之后还没有暂存起来的变化内容. 123456789101112131415161718若要看已经暂存起来的文件和上次提交时的快照之间的差异,可以用:git diff --cached 命令.show diff of staged changes.(Git 1.6.1 及更高版本还允许使用 git diff --staged，效果是相同的).git diff HEADshow diff of all staged or unstated changes.也即比较woking directory和上次提交之间所有的改动.如果想看自从某个版本之后都改动了什么,可以用:git diff [version tag]跟log命令一样,diff也可以加上--stat参数来简化输出.git diff [branchA] [branchB]可以用来比较两个分支.它实际上会返回一个由A到B的patch,不是我们想要的结果.一般我们想要的结果是两个分支分开以后各自的改动都是什么,是由命令:git diff [branchA]…[branchB]给出的.实际上它是:git diff $(git merge-base [branchA] [branchB]) [branchB]的结果. git commit 提交已经被add进来的改动. git commit -m “the commit message” git commit -a 会先把所有已经track的文件的改动add进来,然后提交(有点像svn的一次提交,不用先暂存). 对于没有track的文件,还是需要git add一下. git commit –amend 增补提交. 会使用与当前提交节点相同的父节点进行一次新的提交,旧的提交将会被取消. git reset undo changes and commits. 这里的HEAD关键字指的是当前分支最末梢最新的一个提交.也就是版本库中该分支上的最新版本. git reset HEAD: unstage files from index and reset pointer to HEAD 这个命令用来把不小心add进去的文件从staged状态取出来,可以单独针对某一个文件操作: git reset HEAD - - filename, 这个- - 也可以不加. git reset –soft move HEAD to specific commit reference, index and staging are untouched. git reset –hard unstage files AND undo any changes in the working directory since last commit. 使用git reset —hard HEAD进行reset,即上次提交之后,所有staged的改动和工作目录的改动都会消失,还原到上次提交的状态. 这里的HEAD可以被写成任何一次提交的SHA-1. 不带soft和hard参数的git reset,实际上带的是默认参数mixed. 123456总结:git reset --mixed id,是将git的HEAD变了(也就是提交记录变了),但文件并没有改变，(也就是working tree并没有改变). 取消了commit和add的内容.git reset --soft id. 实际上，是git reset –mixed id 后,又做了一次git add.即取消了commit的内容.git reset --hard id.是将git的HEAD变了,文件也变了.按改动范围排序如下:soft (commit) &lt; mixed (commit + add) &lt; hard (commit + add + local working) git revert 反转撤销提交.只要把出错的提交(commit)的名字(reference)作为参数传给命令就可以了. git revert HEAD: 撤销最近的一个提交. git revert会创建一个反向的新提交,可以通过参数-n来告诉Git先不要提交. git rm git rm file: 从staging区移除文件,同时也移除出工作目录. git rm –cached: 从staging区移除文件,但留在工作目录中. git rm –cached从功能上等同于git reset HEAD,清除了缓存区,但不动工作目录树. git clean git clean是从工作目录中移除没有track的文件. 通常的参数是git clean -df: -d表示同时移除目录,-f表示force,因为在git的配置文件中, clean.requireForce=true,如果不加-f,clean将会拒绝执行. git mv git rm - - cached orig; mv orig new; git add new git stash 把当前的改动压入一个栈. git stash将会把当前目录和index中的所有改动(但不包括未track的文件)压入一个栈,然后留给你一个clean的工作状态,即处于上一次最新提交处. git stash list会显示这个栈的list. git stash apply:取出stash中的上一个项目(stash@{0}),并且应用于当前的工作目录. 也可以指定别的项目,比如git stash apply stash@{1}. 如果你在应用stash中项目的同时想要删除它,可以用git stash pop 123删除stash中的项目:git stash drop: 删除上一个,也可指定参数删除指定的一个项目.git stash clear: 删除所有项目. git branch git branch可以用来列出分支,创建分支和删除分支. git branch -v可以看见每一个分支的最后一次提交. git branch: 列出本地所有分支,当前分支会被星号标示出. git branch (branchname): 创建一个新的分支(当你用这种方式创建分支的时候,分支是基于你的上一次提交建立的). git branch -d (branchname): 删除一个分支. 删除remote的分支: git push (remote-name) :(branch-name): delete a remote branch. 这个是因为完整的命令形式是: git push remote-name local-branch:remote-branch 而这里local-branch的部分为空,就意味着删除了remote-branch git checkout git checkout (branchname) 切换到一个分支. git checkout -b (branchname): 创建并切换到新的分支. 这个命令是将git branch newbranch和git checkout newbranch合在一起的结果. checkout还有另一个作用:替换本地改动: git checkout – 此命令会使用HEAD中的最新内容替换掉你的工作目录中的文件.已添加到暂存区的改动以及新文件都不会受到影响. 注意:git checkout filename会删除该文件中所有没有暂存和提交的改动,这个操作是不可逆的. git merge 把一个分支merge进当前的分支. git merge [alias]/[branch] 把远程分支merge到当前分支. 12如果出现冲突,需要手动修改,可以用git mergetool.解决冲突的时候可以用到git diff,解决完之后用git add添加,即表示冲突已经被resolved. git tag tag a point in history as import. 会在一个提交上建立永久性的书签,通常是发布一个release版本或者ship了什么东西之后加tag. 比如: git tag v1.0 git tag -a v1.0, -a参数会允许你添加一些信息,即make an annotated tag. 当你运行git tag -a命令的时候,Git会打开一个编辑器让你输入tag信息. 12345我们可以利用commit SHA来给一个过去的提交打tag:git tag -a v0.9 XXXXpush的时候是不包含tag的,如果想包含,可以在push时加上--tags参数.fetch的时候,branch HEAD可以reach的tags是自动被fetch下来的, tags that aren’t reachable from branch heads will be skipped.如果想确保所有的tags都被包含进来,需要加上--tags选项. git remote list, add and delete remote repository aliases. 因为不需要每次都用完整的url,所以Git为每一个remote repo的url都建立一个别名,然后用git remote来管理这个list. git remote: 列出remote aliases. 如果你clone一个project,Git会自动将原来的url添加进来,别名就叫做:origin. git remote -v:可以看见每一个别名对应的实际url. git remote add [alias] [url]: 添加一个新的remote repo. git remote rm [alias]: 删除一个存在的remote alias. git remote rename [old-alias] [new-alias]: 重命名. git remote set-url [alias] [url]:更新url. 可以加上—push和fetch参数,为同一个别名set不同的存取地址. git fetch download new branches and data from a remote repository. 可以git fetch [alias]取某一个远程repo,也可以git fetch –all取到全部repo fetch将会取到所有你本地没有的数据,所有取下来的分支可以被叫做remote branches,它们和本地分支一样(可以看diff,log等,也可以merge到其他分支),但是Git不允许你checkout到它们. git pull fetch from a remote repo and try to merge into the current branch. pull == fetch + merge FETCH_HEAD git pull会首先执行git fetch,然后执行git merge,把取来的分支的head merge到当前分支.这个merge操作会产生一个新的commit.如果使用–rebase参数,它会执行git rebase来取代原来的git merge. git rebase –rebase不会产生合并的提交,它会将本地的所有提交临时保存为补丁(patch),放在”.git/rebase”目录中,然后将当前分支更新到最新的分支尖端,最后把保存的补丁应用到分支上. rebase的过程中,也许会出现冲突,Git会停止rebase并让你解决冲突,在解决完冲突之后,用git add去更新这些内容,然后无需执行commit,只需要: git rebase –continue就会继续打余下的补丁. git rebase –abort将会终止rebase,当前分支将会回到rebase之前的状态. git push push your new branches and data to a remote repository. git push [alias] [branch] 将会把当前分支merge到alias上的[branch]分支.如果分支已经存在,将会更新,如果不存在,将会添加这个分支. 如果有多个人向同一个remote repo push代码, Git会首先在你试图push的分支上运行git log,检查它的历史中是否能看到server上的branch现在的tip,如果本地历史中不能看到server的tip,说明本地的代码不是最新的,Git会拒绝你的push,让你先fetch,merge,之后再push,这样就保证了所有人的改动都会被考虑进来. git reflog git reflog是对reflog进行管理的命令,reflog是git用来记录引用变化的一种机制,比如记录分支的变化或者是HEAD引用的变化. 当git reflog不指定引用的时候,默认列出HEAD的reflog. HEAD@{0}代表HEAD当前的值,HEAD@{3}代表HEAD在3次变化之前的值. git会将变化记录到HEAD对应的reflog文件中,其路径为.git/logs/HEAD, 分支的reflog文件都放在.git/logs/refs目录下的子目录中. 分支如果团队中有多个人再开发一下项目，一同事再开发一个新的功能，需要一周时间完成，他写了其中的30%还没有写完，如果他提交了这个版本，那么团队中的其它人就不能继续开发了。但是等到他全部写完再全部提交，大家又看不到他的开发进度，也不能继续干活，这如何是好呢？ 对于上面的这个问题，我们就可以用分支管理的办法来解决，一同事开发新功能他可以创建一个属于他自己的分支，其它同事暂时看不到，继续在开发分支（一般都 有多个分支）上干活，他在自己的分支上干活，等他全部开发完成，再一次性的合并到开发分支上，这样我们既可知道他的开发进度，又不影响大家干活，是不是很方便呢？ 分支本质上其实就是一个指向某次提交的可变指针。Git 的默认分支名字为 master 。而我们是怎么知道当前处于哪个分支当中呢？答案就是在于 HEAD 这个十分特殊的指针，它专门用于指向于本地分支中的当前分支。我们可以简单理解为：commit &lt;- branch &lt;- HEAD 下面我们来创建分支。 解决冲突1、git冲突的场景 情景一：多个分支代码合并到一个分支时； 情景二：多个分支向同一个远端分支推送代码时； 实际上，push操作即是将本地代码merge到远端库分支上。 关于push和pull其实就分别是用本地分支合并到远程分支 和 将远程分支合并到本地分支 所以这两个过程中也可能存在冲突。 git的合并中产生冲突的具体情况： &lt;1&gt;两个分支中修改了同一个文件（不管什么地方） &lt;2&gt;两个分支中修改了同一个文件的名称 两个分支中分别修改了不同文件中的部分，不会产生冲突，可以直接将两部分合并。 2、冲突解决方法 情景一：在当前分支上，直接修改冲突代码—&gt;add—&gt;commit。 情景二：在本地当前分支上，修改冲突代码—&gt;add—&gt;commit—&gt;push","categories":[],"tags":[]},{"title":"微服务和RPC框架","slug":"My-blog-63","date":"2018-08-08T14:35:42.000Z","updated":"2019-10-22T12:50:29.775Z","comments":true,"path":"2018/08/08/My-blog-63/","link":"","permalink":"http://yoursite.com/2018/08/08/My-blog-63/","excerpt":"","text":"一般情况下，业务应用我们都会采用模块化的分层式架构，所有的业务逻辑代码最终会在一个代码库中并统一部署，我们称这种应用架构为单体应用。 单体应用的问题是，全部开发人员会共享一个代码库，不同模块的边界模糊，实现高内聚、松耦合极其困难。 肯定大家会碰到过这类场景，当尝试去重构改进代码时，改了一个地方好几个其他模块也需要同步改动， 当初划分的模块边界轻易被穿透，有人给这种应用的架构起了一个很形象的名字叫 “洋葱架构”。 特点： 1、服务注册、服务发现、健康检查 如果我们采用进程内LB方案，那么服务自注册一般统一做在服务器端框架中，健康检查逻辑由具体业务服务定制，框架层提供调用健康检查逻辑的机制，服务发现和负载均衡则集成在服务客户端框架中。 2、RPC/REST和序列化 框架层要支持将业务逻辑以HTTP/REST或者RPC方式暴露出来，HTTP/REST是当前主流API暴露方式，在性能要求高的场合则可采用Binary/RPC方式。针对当前多样化的设备类型(浏览器、普通PC、无线设备等)，框架层要支持可定制的序列化机制，例如，对浏览器，框架支持输出Ajax友好的JSON消息格式，而对无线设备上的Native App，框架支持输出性能高的Binary消息格式。 3、管理接口 框架集成管理接口，一方面可以在线查看框架和服务内部状态，同时还可以动态调整内部状态，对调试、监控和管理能提供快速反馈。Spring Boot微框架的Actuator模块就是一个强大的管理接口。对于框架层和服务的内部异常，如果框架层能够统一处理并记录日志，对服务监控和快速问题定位有很大帮助。 4、安全控制 安全和访问控制逻辑可以在框架层统一进行封装，可做成插件形式，具体业务服务根据需要加载相关安全插件。 5、配置管理 除了支持普通配置文件方式的配置，框架层还可集成动态运行时配置，能够在运行时针对不同环境动态调整服务的参数和配置。 6、监控日志 框架一方面要记录重要的框架层日志、服务调用链数据，还要将日志、调用链数据等接口暴露出来，让业务层能根据需要记录业务日志数据。在运行环境中，所有日志数据一般由日志系统做进一步分析和处理。 7、统一错误处理 对于框架层和服务的内部异常，如果框架层能够统一处理并记录日志，对服务监控和快速问题定位有很大帮助。 8、流控和容错 框架集成限流容错组件，能够在运行时自动限流和容错，保护服务，如果进一步和动态配置相结合，还可以实现动态限流和熔断。 RPC框架RPC=Remote Produce Call 是一种技术的概念名词. HTTP是一种协议,RPC可以通过HTTP来实现,也可以通过Socket自己实现一套协议来实现.所以楼主可以换一个问法,为何RPC还有除HTTP 之外的实现法,有何必要.毕竟除了HTTP实现外,私有协议不具备通用性.那么我想唯一的答案就在于HTTP不能满足其业务场景的地方,所以这个就要具体 案例具体分析了. http接口是在接口不多、系统与系统交互较少的情况下，解决信息孤岛初期常使用的一种通信手段；优点就是简单、直接、开发方便。利用现成的http协议 进行传输。但是如果是一个大型的网站，内部子系统较多、接口非常多的情况下，RPC框架的好处就显示出来了，首先就是长链接，不必每次通信都要像http 一样去3次握手什么的，减少了网络开销；其次就是RPC框架一般都有注册中心，有丰富的监控管理；发布、下线接口、动态扩展等，对调用方来说是无感知、统 一化的操作。第三个来说就是安全性。最后就是最近流行的服务化架构、服务化治理，RPC框架是一个强力的支撑 RPC是一个软件结构概念，是构建分布式应用的理论基础。就好比为啥你家可以用到发电厂发出来的电？是因为电是可以传输的。至于用铜线还是用铁丝还是其他 种类的导线，也就是用http还是用其他协议的问题了。这个要看什么场景，对性能要求怎么样。比如在java中的最基本的就是RMI技术，它是java原 生的应用层分布式技术。我们可以肯定的是在传输性能方面，RMI的性能是优于HTTP的。那为啥很少用到这个技术？那是因为用这个有很多局限性，首先它要 保证传输的两端都要要用java实现，且两边需要有相同的对象类型和代理接口，不需要容器，但是加大了编程的难度，在应用内部的各个子系统之间还是会看到 他的身影，比如EJB就是基于rmi技术的。这就与目前的bs架构的软件大相径庭。用http必须要服务端位于http容器里面，这样减少了网络传输方面 的开发，只需要关注业务开发即可。所以在架构一个软件的时候，不能一定根据需求选定技术。 ThriftThrift实际上是实现了C/S模式，通过代码生成工具将接口定义文件生成服务器端和客户端代码（可以为不同语言），从而实现服务端和客户端跨语言的支持。用户在Thirft描述文件中声明自己的服务，这些服务经过编译后会生成相应语言的代码文件，然后用户实现服务（客户端调用服务，服务器端提服务）便可以了。其中protocol（协议层, 定义数据传输格式，可以为二进制或者XML等）和transport（传输层，定义数据传输方式，可以为TCP/IP传输，内存共享或者文件共享等）被用作运行时库。 Thrift最初由facebook开发，07年四月开放源码，08年5月进入apache孵化器。 Apache Thrift 是一款跨语言的服务框架，传输数据采用二进制格式，相对 XML 和 JSON 体积更小，对于高并发、大数据量和多语言的环境更有优势。 数据类型 Thrift 支持 8 种数据类型： bool: true or false byte: signed byte i16/i32/i64: 16/32/64位 signed integer double: 64位 binary: byte array string 3 种容器： list: 排序数组，可以重复 set: 集合，每个元素唯一 map: t1 唯一 什么是ThriftThrift是一种接口描述语言和二进制通讯协议，它被用来定义和创建跨语言的服务，这是维基百科的描述。简单来说就是你可以按照Thrift定义语法编写.thrift,然后用Thrift命令行生成各种语言的代码，比如OC、Java、C++、JS，调用这些代码就可以完成客户端与服务器的通信了，不需要自己去写网络请求、数据解析等接口。更多详情可以通过这里了解。 为什么使用Thrift在实际项目中主要考虑到这两个优点： RPC。通过简单定义Thrift描述语言文件，使用Thrift -gen命令可以生成多种语言的代码，这些代码包含了网络通信,数据编解码的功能。这就免去了前后台编写这部分繁琐的代码，同时也统一了前后台的实现逻辑。 Thrift的二进制数据的编码比json更加紧凑、减少了无用的数据的传输。这也是本文讨论的重点。 ThriftPy2是Apache Thrift 的纯 python 实现1pip3 install thriftpy2","categories":[],"tags":[]},{"title":"同源策略","slug":"My-blog-62","date":"2018-07-15T09:30:48.000Z","updated":"2019-10-22T12:49:38.962Z","comments":true,"path":"2018/07/15/My-blog-62/","link":"","permalink":"http://yoursite.com/2018/07/15/My-blog-62/","excerpt":"","text":"什么是同源策略：同源策略是禁止javascript进行跨域访问的安全策略，他是浏览器的沙盒环境所提供的一项制约。 什么是相同源：源是一个由&lt;协议，主机，端口&gt;构成的三元组，而两个相同源需要这个三元组的所有属性都相同。 同源策略的好处：限制一个源的脚本从另外一个源加载资源的行为，在对恶意脚本的防护上起到巨大的作用。ps：需要注意的是，同源策略不是禁止脚本的执行，而是禁止读取HTTP的响应。也就是跨域的资源访问请求是允许的，但是服务端返回的数据是禁止读取的。 虽然同源策略对恶意脚本的防护是必要的，但是当我们需要合理的跨域资源访问时，要如何实现： JSONP跨域请求：这不是重点，所以略过。 CORS跨域请求： 1、CORS 的全称是cross-origin sharing stander，跨域资源共享标准 左侧的Origin字段表示的是请求页面所在的域。右侧Access-Control-Allow-Origin是响应头，用于标识允许请求的域。只有当左侧的请求域属于右侧字段定义的域的子域才能进行跨域通信。 此时根据上课的大佬解释，存在CORS漏洞，可以获取到敏感信息，具体自己去看视频吧，菜鸡就不解释了：https://www.bilibili.com/video/av38650052?t=120 &lt;b站漏洞银行咖面&gt;小声bb：有点跑题，以后一定要再写一篇博客。 3、这个标准允许在以下的几个场景中发起跨域请求： 由XMLHttpRequest或Fetch等发起的跨域请求；Web 字体，通过 @font-face 进行跨域调用；WebGL 贴图；使用 drawImage 将 Images/video 画面绘制到 canvas；样式表（使用 CSSOM）；scripts；1234564、CORS进阶：菜鸡已经写不清楚了，所以就转一个大佬的博客吧【菜哭】 CORS通过一些特殊的 HTTP 头来确保哪些源站可以请求哪些资源，除此之外，如果这个请求会对服务器的数据产生修改的可能（这个说法并不是很准确，后面会详细阐述），将会在跨域之前发起一个preflight请求进行检查，如果检查不通过，那么不会发起跨域请求。 首先要明确一点，并不是所有的请求都会触发preflight机制，这些不会触发preflight的请求被称为simple request。符合下列条件的请求，将不会触发preflight机制并视为simple request： GET 请求HEAD 请求Content-Type 为指定值的 POST 请求，包括text/plain，multipart/form-data以及application/x-www-form-urlencodeHTTP 首部字段不能包含下列以外的值：AcceptAccept-LanguageContent-LanguageContent-TypeDPRDownlinkSave-DataViewport-WidthWidth凡是不满足上述条件的请求，将被视为preflight request，并在发起跨域请求前，预先发起一个OPTIONS请求进行检查。在preflight request的返回头中，会包含一些关于是否允许发起跨域的信息。例如我们看这个preflight的例子，这个是向一个地址 POST XML 内容的请求： 在OPTIONS请求中，发出了两个特殊的 HTTP 头，分别是Access-Control-Request-Method和Access-Control-Request-Headers，意思是告诉服务端，我接下来的请求中，会使用POST方法，并且会携带两个自定义的头部字段X-TEST和Content-Type（因为Content-Type的内容并非是我们上文中提到的三种之一，所以被看做是自定义头部）。","categories":[],"tags":[]},{"title":"最近整理的一些有意思的","slug":"My-blog-61","date":"2018-06-20T12:58:12.000Z","updated":"2019-10-22T12:47:27.046Z","comments":true,"path":"2018/06/20/My-blog-61/","link":"","permalink":"http://yoursite.com/2018/06/20/My-blog-61/","excerpt":"","text":"解释型语言和编译型语言的区别a) 编译型语言是在运行之前对源码进行编译，使其成为机器可以识别的机器语言b) 解释型语言不需要编译，而是在语句执行时候才将语句翻译为机器语言c）解释型语言相对于编译型语言来说由于执行时候需要翻译，因此效率比较低Python 解释器种类和特点a）CPython（使用最广）b）IPython（交互式解释器）c）PyPy（显著提高执行效率）d）Jpythone）IronPython位和字节的关系a）1 byte = 8 bitsPython进制转换a）使用int() 十进制：二进制b）hex() 十六进制c）bin() 二进制d）oct() 八进制Python递归的最大层数？a) 998b）可以通过一下代码来设置sys.setrecursionlimit()1ascii、unicode、utf-8、gbk的区别a）ascii是英语字符和二进制数之间的关系。一共规定了128个字符的编码b）Unicode是一个更大的字符和二进制之前的对应关系，一共容纳100多万个字符c）UTF-8是Unicode最广的实现方式d）gbk？字节码和机器码的区别？a）机器码是机器可以识别，可以直接在机器上运行的二进制b）字节码来自源码，由编译器编译源码而形成的二进制文件，可以在不同的运行环境中，通过虚拟运行环境来在机器上执行三元运算规则a）condistion_true if condition else condistion_falseb）用一行代码实现数值交换a）a, b = b, axrange 和 range的区别a）xrange 生成的是一个生成器，range生成的是一个list对象b）要生成很大的数字序列的时候，使用xrange会比使用range性能更优lambda 表达式格式以及应用场景a）lambda x: func_bodyb）常用来作为函数参数输入函数，像是作为sort的key parameterpass的作用a）作为占位符，来保证程序结构的完整性args 和 *kwarg 的作用a）args 是用来传入任意数量的位置参数b）kwarg是传入任意数量的关键字参数is 和 ==的区别a）is 和==都是用来 比较python对象的b）python对象包含三个基本元素， id， type（数据类型）， value（值）c）is 比较对象id， 判断对象是否为同一实例对象，是否指向同一内存地址e）== 判断是，两个对象的内容是否相等python的深浅拷贝和应用场景a）浅拷贝：创造新的对象b）深拷贝：使用旧的对象c）copy.copy() 创造浅拷贝， 注意有的时候会是深拷贝d）copy.deepcopy() 创造深拷贝Python 的垃圾回收机制a) 引用计数机制为主，标记-清除和分代收集为辅常见的内置函数a）abs(), all(), set(), any(), dict(), dir(),b）enumerate(), eval()c）filter(), map(), float(), input(), len(),e) open()，round(), sorted()","categories":[],"tags":[]},{"title":"TCP和UDP的区别？边缘触发和水平触发的区别","slug":"My-blog-60","date":"2018-05-31T13:38:09.000Z","updated":"2019-10-22T12:44:55.725Z","comments":true,"path":"2018/05/31/My-blog-60/","link":"","permalink":"http://yoursite.com/2018/05/31/My-blog-60/","excerpt":"","text":"a. 基本区别： 基于连接与无连接TCP要求系统资源较多，UDP较少；UDP程序结构较简单流模式（TCP）与数据报模式(UDP);TCP保证数据正确性，UDP可能丢包TCP保证数据顺序，UDP不保证b. 编程中的区别 socket()的参数不同UDP Server不需要调用listen和acceptUDP收发数据用sendto/recvfrom函数TCP：地址信息在connect/accept时确定UDP：在sendto/recvfrom函数中每次均 需指定地址信息UDP：shutdown函数无效","categories":[],"tags":[]},{"title":"select和epoll的区别","slug":"My-blog-59","date":"2018-05-06T10:18:01.000Z","updated":"2019-10-22T12:44:03.744Z","comments":true,"path":"2018/05/06/My-blog-59/","link":"","permalink":"http://yoursite.com/2018/05/06/My-blog-59/","excerpt":"","text":"a. select实现需要自己不断轮询所有fd集合，直到设备就绪，期间可能要睡眠和唤醒多次交替。而epoll其实也需要调用epoll_wait不断轮询就绪链表，期间也可能多次睡眠和唤醒交替，但是它是设备就绪时，调用回调函数，把就绪fd放入就绪链表中，并唤醒在epoll_wait中进入睡眠的进程。虽然都要睡眠和交替，但是select在“醒着”的时候要遍历整个fd集合，而epoll在“醒着”的时候只要判断一下就绪链表是否为空就行了，这节省了大量的CPU时间。 b. select每次调用都要把fd集合从用户态往内核态拷贝一次，并且要把current往设备等待队列中挂一次，而epoll只要一次拷贝，而且把current往等待队列上挂也只挂一次（在epoll_wait的开始，注意这里的等待队列并不是设备等待队列，只是一个epoll内部定义的等待队列）。这也能节省不少的开销。","categories":[],"tags":[]},{"title":"多进程与多线程的区别？","slug":"My-blog-58","date":"2018-04-12T08:45:55.000Z","updated":"2019-10-22T12:43:18.930Z","comments":true,"path":"2018/04/12/My-blog-58/","link":"","permalink":"http://yoursite.com/2018/04/12/My-blog-58/","excerpt":"","text":"a. 简而言之,一个程序至少有一个进程，一个进程至少有一个线程。 b. 线程的划分尺度小于进程，使得多线程程序的并发性高。 c. 另外，进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率。 d. 线程在执行过程中与进程还是有区别的。每个独立的线程有一个程序运行的入口、顺序执行序列和程序的出口。但是线程不能够独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 e. 从逻辑角度来看，多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理以及资源分配。这就是进程和线程的重要区别","categories":[],"tags":[]},{"title":"如何对查询命令进行优化？","slug":"My-blog-57","date":"2018-04-03T08:04:22.000Z","updated":"2019-10-22T12:42:43.762Z","comments":true,"path":"2018/04/03/My-blog-57/","link":"","permalink":"http://yoursite.com/2018/04/03/My-blog-57/","excerpt":"","text":"a. 应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索。 b. 应尽量避免在 where 子句中对字段进行 null 值判断，避免使用!=或&lt;&gt;操作符，避免使用 or 连接条件，或在where子句中使用参数、对字段进行表达式或函数操作，否则会导致权标扫描 c. 不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。 d. 使用索引字段作为条件时，如果该索引是复合索引，那么必须使用到该索引中的第一个字段作为条件时才能保证系统使用该索引，否则该索引将不会被使用。 e. 很多时候可考虑用 exists 代替 in f. 尽量使用数字型字段 g. 尽可能的使用 varchar/nvarchar 代替 char/nchar h. 任何地方都不要使用 select * from t ，用具体的字段列表代替“*”，不要返回用不到的任何字段。 i. 尽量使用表变量来代替临时表。 j. 避免频繁创建和删除临时表，以减少系统表资源的消耗。 k. 尽量避免使用游标，因为游标的效率较差。 l. 在所有的存储过程和触发器的开始处设置 SET NOCOUNT ON ，在结束时设置 SET NOCOUNT OFF m. 尽量避免大事务操作，提高系统并发能力。 n. 尽量避免向客户端返回大数据量，若数据量过大，应该考虑相应需求是否合理。","categories":[],"tags":[]},{"title":"python特殊方法(魔术方法)","slug":"My-blog-56","date":"2018-03-21T09:07:12.000Z","updated":"2019-10-22T12:41:09.320Z","comments":true,"path":"2018/03/21/My-blog-56/","link":"","permalink":"http://yoursite.com/2018/03/21/My-blog-56/","excerpt":"","text":"构造和初始化 12345678910111213141516171819每个人都知道一个最基本的魔术方法， __init__ 。通过此方法我们可以定义一个对象的初始操作。然而，当我调用 x = SomeClass() 的时候， __init__ 并不是第一个被调用的方法。实际上，还有一个叫做 __new__ 的方法，来构造这个实例。然后给在开始创建时候的初始化函数来传递参数。在对象生命周期的另一端，也有一个 __del__ 方法。我们现在来近距离的看一看这三个方法:__new__(cls, [...) __new__ 是在一个对象实例化的时候所调用的第一个方法。它的第一个参数是这个类，其他的参数是用来直接传递给 __init__ 方法。 __new__ 方法相当不常用,但是它有自己的特性，特别是当继承一个不可变的类型比如一个tuple或者string。我不希望在 __new__ 上有太多细节，因为并不是很有用处，但是在 Python文档 中有详细的阐述。__init__(self, […) 此方法为类的初始化方法。当构造函数被调用的时候的任何参数都将会传给它。(比如如果我们调用 x = SomeClass(10, &apos;foo&apos;))，那么 __init__ 将会得到两个参数10和foo。 __init__ 在Python的类定义中被广泛用到。__del__(self) 如果 __new__ 和 __init__ 是对象的构造器的话，那么 __del__ 就是析构器。它不实现语句 del x (以上代码将不会翻译为 x.__del__() )。它定义的是当一个对象进行垃圾回收时候的行为。当一个对象在删除的时需要更多的清洁工作的时候此方法会很有用，比如套接字对象或者是文件对象。注意，如果解释器退出的时候对象还存存在，就不能保证 __del__ 能够被执行，所以 __del__ can’t serve as a replacement for good coding practices ()~~~~~~~放在一起的话，这里是一个 __init__ 和 __del__ 实际使用的例子 123456789101112from os.path import joinclass FileObject: &apos;&apos;&apos;给文件对象进行包装从而确认在删除时文件流关闭&apos;&apos;&apos; def __init__(self, filepath=&apos;~&apos;, filename=&apos;sample.txt&apos;): #读写模式打开一个文件 self.file = open(join(filepath, filename), &apos;r+&apos;) def __del__(self): self.file.close() del self.file","categories":[],"tags":[]},{"title":"单元测试","slug":"My-blog-55","date":"2018-03-11T13:02:09.000Z","updated":"2019-10-22T12:40:17.107Z","comments":true,"path":"2018/03/11/My-blog-55/","link":"","permalink":"http://yoursite.com/2018/03/11/My-blog-55/","excerpt":"","text":"传统测试无非就是自己运行一下程序查看结果，或者前后端服务进行联调，这里要说的是走正规流程的单元测试，那到底什么是单元测试呢？顾名思义，只测试当前单元的程序或者代码，也可以理解当前模块的代码块，单元测试假设所有的内部或外部的依赖应该是稳定的, 已经在别处进行测试过的.使用mock 就可以对外部依赖组件实现进行模拟并且替换掉, 从而使得单元测试将焦点只放在当前的单元功能。 简单地说，mock就是帮我们解决测试依赖的一个模块，在Python3中，mock已经被集成到了unittest单元测试框架中，所以不需要单独安装，可以直接使用。","categories":[],"tags":[]},{"title":"python中的设计模式","slug":"My-blog-54","date":"2018-02-18T10:49:47.000Z","updated":"2019-10-22T12:39:43.384Z","comments":true,"path":"2018/02/18/My-blog-54/","link":"","permalink":"http://yoursite.com/2018/02/18/My-blog-54/","excerpt":"","text":"Python设计模式 设计模式的定义:为了解决面向对象系统中重要和重复的设计封装在一起的一种代码实现框架,可以使得代码更加易于扩展和调用 四个基本要素:模式名称,问题,解决方案,效果 六大原则: 1.开闭原则:一个软件实体,如类,模块和函数应该对扩展开发,对修改关闭.既软件实体应尽量在不修改原有代码的情况下进行扩展. 1234567892.里氏替换原则:所有引用父类的方法必须能透明的使用其子类的对象3.依赖倒置原则:高层模块不应该依赖底层模块,二者都应该依赖其抽象,抽象不应该依赖于细节,细节应该依赖抽象,换而言之,要针对接口编程而不是针对实现编程4.接口隔离原则:使用多个专门的接口,而不是使用单一的总接口,即客户端不应该依赖那些并不需要的接口5.迪米特法则:一个软件实体应该尽可能的少与其他实体相互作用6.单一直责原则:不要存在多个导致类变更的原因.即一个类只负责一项职责 接口定义:一种特殊的类,声明了若干方法,要求继承该接口的类必须实现这种方法 作用:限制继承接口的类的方法的名称及调用方式,隐藏了类的内部实现 12345678910111213141516171819from abc import ABCMeta,abstractmethodclass Payment(metaclass=ABCMeta): @abstractmethod#定义抽象方法的关键字 def pay(self,money): pass # @abstractmethod # def pay(self,money): # raise NotImplementedErrorclass AiliPay(Payment): #子类继承接口,必须实现接口中定义的抽象方法,否则不能实例化对象 def pay(self,money): print(&apos;使用支付宝支付%s元&apos;%money)class ApplePay(Payment): def pay(self,money): print(&apos;使用苹果支付支付%s元&apos;%money) 单例模式定义:保证一个类只有一个实例,并提供一个访问它的全局访问点 适用场景:当一个类只能有一个实例而客户可以从一个众所周知的访问点访问它时 优点:对唯一实例的受控访问,相当于全局变量,但是又可以防止此变量被篡改 12345678910111213141516171819202122class Singleton(object): #如果该类已经有了一个实例则直接返回,否则创建一个全局唯一的实例 def __new__(cls, *args, **kwargs): if not hasattr(cls,&apos;_instance&apos;): cls._instance = super(Singleton,cls).__new__(cls) return cls._instanceclass MyClass(Singleton): def __init__(self,name): if name: self.name = namea = MyClass(&apos;a&apos;)print(a)print(a.name)b = MyClass(&apos;b&apos;)print(b)print(b.name)print(a)print(a.name) 工厂模式（celery）定义:不直接向客户暴露对象创建的实现细节,而是通过一个工厂类来负责创建产品类的实例 角色:工厂角色,抽象产品角色,具体产品角色 优点:隐藏了对象创建代码的细节,客户端不需要修改代码 缺点:违反了单一职责原则,将创建逻辑集中到一个工厂里面,当要添加新产品时,违背了开闭原则 1234567891011121314151617181920212223242526272829303132333435363738394041from abc import ABCMeta,abstractmethodclass Payment(metaclass=ABCMeta): #抽象产品角色 @abstractmethod def pay(self,money): passclass AiliPay(Payment): #具体产品角色 def __init__(self,enable_yuebao=False): self.enable_yuebao = enable_yuebao def pay(self,money): if self.enable_yuebao: print(&apos;使用余额宝支付%s元&apos;%money) else: print(&apos;使用支付宝支付%s元&apos;%money)class ApplePay(Payment): # 具体产品角色 def pay(self,money): print(&apos;使用苹果支付支付%s元&apos;%money)class PaymentFactory: #工厂角色 def create_payment(self,method): if method == &apos;alipay&apos;: return AiliPay() elif method == &apos;yuebao&apos;: return AiliPay(True) elif method == &apos;applepay&apos;: return ApplePay() else: return NameErrorp = PaymentFactory()f = p.create_payment(&apos;yuebao&apos;)f.pay(100) 观察者模式定义:定义对象间的一种一对多的依赖关系,当一个对象的状态发生改变时,所有依赖它的对象都会得到通知并被自动更新.观察者模式又称为’发布订阅’模式 角色:抽象主题,具体主题(发布者),抽象观察者,具体观察者(订阅者) 适用场景:当一个抽象模型有两个方面,其中一个方面依赖于另一个方面.将两者封装在独立的对象中以使它们各自独立的改变和复用 当一个对象的改变需要同时改变其他对象,而且不知道具体有多少对象以待改变 当一个对象必须通知其他对象,而又不知道其他对象是谁,即这些对象之间是解耦的 优点:目标和观察者之间的耦合最小,支持广播通信 缺点:多个观察者之间互不知道对方的存在,因此一个观察者对主题的修改可能造成错误的更新 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465from abc import ABCMeta, abstractmethod#抽象主题class Oberserver(metaclass=ABCMeta): @abstractmethod def update(self): pass#具体主题class Notice: def __init__(self): self.observers = [] def attach(self,obs): self.observers.append(obs) def detach(self,obs): self.observers.remove(obs) def notify(self): for obj in self.observers: obj.update(self)#抽象观察者class ManagerNotice(Notice): def __init__(self,company_info=None): super().__init__() self.__company_info = company_info @property def company_info(self): return self.__company_info @company_info.setter def company_info(self,info): self.__company_info = info self.notify()#具体观察者class Manager(Oberserver): def __init__(self): self.company_info = None def update(self,noti): self.company_info = noti.company_info#消息订阅-发送notice = ManagerNotice()alex=Manager()tony=Manager()notice.attach(alex)notice.attach(tony)notice.company_info=&quot;公司运行良好&quot;print(alex.company_info)print(tony.company_info)notice.company_info=&quot;公司将要上市&quot;print(alex.company_info)print(tony.company_info)notice.detach(tony)notice.company_info=&quot;公司要破产了，赶快跑路&quot;print(alex.company_info)print(tony.company_info)","categories":[],"tags":[]},{"title":"python中的高阶函数","slug":"My-blog-53","date":"2018-02-03T04:32:31.000Z","updated":"2019-10-22T12:38:43.298Z","comments":true,"path":"2018/02/03/My-blog-53/","link":"","permalink":"http://yoursite.com/2018/02/03/My-blog-53/","excerpt":"","text":"1、map一般情况map()函数接收两个参数，一个函数（该函数接收一个参数），一个序列，将传入的函数依次作用到序列的每个元素，并返回一个新的Iterator（迭代器）。 例如有这样一个list：[‘pYthon’, ‘jaVa’, ‘kOtlin’],现在要把list中每个元素首字母改为大写，其它的改为小写，可以这样操作： 123456&gt;&gt;&gt; def f(s):... return s.title()... &gt;&gt;&gt; l = map(f, [&apos;pYthon&apos;, &apos;jaVa&apos;, &apos;kOtlin&apos;])&gt;&gt;&gt; list(l)[&apos;Python&apos;, &apos;Java&apos;, &apos;Kotlin&apos;] 2、reduce和map()用法类似，reduce把传入的函数作用在一个序列上，但传入的函数需要接收两个参数，传入函数的计算结果继续和序列的下一个元素做累积计算。 例如有一个list，里边的元素都是字符串，要把它拼接成一个字符串： 123456&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def f(x, y):... return x + y... &gt;&gt;&gt; reduce(f, [&apos;ab&apos;, &apos;c&apos;, &apos;de&apos;, &apos;f&apos;])&apos;abcdef&apos; 3、filterfilter()同样接收一个函数和一个序列，然后把传入的函数依次作用于序列的每个元素，如果传入的函数返回true则保留元素，否则丢弃，最终返回一个Iterator。 例如一个list中元素有纯字母、纯数字、字母数字组合的，我们要保留纯字母的： 123456&gt;&gt;&gt; def f(s):... return s.isalpha()... &gt;&gt;&gt; l = filter(f, [&apos;abc&apos;, &apos;xyz&apos;, &apos;123kg&apos;, &apos;666&apos;])&gt;&gt;&gt; list(l)[&apos;abc&apos;, &apos;xyz&apos;] 4、sortedsorted()函数就是用来排序的，同时可以自己定义排序的规则。 123456789101112131415161718&gt;&gt;&gt; sorted([6, -2, 4, -1])[-2, -1, 4, 6]&gt;&gt;&gt; sorted([6, -2, 4, -1], key=abs)[-1, -2, 4, 6]&gt;&gt;&gt; sorted([6, -2, 4, -1], key=abs, reverse=True)[6, 4, -2, -1]&gt;&gt;&gt; sorted([&apos;Windows&apos;, &apos;iOS&apos;, &apos;Android&apos;])[&apos;Android&apos;, &apos;Windows&apos;, &apos;iOS&apos;]&gt;&gt;&gt; d = [(&apos;Tom&apos;, 170), (&apos;Jim&apos;, 175), (&apos;Andy&apos;, 168), (&apos;Bob&apos;, 185)]&gt;&gt;&gt; def by_height(t):... return t[1]... &gt;&gt;&gt; sorted(d, key=by_height) [(&apos;Andy&apos;, 168), (&apos;Tom&apos;, 170), (&apos;Jim&apos;, 175), (&apos;Bob&apos;, 185)]","categories":[],"tags":[]},{"title":"上下文管理(with)","slug":"My-blog-52","date":"2018-01-22T11:15:24.000Z","updated":"2019-10-22T12:37:53.802Z","comments":true,"path":"2018/01/22/My-blog-52/","link":"","permalink":"http://yoursite.com/2018/01/22/My-blog-52/","excerpt":"","text":"with是一种上下文管理协议，目的在于从流程图中把 try,except 和finally 关键字和资源分配释放相关代码统统去掉，简化try….except….finlally的处理流程。with通过enter方法初始化，然后在exit中做善后以及处理异常。所以使用with处理的对象必须有enter()和exit()这两个方法。其中enter()方法在语句体（with语句包裹起来的代码块）执行之前进入运行，exit()方法在语句体执行完毕退出后运行。 with 语句适用于对资源进行访问的场合，确保不管使用过程中是否发生异常都会执行必要的“清理”操作，释放资源，比如文件使用后自动关闭、线程中锁的自动获取和释放等。 with的使用场景 如果某项工作完成后需要有释放资源或者其他清理工作，比如说文件操作时，就可以使用with优雅的处理，不用自己手动关闭文件句柄，而且with还能很好的管理上下文异常。 123with open(&apos;a.py&apos;) as fp: for line in fp: print(line,end=&quot; &quot;) with工作原理 with后面的语句被求值后，该语句返回的对象的enter()方法被调用，这个方法将返回的值赋给as后面的变量，当with包围的语句块全部执行完毕后，自动调用对象的exit()方法。 with处理异常会更方便，省去try…else…finally复杂的流程，这个用到的就是对象的exit()方法： exit( exc_type, exc_value, exc_trackback) 后面三个参数是固定的，用来接收with执行过程中异常类型，异常名称和异常的详细信息 其他场景应用 123456789101112131415import contextlib@contextlib.contextmanagerdef database(): db = Database() try: if not db.connected: db.connect() yield db except Exception as e: db.close()def handle_query(): with database() as db: print &apos;handle ---&apos;, db.query()","categories":[],"tags":[]},{"title":"类方法和静态方法","slug":"My-blog-51","date":"2018-01-07T09:36:14.000Z","updated":"2019-10-22T12:37:19.258Z","comments":true,"path":"2018/01/07/My-blog-51/","link":"","permalink":"http://yoursite.com/2018/01/07/My-blog-51/","excerpt":"","text":"普通实例方法，第一个参数需要是self，它表示一个具体的实例本身。 如果用了staticmethod，那么就可以无视这个self，而将这个方法当成一个普通的函数使用。 而对于classmethod，它的第一个参数不是self，是cls，它表示这个类本身。 @classmethod修饰符对应的函数不需要实例化，不需要self参数，第一个参数需要是表示自身类的cls参数，cls参数可以用来调用类的属性，类的方法，实例化对象等。 @staticmethod返回函数的静态方法，该方法不强制要求传递参数 示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Foo(object): &quot;&quot;&quot;类三种方法语法形式&quot;&quot;&quot; def instance_method(self): print(&quot;是类&#123;&#125;的实例方法，只能被实例对象调用&quot;.format(Foo)) @staticmethod def static_method(): print(&quot;是静态方法&quot;) @classmethod def class_method(cls): print(&quot;是类方法&quot;)foo = Foo()foo.instance_method()foo.static_method()foo.class_method()print(&apos;----------------&apos;)Foo.static_method()Foo.class_method()class C(object): @staticmethod def f(): print(&apos;runoob&apos;);C.f(); # 静态方法无需实例化cobj = C()cobj.f() # 也可以实例化后调用class A(object): # 属性默认为类属性（可以给直接被类本身调用） num = &quot;类属性&quot; # 实例化方法（必须实例化类之后才能被调用） def func1(self): # self : 表示实例化类后的地址id print(&quot;func1&quot;) print(self) # 类方法（不需要实例化类就可以被类本身调用） @classmethod def func2(cls): # cls : 表示没用被实例化的类本身 print(&quot;func2&quot;) print(cls) print(cls.num) cls().func1() # 不传递默认self参数的方法（该方法也是可以直接被类调用的，但是这样做不标准） def func3(): print(&quot;func3&quot;) print(A.num) # 属性是可以直接用类本身调用的","categories":[],"tags":[]},{"title":"生成器和迭代器","slug":"My-blog-50","date":"2017-12-11T13:37:09.000Z","updated":"2019-10-22T12:35:50.718Z","comments":true,"path":"2017/12/11/My-blog-50/","link":"","permalink":"http://yoursite.com/2017/12/11/My-blog-50/","excerpt":"","text":"1.迭代的概念上一次输出的结果为下一次输入的初始值，重复的过程称为迭代,每次重复即一次迭代，并且每次迭代的结果是下一次迭代的初始值 2.可迭代的对象内置iter方法的，都是可迭代的对象。 list是可迭代对象，dict是可迭代对象，set也是可迭代对象。 3.迭代器1.为什么要有迭代器？对于没有索引的数据类型，必须提供一种不依赖索引的迭代方式。 2.迭代器定义：迭代器：可迭代对象执行iter方法，得到的结果就是迭代器，迭代器对象有next方法 它是一个带状态的对象，他能在你调用next()方法的时候返回容器中的下一个值，任何实现了iter和next()方法的对象都是迭代器，iter返回迭代器自身，next返回容器中的下一个值，如果容器中没有更多元素了，则抛出StopIteration异常 二、生成器1.定义生成器(generator)是一个特殊的迭代器，它的实现更简单优雅，yield是生成器实现next()方法的关键。它作为生成器执行的暂停恢复点，可以对yield表达式进行赋值，也可以将yield表达式的值返回。 也就是说，yield是一个语法糖，内部实现支持了迭代器协议，同时yield内部是一个状态机，维护着挂起和继续的状态。 yield的功能：1.相当于为函数封装好iter和next 2.return只能返回一次值，函数就终止了，而yield能返回多次值，每次返回都会将函数暂停，下一次next会从上一次暂停的位置继续执行 为什么说生成器是一种迭代器？Python 判断一个对象是否是迭代器的标准是看这个对象是否遵守迭代器协议 ，判断一个对象是否遵守迭代器协议主要看两个方面： 1对象首先得实现 iter 和 next 方法 2其次iter 方法必须返回它自己 而生成器恰好满足了这两个条件（可以自己写个生成器，然后调用生成器的这两个方法试试）。我们平常还会经常碰到另外一个概念：可迭代对象。可迭代对象就是可迭代的对象，可迭代的对象就是说我们可以从这个对象拿到一个迭代器。在 Python 中，iter 方法可以帮我们完成这个事情，也就是说，可迭代对象和迭代器满足这样一个关系：iter(iterable) -&gt; iterator。 在 Python 中，list 是个可迭代对象，所以我们经常会写这样的代码： 123&gt;&gt;&gt; l = [1, 2, 3]&gt;&gt;&gt; for element in l:... print(element) 但你想过为什么我们可以这么写吗？为啥在 c 语言里面，我们访问数组元素的时候，必须要通过 index? 因为：list 是个可迭代对象，我们在 Python 中使用 for … in 时，Python 会给我们生成一个迭代器对象，而如上所说：迭代器是个数据流，它可以产生数据，我们一直从里面取数据就好了，而不需要我们在代码中维护 index，Python 已经通过迭代器给我们完成了这个事情。","categories":[],"tags":[]},{"title":"如何避免GIL带来的影响","slug":"My-blog-49","date":"2017-11-23T08:28:03.000Z","updated":"2019-10-22T12:34:24.959Z","comments":true,"path":"2017/11/23/My-blog-49/","link":"","permalink":"http://yoursite.com/2017/11/23/My-blog-49/","excerpt":"","text":"方法一：用进程+协程 代替 多线程的方式 在多进程中，由于每个进程都是独立的存在，所以每个进程内的线程都拥有独立的GIL锁，互不影响。但是，由于进程之间是独立的存在，所以进程间通信就需要通过队列的方式来实现。 方法二：更换解释器像JPython和IronPython这样的解析器由于实现语言的特性，他们不需要GIL的帮助。然而由于用了Java/C#用于解析器实现，他们也失去了利用社区众多C语言模块有用特性的机会。所以这些解析器也因此一直都比较小众。 一些其他的解释器CPython 当我们从Python官方网站下载并安装好Python 2.7后，我们就直接获得了一个官方版本的解释器：CPython。这个解释器是用C语言开发的，所以叫CPython。在命令行下运行python就是启动CPython解释器。 CPython是使用最广的Python解释器。教程的所有代码也都在CPython下执行。 IPython IPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强，但是执行Python代码的功能和CPython是完全一样的。好比很多国产浏览器虽然外观不同，但内核其实都是调用了IE。 CPython用&gt;&gt;&gt;作为提示符，而IPython用In [序号]:作为提示符。 PyPy PyPy是另一个Python解释器，它的目标是执行速度。PyPy采用JIT技术，对Python代码进行动态编译（注意不是解释），所以可以显著提高Python代码的执行速度。 绝大部分Python代码都可以在PyPy下运行，但是PyPy和CPython有一些是不同的，这就导致相同的Python代码在两种解释器下执行可能会有不同的结果。如果你的代码要放到PyPy下执行，就需要了解PyPy和CPython的不同点。 Jython Jython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行。 IronPython IronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器，可以直接把Python代码编译成.Net的字节码。","categories":[],"tags":[]},{"title":"Python中 进程 线程 协程","slug":"My-blog-48","date":"2017-11-04T11:27:32.000Z","updated":"2019-10-22T12:32:52.602Z","comments":true,"path":"2017/11/04/My-blog-48/","link":"","permalink":"http://yoursite.com/2017/11/04/My-blog-48/","excerpt":"","text":"进程，是执行中的计算机程序。也就是说，每个代码在执行的时候，首先本身即是一个进程。一个进程具有:就绪，运行，中断，僵死，结束等状态(不同操作系统不一样)。 生命周期： 用户编写代码(代码本身是以进程运行的) 启动程序，进入进程“就绪”状态 操作系统调度资源，做“程序切换”，使得进程进入“运行”状态 结束/中断 特性 每个程序，本身首先是一个进程 运行中每个进程都拥有自己的地址空间、内存、数据栈及其它资源。 操作系统本身自动管理着所有的进程(不需要用户代码干涉)，并为这些进程合理分配可以执行时间。 进程可以通过派生新的进程来执行其它任务，不过每个进程还是都拥有自己的内存和数据栈等。 进程间可以通讯(发消息和数据)，采用 进程间通信(IPC) 方式。 说明 多个进程可以在不同的 CPU 上运行，互不干扰 同一个CPU上，可以运行多个进程，由操作系统来自动分配时间片 由于进程间资源不能共享，需要进程间通信，来发送数据，接受消息等 多进程，也称为“并行”。 进程间通信进程彼此之间互相隔离，要实现进程间通信（IPC），multiprocessing模块支持两种形式：队列和管道，这两种方式都是使用消息传递的。 进程队列queue不同于线程queue，进程queue的生成是用multiprocessing模块生成的。 在生成子进程的时候，会将代码拷贝到子进程中执行一遍，及子进程拥有和主进程内容一样的不同的名称空间。 multiprocess.Queue 是跨进程通信队列 常用方法 1234567q.put方法用以插入数据到队列中，put方法还有两个可选参数：blocked和timeout。如果blocked为True（默认值），并且timeout为正值，该方法会阻塞timeout指定的时间，直到该队列有剩余的空间。如果超时，会抛出Queue.Full异常。如果blocked为False，但该Queue已满，会立即抛出Queue.Full异常。q.get方法可以从队列读取并且删除一个元素。同样，get方法有两个可选参数：blocked和timeout。如果blocked为True（默认值），并且timeout为正值，那么在等待时间内没有取到任何元素，会抛出Queue.Empty异常。如果blocked为False，有两种情况存在，如果Queue有一个值可用，则立即返回该值，否则，如果队列为空，则立即抛出Queue.Empty异常.q.get_nowait():同q.get(False)q.put_nowait():同q.put(False)q.empty():调用此方法时q为空则返回True，该结果不可靠，比如在返回True的过程中，如果队列中又加入了项目。q.full()：调用此方法时q已满则返回True，该结果不可靠，比如在返回True的过程中，如果队列中的项目被取走。q.qsize():返回队列中目前项目的正确数量，结果也不可靠，理由同q.empty()和q.full()一样 管道pipe管道就是管道，就像生活中的管道，两头都能进能出 默认管道是全双工的，如果创建管道的时候映射成False，左边只能用于接收，右边只能用于发送，类似于单行道 123456789101112import multiprocessingdef foo(sk): sk.send(&apos;hello world&apos;) print(sk.recv())if __name__ == &apos;__main__&apos;: conn1,conn2=multiprocessing.Pipe() #开辟两个口，都是能进能出，括号中如果False即单向通信 p=multiprocessing.Process(target=foo,args=(conn1,)) #子进程使用sock口，调用foo函数 p.start() print(conn2.recv()) #主进程使用conn口接收 conn2.send(&apos;hi son&apos;) #主进程使用conn口发送 常用方法 123conn1.recv():接收conn2.send(obj)发送的对象。如果没有消息可接收，recv方法会一直阻塞。如果连接的另外一端已经关闭，那么recv方法会抛出EOFError。conn1.send(obj):通过连接发送对象。obj是与序列化兼容的任意对象注意：send()和recv()方法使用pickle模块对对象进行序列化 共享数据manageQueue和pipe只是实现了数据交互，并没实现数据共享，即一个进程去更改另一个进程的数据。 注：进程间通信应该尽量避免使用共享数据的方式 进程池开多进程是为了并发，通常有几个cpu核心就开几个进程，但是进程开多了会影响效率，主要体现在切换的开销，所以引入进程池限制进程的数量。 进程池内部维护一个进程序列，当使用时，则去进程池中获取一个进程，如果进程池序列中没有可供使用的进进程，那么程序就会等待，直到进程池中有可用进程为止。 线程线程，是在进程中执行的代码。 一个进程下可以运行多个线程，这些线程之间共享主进程内申请的操作系统资源。 在一个进程中启动多个线程的时候，每个线程按照顺序执行。现在的操作系统中，也支持线程抢占，也就是说其它等待运行的线程，可以通过优先级，信号等方式，将运行的线程挂起，自己先运行。 使用 用户编写包含线程的程序(每个程序本身都是一个进程) 操作系统“程序切换”进入当前进程 当前进程包含了线程，则启动线程 多个线程，则按照顺序执行，除非抢占 特性 线程，必须在一个存在的进程中启动运行 线程使用进程获得的系统资源，不会像进程那样需要申请CPU等资源 线程无法给予公平执行时间，它可以被其他线程抢占，而进程按照操作系统的设定分配执行时间 每个进程中，都可以启动很多个线程 说明 多线程，也被称为”并发“执行。 线程池系统启动一个新线程的成本是比较高的，因为它涉及与操作系统的交互。在这种情形下，使用线程池可以很好地提升性能，尤其是当程序中需要创建大量生存期很短暂的线程时，更应该考虑使用线程池。 线程池在系统启动时即创建大量空闲的线程，程序只要将一个函数提交给线程池，线程池就会启动一个空闲的线程来执行它。当该函数执行结束后，该线程并不会死亡，而是再次返回到线程池中变成空闲状态，等待执行下一个函数。 此外，使用线程池可以有效地控制系统中并发线程的数量。当系统中包含有大量的并发线程时，会导致系统性能急剧下降，甚至导致 Python 解释器崩溃，而线程池的最大线程数参数可以控制系统中并发线程的数量不超过此数。 多线程通信共享变量创建全局变量，多个线程公用一个全局变量，方便简单。但是坏处就是共享变量容易出现数据竞争，不是线程安全的，解决方法就是使用互斥锁。 变量共享引申出线程同步问题如果多个线程共同对某个数据修改，则可能出现不可预料的结果，为了保证数据的正确性，需要对多个线程进行同步。 使用Thread对象的Lock和Rlock可以实现简单的线程同步，这两个对象都有acquire方法和release方法，对于那些需要每次只允许一个线程操作的数据，可以将其操作放到acquire和release方法之间。 队列线程间使用队列进行通信，因为队列所有方法都是线程安全的，所以不会出现线程竞争资源的情况 Queue.Queue 是进程内非阻塞队列 进程和线程的区别一个进程中的各个线程与主进程共享相同的资源，与进程间互相独立相比，线程之间信息共享和通信更加容易(都在进程中，并且共享内存等)。 线程一般以并发执行，正是由于这种并发和数据共享机制，使多任务间的协作成为可能。 进程一般以并行执行，这种并行能使得程序能同时在多个CPU上运行; 区别于多个线程只能在进程申请到的的“时间片”内运行(一个CPU内的进程，启动了多个线程，线程调度共享这个进程的可执行时间片)，进程可以真正实现程序的“同时”运行(多个CPU同时运行)。 进程和线程的常用应用场景 一般来说,在Python中编写并发程序的经验: 计算密集型任务使用多进程 IO密集型(如:网络通讯)任务使用多线程，较少使用多进程. 这是由于 IO操作需要独占资源，比如: 网络通讯(微观上每次只有一个人说话，宏观上看起来像同时聊天)每次只能有一个人说话 文件读写同时只能有一个程序操作(如果两个程序同时给同一个文件写入 ‘a’, ‘b’，那么到底写入文件的哪个呢?) 都需要控制资源每次只能有一个程序在使用，在多线程中，由主进程申请IO资源，多线程逐个执行，哪怕抢占了，也是逐个运行，感觉上“多线程”并发执行了。 如果多进程，除非一个进程结束，否则另外一个完全不能用，显然多进程就“浪费”资源了。 当然如上解释可能还不足够立即理解问题所在，让我们通过不断的实操来体验其中的“门道”。 协程协程: 协程，又称微线程，纤程，英文名Coroutine。协程的作用，是在执行函数A时，可以随时中断，去执行函数B，然后中断继续执行函数A（可以自由切换）。但这一过程并不是函数调用（没有调用语句），这一整个过程看似像多线程，然而协程只有一个线程执行. 协程由于由程序主动控制切换，没有线程切换的开销，所以执行效率极高。对于IO密集型任务非常适用，如果是cpu密集型，推荐多进程+协程的方式。 协程，又称微线程。 说明 协程的主要特色是: 协程间是协同调度的，这使得并发量数万以上的时候，协程的性能是远远高于线程。 注意这里也是“并发”，不是“并行”。 常用库：greenlet gevent 协程优点： 1. 协程的切换开销更小，属于程序级别的切换，操作系统完全感知不到，因而更加轻量级 2. 单线程内就可以实现并发的效果，最大限度地利用cpu 协程缺点： 1.协程的本质是单线程下，无法利用多核，可以是一个程序开启多个进程，每个进程内开启多个线程，每个线程内开启协程 2.协程指的是单个线程，因而一旦协程出现阻塞，将会阻塞整个线程 python中的协程一个协程是一个函数/子程序（可以认为函数和子程序是指一个东西）。这个函数可以暂停执行， 把执行权让给 YieldInstruction，等 YieldInstruction 执行完成后，这个函数可以继续执行。 这个函数可以多次这样的暂停与继续。 注：这里的 YieldInstruction, 我们其实也可以简单理解为函数。 协程可以在“卡住”的时候可以干其它事情。 12345678910111213141516171819202122async def long_task():... print(&apos;long task started&apos;)... await asyncio.sleep(1)... print(&apos;long task finished&apos;)...&gt;&gt;&gt; loop.create_task(long_task())&lt;Task pending coro=&lt;long_task() running at &lt;stdin&gt;:1&gt;&gt;&gt;&gt;&gt; loop.create_task(job1()) &gt;&gt;&gt;&gt;&gt; loop.create_task(job1())&lt;Task pending coro=&lt;job1() running at &lt;stdin&gt;:1&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; try:... loop.run_forever()... except KeyboardInterrupt:... pass...long task startedjob1 started...job1 pausedhello worldjob1 resumedjob1 finishedlong task finished 从这段程序的输出可以看出，程序本来是在执行 long task 协程，但由于 long task 要 await sleep 1 秒，于是 long task 自动暂停了，hello_world 协程自动开始执行， hello world 执行完之后，long task 继续执行。 协程有两种定义的方法， 其中使用生成器形式定义的协程叫做 generator-based coroutine, 通过 async/await 声明的协程叫做 native coroutine，两者底层实现都是生成器。接着， 我们阐述了协程的概念，从概念和例子出发，讲了协程和生成器最主要的特征：可以暂停执行和恢复执行。 协程异常处理使用协程的时候一定加了很多的异常,但百密而一疏,总是会有想象不到的异常发生,这个时候为了不让程序整体崩溃应该使用协程的额外异常处理方法,这个方法会去执行绑定的回调函数. 123456789101112131415g_dict=dict&#123;&#125;g = gevent.spawn(self._g_fetcher, feed_name) # 创建协程g_dict[feed_name] = [g,False]g.link_exception(self._link_exception_callback) # 给该协程添加出现处理不了的异常时候的回调函数def _link_exception_callback(self, g): # 可能遇到无法修复的错误，需要修改代码 todo 报警 # 可以在这个函数里面做一些错误异常的打印,或者文件的关闭,连接的关闭. self.terminated_flag = True # 停止整个程序 让 supervior重启 logger.info(&quot;_link_exception_callback &#123;0&#125; &#123;1&#125;&quot;.format(g, g.exception)) self._kill_sleep_gevent() # 轮询结束休眠的协程def _kill_sleep_gevent(self): for i,is_sleep in g_dict.items(): if is_sleep[1] == &quot;True&quot;: gevent.kill(is_sleep[0]) greenlet框架实现协程（封装yield的基础库）greenlet机制的主要思想是：生成器函数或者协程函数中的yield语句挂起函数的执行，直到稍后使用next()或send()操作进行恢复为止。可以使用一个调度器循环在一组生成器函数之间协作多个任务。greentlet是python中实现我们所谓的”Coroutine(协程)”的一个基础库。 基于greenlet框架的高级库gevent模块gevent是第三方库，通过greenlet实现协程，其基本思想是： 当一个greenlet遇到IO操作时，比如访问网络，就自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换回来继续执行。由于IO操作非常耗时，经常使程序处于等待状态，有了gevent为我们自动切换协程，就保证总有greenlet在运行，而不是等待IO。 原生协程协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存，在调度回来的时候，恢复先前保存的寄存器上下文和栈。因此协程能保留上一次调用时的状态，即所有局部状态的一个特定组合 async函数完全可以看作多个异步操作，包装成的一个 Promise 对象，而await命令就是内部then命令的语法糖 123456789101112131415161718192021222324import timedef job(t): time.sleep(t) print(&apos;用了%s&apos; % t)def main(): [job(t) for t in range(1,3)]start = time.time()main()print(time.time()-start)import timeimport asyncioasync def job(t): # 使用 async 关键字将一个函数定义为协程 await asyncio.sleep(t) # 等待 t 秒, 期间切换执行其他任务 print(&apos;用了%s秒&apos; % t)async def main(loop): # 使用 async 关键字将一个函数定义为协程 tasks = [loop.create_task(job(t)) for t in range(1,3)] # 创建任务, 不立即执行 await asyncio.wait(tasks) # 执行并等待所有任务完成start = time.time()loop = asyncio.get_event_loop() # 建立 looploop.run_until_complete(main(loop)) # 执行 looploop.close() # 关闭 loopprint(time.time()-start)","categories":[],"tags":[]},{"title":"python3 dict性能优化","slug":"My-blog-47","date":"2017-10-19T06:16:43.000Z","updated":"2019-10-22T12:31:45.000Z","comments":true,"path":"2017/10/19/My-blog-47/","link":"","permalink":"http://yoursite.com/2017/10/19/My-blog-47/","excerpt":"","text":"节省存储空间：将存储PyDictKeyEntry的稀疏数组更改为存储int的稀疏数组； 之前的dict_entry是稀疏表，经压缩后在密集表上循环，使用更少的内存； 调整大小更快，并且触及更少的内存。 目前，每一个散列/键/值条目在一个过程中被移动或复制调整。 在新的布局中，只有索引是更新。 大多数情况下，散列/键/值条目从不移动（除了偶尔交换填充删除留下的空洞）。","categories":[],"tags":[]},{"title":"关于哈希表","slug":"My-blog-46","date":"2017-10-02T09:30:41.000Z","updated":"2019-10-22T12:30:50.352Z","comments":true,"path":"2017/10/02/My-blog-46/","link":"","permalink":"http://yoursite.com/2017/10/02/My-blog-46/","excerpt":"","text":"散列表概念 散列表（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的数据结构。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做散列函数，存放记录的数组叫做散列表。 哈希函数 给定表M，存在函数f(key)，对任意给定的关键字值key，代入函数后若能得到包含该关键字的记录在表中的地址，则称表M为哈希(Hash）表，函数f(key)为哈希(Hash) 函数。 （相关：是不是可以这样理解，数组可以通过下标进行访问，时间复杂度是O(1)，对于不连续存储的数据结构，如果知道下标也可以直接进行访问，所以可以通过哈希函数将key映射成数组下标，进行访问） 冲突 不同的key经过hash函数运行后得到相同的值，产生冲突； 冲突解决方式 开放寻址：线性探测、二次探测、伪随机数序列（python的dict解决冲突用的这个，具体的策略没有看太明白） 再哈希法：将哈希值再哈希，然后存储； 链地址法：hash过后值相同的存储在链表里； 公共溢出区 python实现dict无序到有序： 原先的内存布局entries为哈希表，表中直接存储PyDictKeyEntry（hash、key、value），也就是说当当前位置为空的时候存的是（0， null， null）浪费了大量内存； python3.6: indices充当哈希表，存储的entries的index，使用index去访问存有PyDictKeyEntry的数组","categories":[],"tags":[]},{"title":"python2和python3的区别","slug":"My-blog-45","date":"2017-09-12T02:39:42.000Z","updated":"2019-10-22T12:30:06.322Z","comments":true,"path":"2017/09/12/My-blog-45/","link":"","permalink":"http://yoursite.com/2017/09/12/My-blog-45/","excerpt":"","text":"print语句被python3废弃，只能使用print函数 Python3中字符串是Unicode (utf-8)编码，支持中文做标识符。 python2中是ASCII编码，需要更改字符集才能正常支持中文，所以在.py文件中会看到#– coding: UTF-8 – 异常处理 Python2中try:…except Exception, e:…，在Python3中改为了try:…except Exception as e:… Python3中不再使用xrange方法，只有range方法。 range在Python2中返回列表，而在Python3中返回range可迭代对象。 在Python2中有两个不等运算符!=和&lt;&gt;，在Python3中去掉了&lt;&gt;，只有!=符号表示不等 在Python2中双反引号可以替代repr函数，在Python3中去掉了双反引号的表是方法，只能用repr方法。 StringIO模块现在被合并到新的io模组内。new, md5, gopherlib等模块被删除。 httplib, BaseHTTPServer, CGIHTTPServer, SimpleHTTPServer, Cookie, cookielib被合并到http包内。 取消了exec语句，只剩下exec()函数。 在Python2中long是比int取值范围更大的整数，Python3中取消了long类型，int的取值范围扩大到之前的long类型范围。 列表推导 不再支持[n for n in a,b]语法，改为[n for n in (a,b)]或[n for n in [a,b]] python 2 中通过input输入的类型是int，只有通过raw_input()输入的类型才是str。 python 3中通过input输入的类型都是str，去掉了row_input()方法。 python3.6中dict有序","categories":[],"tags":[]},{"title":"浅谈python集合","slug":"My-blog-44","date":"2017-08-26T12:29:27.000Z","updated":"2019-10-22T12:28:50.316Z","comments":true,"path":"2017/08/26/My-blog-44/","link":"","permalink":"http://yoursite.com/2017/08/26/My-blog-44/","excerpt":"","text":"集合是一种鲁棒性很好的数据结构，当元素顺序的重要性不如元素的唯一性和测试元素是否包含在集合中的效率时，大部分情况下这种数据结构极其有用。 python的内置集合类型有两种： set(): 一种可变的、无序的、有限的集合，其元素是唯一的、不可变的（可哈希的）对象。 frozenset(): 一种不可变的、可哈希的、无序的集合，其元素是唯一的，不可变的哈希对象。 set里的元素必须是唯一的，不可变的。但是set是可变的，所以set作为set的元素会报错。 实现细节 CPython中集合和字典非常相似。事实上，集合被实现为带有空值的字典，只有键才是实际的集合元素。此外，集合还利用这种没有值的映射做了其它的优化。 由于这一点，可以快速的向集合中添加元素、删除元素、检查元素是否存在。平均时间复杂度为O(1),最坏的事件复杂度是O(n)。","categories":[],"tags":[]},{"title":"字典底层实现","slug":"My-blog-43","date":"2017-08-05T08:41:29.000Z","updated":"2019-10-22T12:26:34.735Z","comments":true,"path":"2017/08/05/My-blog-43/","link":"","permalink":"http://yoursite.com/2017/08/05/My-blog-43/","excerpt":"","text":"字典是python中最通用的数据结构之一。dict可以将一组唯一的键映射到相应的值。 字典推导式 12squares = &#123;number: number**2 for number in range(10)&#125;print(squares) 在遍历字典元素时，有一点需要特别注意。字典里的keys(), values()和items()3个方法的返回值不再是列表，而是视图对象（view objects）。 keys(): 返回dict_keys对象，可以查看字典所有键 values():返回dict_values对象，可以查看字典的所有值 items():返回dict_items对象，可以查看字典所有的{key, value}二元元组。 CPython使用伪随机探测(pseudo-random probing)的散列表(hash table)作为字典的底层数据结构。由于这个实现细节，只有可哈希的对象才能作为字典的键。 Python中所有不可变的内置类型都是可哈希的。可变类型（如列表，字典和集合）就是不可哈希的，因此不能作为字典的键。 字典的三个基本操作（添加元素，获取元素和删除元素）的平均事件复杂度为O(1)，但是他们的平摊最坏情况复杂度要高得多，为O(N). 还有一点很重要，在复制和遍历字典的操作中，最坏的复杂度中的n是字典曾经达到的最大元素数目，而不是当前的元素数目。换句话说，如果一个字典曾经元素个数很多，后来又大大减小了，那么遍历这个字典可能会花费相当长的事件。因此在某些情况下，如果需要频繁的遍历某个词典，那么最好创建一个新的字典对象，而不是仅在旧字典中删除元素。 字典的缺点和替代方案 使用字典的常见陷阱就是，它并不会按照键的添加顺序来保存元素的顺序。在某些情况下，字典的键是连续的，对应的散列值也是连续值（例如整数），那么由于字典的内部实现，元 素的实现可能和添加的顺序相同： 12keys = &#123;num: None for num in range(5)&#125;.keys()print(keys) 如果我们需要保存添加顺序怎么办？python 标准库的collections模块提供了名为OrderedDicr的有序字典","categories":[],"tags":[]},{"title":"列表底层实现","slug":"My-blog-42","date":"2017-08-05T08:36:26.000Z","updated":"2019-10-22T12:25:49.016Z","comments":true,"path":"2017/08/05/My-blog-42/","link":"","permalink":"http://yoursite.com/2017/08/05/My-blog-42/","excerpt":"","text":"实现细节 python中的列表的英文名是list，因此很容易和其它语言(C++, Java等)标准库中常见的链表混淆。事实上CPython的列表根本不是列表（可能换成英文理解起来容易些：python中的list不是list）。在CPython中，列表被实现为长度可变的数组。 从细节上看，Python中的列表是由对其它对象的引用组成的连续数组。指向这个数组的指针及其长度被保存在一个列表头结构中。这意味着，每次添加或删除一个元素时，由引用组成的数组需要该标大小（重新分配）。幸运的是，Python在创建这些数组时采用了指数过分配，所以并不是每次操作都需要改变数组的大小。但是，也因为这个原因添加或取出元素的平摊复杂度较低。 不幸的是，在普通链表上“代价很小”的其它一些操作在Python中计算复杂度相对过高。 利用 list.insert方法在任意位置插入一个元素——复杂度O(N) 利用 list.delete或del删除一个元素——复杂度O(N) 列表推导要习惯用列表推导，因为这更加高效和简短，涉及的语法元素少。在大型的程序中，这意味着更少的错误，代码也更容易阅读。 12&gt;&gt;&gt;[i for i in range(10) if i % 2 == 0] [0, 2, 4, 6, 8] 1.使用enumerate.在循环使用序列时，这个内置函数可以方便的获取其索引： 12for i, element in enumerate([&apos;one&apos;, &apos;two&apos;, &apos;three&apos;]): print(i, element)","categories":[],"tags":[]},{"title":"列表和字典的区别","slug":"My-blog-40","date":"2017-06-01T00:09:26.000Z","updated":"2019-10-22T12:22:36.380Z","comments":true,"path":"2017/06/01/My-blog-40/","link":"","permalink":"http://yoursite.com/2017/06/01/My-blog-40/","excerpt":"","text":"列表是序列，可以理解为数据结构中的数组，字典可以理解为数据结构中的hashmap 他俩都可以作为集合来存储数据 从差异特征上来说 list是有序的，dict是无需的 list通过索引访问，dict使用key访问 list随着数量的正常增长要想查找元素的时间复杂度为O(n), dict不随数量而增长而变化，时间负责都为O(1) dict的占用内存稍比list大，会在1.5倍左右 特征决定用途： list一般可作为队列、堆栈使用，而dict一般作为聚合统计或者快速使用特征访问等","categories":[],"tags":[]},{"title":"列表和元组之间的区别","slug":"My-blog-39","date":"2017-05-28T07:19:10.000Z","updated":"2019-10-22T12:21:47.168Z","comments":true,"path":"2017/05/28/My-blog-39/","link":"","permalink":"http://yoursite.com/2017/05/28/My-blog-39/","excerpt":"","text":"二者的主要区别是列表是可变的，而元组是不可变的 不同点一：不可变 VS 可变 两种类型除了字面上的区别(括号与方括号)之外，最重要的一点是tuple是不可变类型，大小固定，而 list 是可变类型、数据可以动态变化，这种差异使得两者提供的方法、应用场景、性能上都有很大的区别。 不同点二：同构 VS 异构 tuple 用于存储异构(heterogeneous)数据，当做没有字段名的记录来用，比如用 tuple 来记录一个人的身高、体重、年龄。 person = (“zhangsan”, 20, 180, 80) 比如记录坐标上的某个点 point = (x, y) 而列表一般用于存储同构数据(homogenous)，同构数据就是具有相同意义的数据，比如下面的都是字符串类型 [“zhangsan”, “Lisi”, “wangwu”] 再比如 list 存放的多条用户记录 [(“zhangsan”, 20, 180, 80), (“wangwu”, 20, 180, 80)] 数据库操作中查询出来的记录就是由元组构成的列表结构。 因为 tuple 作为没有名字的记录来使用在某些场景有一定的局限性，所以又有了一个 namedtuple 类型的存在，namedtuple 可以指定字段名，用来当做一种轻量级的类来使用。","categories":[],"tags":[]},{"title":"深拷贝和浅拷贝之间的区别是什么？","slug":"My-blog-38","date":"2017-05-27T10:26:45.000Z","updated":"2019-10-22T12:20:55.466Z","comments":true,"path":"2017/05/27/My-blog-38/","link":"","permalink":"http://yoursite.com/2017/05/27/My-blog-38/","excerpt":"","text":"深拷贝就是将一个对象拷贝到另一个对象中，这意味着如果你对一个对象的拷贝做出改变时，不会影响原对象。在Python中，我们使用函数deepcopy()执行深拷贝 12import copyb=copy.deepcopy(a) 而浅拷贝则是将一个对象的引用拷贝到另一个对象上，所以如果我们在拷贝中改动，会影响到原对象。我们使用函数function()执行浅拷贝 1b=copy.copy(a)","categories":[],"tags":[]},{"title":"Python中是如何管理内存的","slug":"My-blog-37","date":"2017-05-09T14:16:02.000Z","updated":"2019-10-22T12:19:56.883Z","comments":true,"path":"2017/05/09/My-blog-37/","link":"","permalink":"http://yoursite.com/2017/05/09/My-blog-37/","excerpt":"","text":"Python有一个私有堆空间来保存所有的对象和数据结构。作为开发者，我们无法访问它，是解释器在管理它。但是有了核心API后，我们可以访问一些工具。Python内存管理器控制内存分配。 另外，内置垃圾回收器会回收使用所有的未使用内存，所以使其适用于堆空间。 一、垃圾回收：python不像C++，Java等语言一样，他们可以不用事先声明变量类型而直接对变量进行赋值。对Python语言来讲，对象的类型和内存都是在运行时确定的。这也是为什么我们称Python语言为动态类型的原因(这里我们把动态类型可以简单的归结为对变量内存地址的分配是在运行时自动判断变量类型并对变量进行赋值)。 二、引用计数：Python采用了类似Windows内核对象一样的方式来对内存进行管理。每一个对象，都维护这一个对指向该对对象的引用的计数。当变量被绑定在一个对象上的时候，该变量的引用计数就是1，(还有另外一些情况也会导致变量引用计数的增加),系统会自动维护这些标签，并定时扫描，当某标签的引用计数变为0的时候，该对就会被回收。 1 对象存储 在Python中万物皆对象 不存在基本数据类型，0, 1.2, True, False, &quot;abc&quot;等，这些全都是对象 所有对象, 都会在内存中开辟一块空间进行存储 2.1 会根据不同的类型以及内容, 开辟不同的空间大小进行存储 2.2 返回该空间的地址给外界接收(称为”引用”), 用于后续对这个对象的操作 2.3 可通过 id() 函数获取内存地址(10进制) 2.4 通过 hex() 函数可以查看对应的16进制地址 1234567891011121314class Person: passp = Person()print(p)print(id(p))print(hex(id(p)))&gt;&gt;&gt;&gt; 打印结果&lt;__main__.Person object at 0x107030470&gt;44126055520x107030470 对于整数和短小的字符, Python会进行缓存; 不会创建多个相同对象 此时, 被多次赋值, 只会有多份引用 1234567num1 = 2num2 = 2print(id(num1), id(num2))&gt;&gt;&gt;&gt; 打印结果4366584464 4366584464 容器对象, 存储的其他对象, 仅仅是其他对象的引用, 并不是其他对象本身 4.1 比如字典, 列表, 元组这些”容器对象” 4.2 全局变量是由一个大字典进行引用 4.3 可通过 global() 查看 2 对象回收 2.1 引用计数器 2.1.1概念 一个对象, 会记录着自身被引用的个数 每增加一个引用, 这个对象的引用计数会自动+1 每减少一个引用, 这个对象的引用计数会自动-1 引用计数+1场景 1234567891、对象被创建 p1 = Person()2、对象被引用 p2 = p13、对象被作为参数，传入到一个函数中 log(p1) 这里注意会+2, 因为内部有两个属性引用着这个参数4、对象作为一个元素，存储在容器中 l = [p1] 引用计数-1场景 123456781、对象的别名被显式销毁 del p12、对象的别名被赋予新的对象 p1 = 1233、一个对象离开它的作用域 一个函数执行完毕时 内部的局部变量关联的对象, 它的引用计数就会-14、对象所在的容器被销毁，或从容器中删除对象 查看引用计数 123456789101112131415161718192021222324import sysclass Person: passp1 = Person() # 1print(sys.getrefcount(p1)) # 2p2 = p1 # 2print(sys.getrefcount(p1)) # 3del p2 # 1print(sys.getrefcount(p1)) # 2del p1# print(sys.getrefcount(p1)) #error，因为上一行代码执行类p1对象已经销毁&gt;&gt;&gt;&gt; 打印结果232 循环引用 123456789101112# 循环引用class Person: passclass Dog: passp = Person() d = Dog() p.pet = d d.master = p 对象间互相引用，导致对象不能通过引用计数器进行销毁 手动触发垃圾回收，挥手循环引用 123456789101112131415161718192021222324252627import objgraphimport gcclass Person: passclass Dog: passp = Person()d = Dog()p.pet = dd.master = pdel pdel dgc.collect() #手动触发垃圾回收print(objgraph.count(&quot;Person&quot;))print(objgraph.count(&quot;Dog&quot;))&gt;&gt;&gt;&gt; 打印结果00","categories":[],"tags":[]},{"title":"python中的元类","slug":"My-blog-36","date":"2017-04-25T05:21:38.000Z","updated":"2019-10-22T12:18:25.040Z","comments":true,"path":"2017/04/25/My-blog-36/","link":"","permalink":"http://yoursite.com/2017/04/25/My-blog-36/","excerpt":"","text":"元类是类的类对象，换言之类是元类的实例，Python中默认的元类为type，可以通过自定义元类的方式实现对类创建的控制。 123456789101112131415161718class Base: a = 1 b = 2 print(&apos;class defined&apos;) def __new__(cls, *args, **kwargs): print(cls.__name__, &apos;class instance created&apos;) return super().__new__(cls) def __init__(self): print(type(self).__name__, &apos;class instance inited&apos;) def hello(self): passb = Base() 当调用print(type(b))，得到，可知b是Base类的实例。Python是纯面向对象语言，因此类也是对象，当调用print(type(Base))时得到，可知类Base是type的实例，type就是Python中的原生元类，用来控制、生成类这个对象。 一般地，在定义类时，默认的此类的元类是type，因此，如果我们想控制类的创建，需要将类的元类指为我们自定义的元类，这个自定义的元类需要继承type元类。","categories":[],"tags":[]},{"title":"愚人节快乐","slug":"My-blog-35","date":"2017-04-01T01:25:39.000Z","updated":"2019-10-22T12:17:19.326Z","comments":true,"path":"2017/04/01/My-blog-35/","link":"","permalink":"http://yoursite.com/2017/04/01/My-blog-35/","excerpt":"","text":"今天没啥想写的,愚人节快乐呀,今天一直听哥哥的歌….","categories":[],"tags":[]},{"title":"python中如何创建自己的包","slug":"My-blog-34","date":"2017-03-24T11:47:34.000Z","updated":"2019-10-22T12:15:46.149Z","comments":true,"path":"2017/03/24/My-blog-34/","link":"","permalink":"http://yoursite.com/2017/03/24/My-blog-34/","excerpt":"","text":"Python中创包是比较方便的，只需要在当前目录建立一个文件夹， 文件夹中包含一个init.py文件和若干个模块文件， 其中init.py可以是一个空文件，但还是建议将包中所有需要导出的变量放到all中，这样可以确保包的接口清晰明了，易于使用。","categories":[],"tags":[]},{"title":"解释Python的参数传递机制","slug":"My-blog-33","date":"2017-03-09T08:35:38.000Z","updated":"2019-10-22T12:14:23.291Z","comments":true,"path":"2017/03/09/My-blog-33/","link":"","permalink":"http://yoursite.com/2017/03/09/My-blog-33/","excerpt":"","text":"Python使用按引用传递（pass-by-reference）将参数传递到函数中。如果你改变一个函数内的参数，会影响到函数的调用。这是Python的默认操作。不过，如果我们传递字面参数，比如字符串、数字或元组，它们是按值传递，这是因为它们是不可变的。 python不允许程序员选择采用传值还是传引用。Python参数传递采用的肯定是“传对象引用”的方式。这种方式相当于传值和传引用的一种综合。如果函数收到的是一个可变对象（比如字典或者列表）的引用，就能修改对象的原始值－－相当于通过“传引用”来传递对象。如果函数收到的是一个不可变对象（比如数字、字符或者元组）的引用，就不能直接修改原始对象－－相当于通过“传值’来传递对象。","categories":[],"tags":[]},{"title":"Python中yield的使用","slug":"My-blog-32","date":"2017-03-01T08:49:23.000Z","updated":"2019-10-22T12:13:54.397Z","comments":true,"path":"2017/03/01/My-blog-32/","link":"","permalink":"http://yoursite.com/2017/03/01/My-blog-32/","excerpt":"","text":"ield这个关键词相信大部分初学者第一次都很难弄懂，我也是经过多次的学习忘记，再学习，还有查阅其他人的分享，最后终于搞清楚啦。如果大家想更清晰地了解，建议大家用单步调试调试以下代码： def consumer(name): print(“%s 准备吃包子啦!” %name) while True: baozi = yield “return返回的值…” print(“包子[%s]来了,被[%s]吃了!” %(baozi,name)) c = consumer(“小华”)print(‘——-华丽分割线1————-‘)print(c.next())print(‘——-华丽分割线2————-‘)print(c.next())print(‘——-华丽分割线3————-‘)c.send(“韭菜包”) 输出结果： ——-华丽分割线1————-小华 准备吃包子啦!return返回的值…——-华丽分割线2————-包子[None]来了,被[小华]吃了!return返回的值…——-华丽分割线3————-包子[韭菜包]来了,被[小华]吃了! 下面我们来分析程序的运行过程：1.程序开始执行以后，因为consumer函数中有yield关键字，所以consumer函数并不会真的执行，而是先得到一个生成器generator(相当于一个对象)， 2.直到调用next()方法，consumer函数正式开始执行，先执行consumer函数中的print方法，然后进入while循环， 3.程序遇到yield关键字，然后把yield想想成return,return了一个值之后，程序停止，并没有执行赋值给baozi操作，此时next()语句执行完成，所以输出的前两行（第一个是while上面的print的结果,第二个是return出的结果）是执行print(c.next())的结果，4.又开始执行下面的print(c.next()),这个时候和上面那个差不多，不过不同的是，这个时候是从刚才那个上次程序停止的地方开始执行的，也就是要执行baozi的赋值操作，这时候要注意，这个时候赋值操作的右边是没有值的（因为刚才那个是return出去了，并没有给赋值操作的左边传参数），所以这个时候baozi赋值是None,所以接着下面的输出就是None, 5.程序会继续在while里执行，又一次碰到yield,这个时候同样return， 6.程序执行g.send(“韭菜包”)，程序会从yield关键字那一行继续向下运行，send会把”韭菜包”这个值赋值给baozi变量,执行print(“包子[%s]来了,被[%s]吃了!” %(baozi,name))。","categories":[],"tags":[]},{"title":"一句话明白:Python中生成器和迭代器的区别","slug":"My-blog-31","date":"2017-02-24T01:29:40.000Z","updated":"2019-10-22T12:12:46.172Z","comments":true,"path":"2017/02/24/My-blog-31/","link":"","permalink":"http://yoursite.com/2017/02/24/My-blog-31/","excerpt":"","text":"从数据集中一次按需获取一个数据项，这就是迭代器，迭代器强调是从数据集一次获取一个数据项，而生成器指“凭空”生成元素，然后也是可以一个一个获取；生成器中包含了迭代器的抽象（或者说接口、结构、方法），所以，所有的生成器都是迭代器。很多人视为同一概念","categories":[],"tags":[]},{"title":"浅析递归","slug":"My-blog-30","date":"2017-02-06T09:42:29.000Z","updated":"2019-10-22T12:11:02.433Z","comments":true,"path":"2017/02/06/My-blog-30/","link":"","permalink":"http://yoursite.com/2017/02/06/My-blog-30/","excerpt":"","text":"什么是递归递归的定义在定义一个过程或函数时，出现直接或间接调用自己的成分，称之为递归。 直接调用自己称为直接递归间接调用自己称为间接递归如果一个递归函数中调用递归语句是最后一条执行语句，则称这种递归调用为尾递归。 尾递归算法：可以用循环语句转换为等价的非递归算法其他递归算法：可以通过栈转换为等价的非递归算法何时使用递归定义是递归数据结构是递归的问题的求解方法是递归的递归模型的构成递归出口—确定递归结束情况递归体—确定大小问题的求解情况","categories":[],"tags":[]},{"title":"Lambda表达式","slug":"My-blog-29","date":"2017-01-20T14:25:10.000Z","updated":"2019-10-22T12:09:49.911Z","comments":true,"path":"2017/01/20/My-blog-29/","link":"","permalink":"http://yoursite.com/2017/01/20/My-blog-29/","excerpt":"","text":"如果我们需要一个只有单一表达式的函数，我们可以匿名定义它。拉姆达表达式通常是在需要一个函数，但是又不想费神去命名一个函数的场合下使用，也就是指匿名函数。 1(lambda a,b:a if a&gt;b else b)(3,3.5) 实现斐波那契数列 1fib = lambda n : n if n &lt;= 2 else fib(n-1)+fib(n-2)","categories":[],"tags":[]},{"title":"python不可变集合","slug":"My-blog-28","date":"2016-12-29T13:47:57.000Z","updated":"2019-10-22T12:07:16.211Z","comments":true,"path":"2016/12/29/My-blog-28/","link":"","permalink":"http://yoursite.com/2016/12/29/My-blog-28/","excerpt":"","text":"不可变集合对应于元组（tuple）与列表（list）的关系，对于集合（set），Python提供了一种叫做不可变集合（frozen set）的数据结构。 使用 frozenset 来进行创建： 与集合不同的是，不可变集合一旦创建就不可以改变。 不可变集合的一个主要应用是用来作为字典的键，例如用一个字典来记录两个城市之间的距离： 由于集合不分顺序，所以不同顺序不会影响查阅结果：","categories":[],"tags":[]},{"title":"python切片详解","slug":"My-blog-27","date":"2016-12-02T10:25:50.000Z","updated":"2019-10-22T12:05:43.850Z","comments":true,"path":"2016/12/02/My-blog-27/","link":"","permalink":"http://yoursite.com/2016/12/02/My-blog-27/","excerpt":"","text":"先构造一个list列表a： 12a = list(range(1,8)) # [1, 2, 3, 4, 5, 6, 7]1 切片的使用可以看做 [start🔚interval]，三者的取值可正可负。其中，start、end、interval有时候可以省略。列表a中每个数字对应正、负两个索引，如 a[2] = a[-5] = 3，这是因为在python中，第一个元素对应的下标为0，最后一个元素对应的下标为-1，所以从对于3这个数字，从左往右索引就是2，从右往左索引就是-5。值得注意的是，interval中的正负号规定了取数的方向是从左到右（+）还是从右到左（-）。如果有interval，先确定其方向（没有的话默认为从左到右取数）。 切片的应用： 12345678910111213141516171819a[:3] # [1,2,3] 即索引是 [0,3) =&gt; a[0]、a[1]、a[2]a[:-5] # [1,2] 等价于 a[:(-5+a中元素个数7)] = a[:2] = [1,2]a[:5:2] # [1,3,5] 先取前5个元素，然后按照间隔2取数a[:4:-1] # [7,6] 先看interval为负值，故从左往右取，取到下标为4的前一个为止，即能取到a[6]、a[5]a[:3:-2] # [7,5] 先看interval为负值，故从右往左取，取到下标为3的前一个为止，即能取到a[6]、a[5]、a[4]，然后按照间隔2取数a[:-5:-1] # [7,6,5,4] 先看interval为负值，故从右往左取，取到下标为-5的前一个为止，即能取到a[-1]、a[-2]、a[-3]、a[-4]a[:10] # [1,2,3,4,5,6,7] 即索引是 [0,10)，超过不报错a[:10:-1] # [] 先看interval为负值，故从右往左取，取到下标为10的前一个为止，但是从左到右取最大的下标也才是a[6]，故返回[]a[:-100:-1] # [7,6,5,4,3,2,1] 先看interval为负值，故从右往左取，取到下标为-100的前一个为止，即能取到a[-1]、a[-2]、a[-3]、a[-4]、a[-5]、a[-6]、a[-7] a[::-1] # [7,6,5,4,3,2,1] 先看interval为负值，故从右往左取，取到头（因为end没有指定）为止，即能取到a[-1]、a[-2]、a[-3]、a[-4]、a[-5]、a[-6]、a[-7] ，该方法也是list反转的方法a[::2] # [1,3,5,7] 每隔2个元素取数a[3::2] # [4,6] 从a[3]=4开始，每隔2个元素取数12345678910111213a[:-2] # [1,2,3,4,5]a[-6:10] # [2,3,4,5,6,7]a[-4::-1] # [4,3,2,1]a[-1:-5:-2] # [7,5]b = a[:4] + a[5:] # [1, 2, 3, 4, 6, 7]，切片法去除list中一个元素","categories":[],"tags":[]},{"title":"Python数据类型有哪些？","slug":"My-blog-26","date":"2016-11-22T03:45:29.000Z","updated":"2019-10-22T12:03:59.524Z","comments":true,"path":"2016/11/22/My-blog-26/","link":"","permalink":"http://yoursite.com/2016/11/22/My-blog-26/","excerpt":"","text":"数据类型是每种编程语言必备属性，只有给数据赋予明确的数据类型，计算机才能对数据进行处理运算，因此，正确使用数据类型是十分必要的，不同的语言，数据类型类似，但具体表示方法有所不同，以下是Python编程常用的数据类型： 数字类型 Python数字类型主要包括int（整型）、long（长整型）和float（浮点型），但是在Python3中就不再有long类型了。 int（整型） 在32位机器上，整数的位数是32位，取值范围是-231231-1，即-2147483648214748364；在64位系统上，整数的位数为64位，取值范围为-263263-1，即92233720368547758089223372036854775807。 long（长整型） Python长整型没有指定位宽，但是由于机器内存有限，使用长的长整数数值也不可能无限大。 float（浮点型） 浮点型也就是带有小数点的数，其精度和机器有关。 complex（复数） Python还支持复数，复数由实数部分和虚数部分构成，可以用 a + bj,或者 complex(a,b) 表示， 复数的实部 a 和虚部 b 都是浮点型。 字符串 在Python中，加了引号的字符都被认为是字符串，其声明有三种方式，分别是：单引号、双引号和三引号；Python中的字符串有两种数据类型，分别是str类型和unicode类型，str类型采用的ASCII编码，无法表示中文，unicode类型采用unicode编码，能够表示任意字符，包括中文和其他语言。 布尔型 和其他编程语言一样，Python布尔类型也是用于逻辑运算，有两个值：True（真）和False（假）。 列表 列表是Python中使用最频繁的数据类型，集合中可以放任何数据类型，可对集合进行创建、查找、切片、增加、修改、删除、循环和排序操作。 元组 元组和列表一样，也是一种序列，与列表不同的是，元组是不可修改的，元组用”()”标识，内部元素用逗号隔开。 字典 字典是一种键值对的集合，是除列表以外Python之中最灵活的内置数据结构类型，列表是有序的对象集合，字典是无序的对象集合。 集合 集合是一个无序的、不重复的数据组合，它的主要作用有两个，分别是去重和关系测试。 以上是对Python的七种数据类型的简单介绍，具体使用方法，可以进行深入查找学习！","categories":[],"tags":[]},{"title":"python装饰器","slug":"My-blog-25","date":"2016-11-18T13:15:56.000Z","updated":"2019-10-22T12:02:26.850Z","comments":true,"path":"2016/11/18/My-blog-25/","link":"","permalink":"http://yoursite.com/2016/11/18/My-blog-25/","excerpt":"","text":"什么是装饰器本质上，装饰器（decorator）就是一个返回函数的高阶函数。当我们想对已有程序进行功能扩展时，我们往往不会选择修改函数或者模块内部的源代码，这样是不科学也是不现实的（可以看一下开闭原则）。装饰器由此而来，它满足了以下3点原则： 不修改被装饰的函数的源代码不修改被装饰的函数的调用方式在满足1、2的情况下给程序增加额外的功能装饰器经常被用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等。 装饰器的使用上面对装饰器简单介绍了一下，接下来我们来看看装饰器具体长什么样，以及怎么去装饰函数。 先看一个简单的装饰器： 装饰器函数， 接受函数名作为参数def decorator(func): def wrapper(): # 提示：wrapper函数中的print()操作只是最简单的示例，根据不同的需求可以换成更有实际意义的操作。 print(‘before doing something’) func() # 在包装函数中调用被装饰函数 print(‘something finished’) return wrapper @decoratordef something(): print(‘doing something’) print(something.name)something()‘’’输出： wrapper before doing something doing something something finished‘’’ 看完代码，我们来说明一下装饰器是怎么装饰函数的： 方式一： 使用语法糖，在被修饰函数定义前加上 @ + 装饰器的名字@decoratordef func(): pass 方式二： 手动将被修饰函数的名字作为参数传入装饰器中func = decorator(func) 这两种方式的实际效果是一样的，但是方式一明显更为简捷。 要注意的是，因为返回的那个wrapper()函数名就是wrapper，所以我们需要把原始函数的name等属性复制到wrapper()函数中，否则，有些依赖函数签名的代码执行就会出错。Python内置的functools.wraps方法就能替我们解决这个问题，只需在定义wrapper()的前面加上@functools.wraps(func)即可。下面的示例程序都将使用wraps方法。 接下来实现一个更实用的装饰器，它可以修饰带参数且有返回值的函数： import functools def decorator(func): @functools.wraps(func) def wrapper(args, *kwargs): # 使用可变参数，更具扩展性 print(‘before doing something’) ret = func(args, *kwargs) # 使用变量ret存放被装饰函数的返回 print(‘something finished’) return ret # 包装函数再返回ret，实现返回的传递 return wrapper @decoratordef get_plus(a, b): # 返回两数之和 print(‘call get_plus’) return a + b @decoratordef get_sum(args, *kwargs): # 返回n个数的和 print(‘call get_sum’) return sum(args) print(get_plus.name)print(get_sum.name)sum1 = get_plus(1, 3)sum2 = get_sum(2, 3, 4)print(sum1, sum2) ‘’’输出： get_plus get_sum before doing something call get_plus something finished before doing something call get_sum something finished 4 9‘’’ 可以看到，为wrapper函数添加了可变参数后，装饰器能修饰有固定参数的get_plus函数，也能修饰参数不定的get_sum函数，而且在使用了functools.wraps方法后，被修饰函数的name等属性也一并复制了过去。 如果针对不同的函数，我们需要装饰器来区别对待，那么可以这样做： import functools def decorator(param): # 装饰器接受参数param def outer_wrapper(func): @functools.wraps(func) def wrapper(args, *kwargs): if param == ‘task1’: # 根据参数param判断当前修饰的是哪个函数 func(args, *kwargs) elif param == ‘task2’: func(args, *kwargs) print(param,”is finished.”) return wrapper return outer_wrapper @decorator(param=’task1’)def task1(): print(“doing task1 right now..”) @decorator(param=’task2’)def task2(): print(“doing task2 right now..”) print(task1.name, task2.name)task1()task2()‘’’输出： task1 task2 doing task1 right now.. task1 is finished. doing task2 right now.. task2 is finished.‘’’ 分析一下给装饰器传参的操作： @decorator(param = arg) # 1def func(args, *kwargs): pass #1处的语句与#2处的语句是等价的为了容易理解，我把#2右边的逐步化简如下：func = decorator(arg)(func) # 2 = outer_wrapper(func) # 注意以下两句直接执行是会报错的，此处只是为了方便理解 = wrapper(args, *kwargs) 通过上面的代码我们可以看到，实际上func经过多层的嵌套最终还是被“包”在最里层的wrapper里面，传递给装饰器的参数param只是为了wrapper可以对不同的函数有区别地“包装”，这也算是对装饰器功能的进一步扩展。","categories":[],"tags":[]},{"title":"python中的闭包","slug":"My-blog-24","date":"2016-11-01T01:53:40.000Z","updated":"2019-10-22T12:00:28.911Z","comments":true,"path":"2016/11/01/My-blog-24/","link":"","permalink":"http://yoursite.com/2016/11/01/My-blog-24/","excerpt":"","text":"Python中的闭包是一个比较模糊的概念，有很多朋友都认为不好理解，但是随着深入学习，就会发现闭包无论如何都是需要去理解的，下面我将自己对闭包的理解进行阐述，希望能够对你有所帮助 ~ 闭包的理解我们可以将闭包理解为一种特殊的函数，这种函数由两个函数的嵌套组成，且称之为外函数和内函数，外函数返回值是内函数的引用，此时就构成了闭包。 闭包的格式下面用伪代码进行闭包格式的描述 def 外层函数(参数): def 内层函数(): print(“内层函数执行”, 参数) return 内层函数内层函数的引用 = 外层函数(“传入参数”)内层函数的引用() 外层函数中的参数，不一定要有，据情况而定，但是一般情况下都会有并在内函数中使用到 案例def func(a, b): def line(x): return a * x - b return lineline = func(2, 3)print(line(5)) 结果得到 7在这个案例中，外函数func有接收参数 a=2，b=3，内函数line接收参数x=5，在内函数体中计算了a*x-b 即 2×5-3的值作为返回值，外函数返回内函数的引用，这里的引用指的是内函数line在内存中的起始地址，最终调用内函数line()得到返回值7 内函数中修改外函数的值一般在函数结束时，会释放临时变量，但在闭包中，由于外函数的临时变量在内函数中用到，此时外函数会把临时变量与内函数绑定到一起，这样虽然外函数结束了，但调用内函数时依旧能够使用临时变量，即闭包外层的参数可以在内存中进行保留如果想要在内函数中修改外函数的值，需要使用 nonlocal 关键字声明变量 def func(a, b): def line(x): nonlocal a a = 3 return a * x - b return lineline = func(2, 3)print(line(5)) 此时运行结果为：12 闭包的用途Python中，闭包的主要用途就是用于装饰器的实现，以后博客中将会对装饰器做详细解释↖(^_^)↗","categories":[],"tags":[]},{"title":"Python中的pass语句","slug":"My-blog-23","date":"2016-10-11T11:14:44.000Z","updated":"2019-10-22T11:58:40.317Z","comments":true,"path":"2016/10/11/My-blog-23/","link":"","permalink":"http://yoursite.com/2016/10/11/My-blog-23/","excerpt":"","text":"**pass语句的作用： 什么都不做；保证格式或语义完整； ####hello.py if __ name__==”_main“: if(true): print(“main function.”) else: pass #do nothing 使用环境多为执行语句部分思路还没有完成，即使用pass语句来占位；","categories":[],"tags":[]},{"title":"python之去掉字符串中空格的方法","slug":"My-blog-22","date":"2016-09-19T08:54:29.000Z","updated":"2019-10-22T11:56:04.031Z","comments":true,"path":"2016/09/19/My-blog-22/","link":"","permalink":"http://yoursite.com/2016/09/19/My-blog-22/","excerpt":"","text":"####strip():去掉头和尾的空格str1=” I love U “print(str1.strip()) ####lstrip():去掉左侧空格str2=” I love U –”print(str2.lstrip()) ####rstrip():去掉右侧空格str3=” I love U “print(str3.rstrip(),end=’*’)print(‘\\n’) ####replace(‘c1’,’c2’),可以用replace(‘ ‘,’’)来去掉字符串中的所有空格str4=” I love U *”print(str4.replace(‘ ‘,’’))运行结果： I love UI love U – I love U*","categories":[],"tags":[]},{"title":"Split()方法和join()方法的区别？","slug":"My-blog-21","date":"2016-09-02T15:17:23.000Z","updated":"2019-10-22T11:53:33.729Z","comments":true,"path":"2016/09/02/My-blog-21/","link":"","permalink":"http://yoursite.com/2016/09/02/My-blog-21/","excerpt":"","text":"Split()方法是切割成数组的形式， Join()方法是将数组转换成字符串。 1.join() 方法用于把数组中的所有元素放入一个字符串。 元素是通过指定的分隔符进行分隔的。 指定分隔符方法join(“#”);其中#可以是任意. 2.与之相反的是split()方法：用于把一个字符串分割成字符串数组.stringObject.split(a,b)这是它的语法a是必须的决定个从a这分割b不是必须的，可选。该参数可指定返回的数组的最大长度 。如果设置了该参数，返回的子串不会多于这个参数指定的数组。如果没有设置该参数，整个字符串都会被分割，不考虑它的长度。 注意返回的数组中不包括a本身。","categories":[],"tags":[]},{"title":"python 负索引 numpy 高级索引","slug":"My-blog-20","date":"2016-08-27T13:44:18.000Z","updated":"2019-10-22T11:52:22.851Z","comments":true,"path":"2016/08/27/My-blog-20/","link":"","permalink":"http://yoursite.com/2016/08/27/My-blog-20/","excerpt":"","text":"问题描述：在阅读源码中，有处断言条件用到了a[0]==x[-1]，列表索引并没接触过-1这一参数，通常0表示第一维水平向，1表示第二维竖直向。 查阅得知：1、python中正索引从左到右0，1，2… 负索引从右到左-1,-2,-3…2、高维数组难以直观理解，所以负索引提供了一种从高维到低维的查找方式3、numpy中高级索引，花式索引可将数组作为索引变量，解决手写迭代的繁琐","categories":[],"tags":[]},{"title":"python的*args和**kwargs","slug":"My-blog-19","date":"2016-08-06T06:20:06.000Z","updated":"2019-10-22T11:51:10.912Z","comments":true,"path":"2016/08/06/My-blog-19/","link":"","permalink":"http://yoursite.com/2016/08/06/My-blog-19/","excerpt":"","text":"当我们不知道向函数传递多少参数时，比如我们向传递一个列表或元组，我们就使用*args。 1234&gt;&gt;&gt; def func(*args): for i in args: print(i) &gt;&gt;&gt; func(3,2,1,4,7) 在我们不知道该传递多少关键字参数时，使用**kwargs来收集关键字参数。 1234&gt;&gt;&gt; def func(**kwargs): for i in kwargs: print(i,kwargs[i])&gt;&gt;&gt; func(a=1,b=2,c=7)","categories":[],"tags":[]},{"title":"Python 猴子补丁","slug":"My-blog-18","date":"2016-07-21T12:24:01.000Z","updated":"2019-10-22T11:48:19.684Z","comments":true,"path":"2016/07/21/My-blog-18/","link":"","permalink":"http://yoursite.com/2016/07/21/My-blog-18/","excerpt":"","text":"Python 猴子补丁属性在运行时的动态替换，叫做猴子补丁（Monkey Patch）。 为什么叫猴子补丁属性的运行时替换和猴子也没什么关系，关于猴子补丁的由来网上查到两种说法： 1，这个词原来为Guerrilla Patch，杂牌军、游击队，说明这部分不是原装的，在英文里guerilla发音和gorllia(猩猩)相似，再后来就写了monkey(猴子)。2，还有一种解释是说由于这种方式将原来的代码弄乱了(messing with it)，在英文里叫monkeying about(顽皮的)，所以叫做Monkey Patch。 猴子补丁的叫法有些莫名其妙，只要和“模块运行时替换的功能”对应就行了。 猴子补丁的用法1，运行时动态替换模块的方法stackoverflow上有两个比较热的例子， consider a class that has a method get_data. This method does an external lookup (on a database or web API, for example), and various other methods in the class call it. However, in a unit test, you don’t want to depend on the external data source - so you dynamically replace the get_data method with a stub that returns some fixed data.假设一个类有一个方法get_data。这个方法做一些外部查询（如查询数据库或者Web API等），类里面的很多其他方法都调用了它。然而，在一个单元测试中，你不想依赖外部数据源。所以你用哑方法态替换了这个get_data方法，哑方法只返回一些测试数据。 另一个例子引用了，Zope wiki上对Monkey Patch解释： from SomeOtherProduct.SomeModule import SomeClass def speak(self): return “ook ook eee eee eee!” SomeClass.speak = speak 还有一个比较实用的例子，很多代码用到 import json，后来发现ujson性能更高，如果觉得把每个文件的import json 改成 import ujson as json成本较高，或者说想测试一下用ujson替换json是否符合预期，只需要在入口加上： import jsonimport ujson def monkey_patch_json(): json.name = ‘ujson’ json.dumps = ujson.dumps json.loads = ujson.loads monkey_patch_json() 2，运行时动态增加模块的方法这种场景也比较多，比如我们引用团队通用库里的一个模块，又想丰富模块的功能，除了继承之外也可以考虑用Monkey Patch。个人感觉Monkey Patch带了便利的同时也有搞乱源代码优雅的风险。","categories":[],"tags":[]},{"title":"浅析python类继承","slug":"My-blog-17","date":"2016-06-28T11:17:15.000Z","updated":"2019-10-22T11:46:36.774Z","comments":true,"path":"2016/06/28/My-blog-17/","link":"","permalink":"http://yoursite.com/2016/06/28/My-blog-17/","excerpt":"","text":"面向对象编程 (OOP) 语言的一个主要功能就是“继承”。继承是指这样一种能力：它可以使用现有类的所有功能，并在无需重新编写原来的类的情况下对这些功能进行扩展。 通过继承创建的新类称为“子类”或“派生类”，被继承的类称为“基类”、“父类”或“超类”，继承的过程，就是从一般到特殊的过程。在某些 OOP 语言中，一个子类可以继承多个基类。但是一般情况下，一个子类只能有一个基类，要实现多重继承，可以通过多级继承来实现。 继承概念的实现方式主要有2类：实现继承、接口继承。 实现继承是指使用基类的属性和方法而无需额外编码的能力。接口继承是指仅使用属性和方法的名称、但是子类必须提供实现的能力(子类重构爹类方法)。 继承： 1、继承时，子类会自动调用父类的构造函数 2、多继承：继承的父类会由左向右，一次继承构造第一个继承的父类无构造函数，则会找下一个父类的构造函数，第一个继承的父类有构造函数，那么在实例化子类时就要传相应多的参数，需要多传参数时，就需要重构父类 3、继承类的构造方法： 经典类的写法： 父类名称.__init__(self,参数1，参数2，...) 新式类的写法：super(子类，self).__init__(参数1，参数2，....)4、方法：A类继承B，A类和B类有相同方法时，优先使用A本类的，A类没有时，才去父类B找，使用父类的 5、实例变量：A类继承B，A类和B类有相同实例变量时，优先使用A本类的，A类没有时，才去父类B找，使用父类的 6、类变量：A类继承B，A类和B类有相同类变量时，优先使用A本类的，A类没有时，才去父类B找，使用父类的 7、新式类按广度优选继承的，B、C分别继承A，D继承B、C 执行D，当D没有构造方法找B,当B没有找C，当C没有找A #单类继承class People(object):#新式类 def init(self,name,age): self.name=name self.age=age def A(self): print(“this is People类 A方法,姓名:%s,年龄:%s”%(self.name,self.age))class Man(People):#继承People类 def A(self): print(“this is Man类A方法,姓名:%s,年龄:%s”%(self.name,self.age)) def B(self): print(“this is Man类B方法继承People类,姓名:%s,年龄:%s”%(self.name,self.age))class Women(People):#继承People类 def init(self,name,age,money):#先继承，再重构 People.init(self,name,age)#继承父类构造方法 #super(Women,self).init(name,age)#方法2 self.money=money def B(self): People.A(self) print(“this is Women类A方法,姓名:%s,年龄:%s,薪资:%s” % (self.name, self.age,self.money)) p=People(“wangli”,7)p.A()print(p.name,p.age)m=Man(“chengzi”,11)m.A()m.B()print(m.name,m.age)w=Women(“xixi”,22,111111)w.B()print(w.name,w.age,w.money) C:\\Users\\wangli\\PycharmProjects\\AutoMation\\venv\\Scripts\\python.exe C:/Users/wangli/PycharmProjects/AutoMation/case/test.pythis is People类 A方法,姓名:wangli,年龄:7wangli 7this is Man类A方法,姓名:chengzi,年龄:11this is Man类B方法继承People类,姓名:chengzi,年龄:11chengzi 11this is People类 A方法,姓名:xixi,年龄:22this is Women类A方法,姓名:xixi,年龄:22,薪资:111111xixi 22 111111","categories":[],"tags":[]},{"title":"GIL全局解释器锁","slug":"My-blog-16","date":"2016-06-20T10:13:29.000Z","updated":"2019-10-22T11:43:12.962Z","comments":true,"path":"2016/06/20/My-blog-16/","link":"","permalink":"http://yoursite.com/2016/06/20/My-blog-16/","excerpt":"","text":"Python自带的解释器是 Cpython。 Cpython解释器的多线程实际上是一个假的多线程(在多核CPU中,只能利用一核,不能利用多核)。同一时刻只有一个线程在执行,为了保证同一时刻只有一个线程在执行,在CPython解释器中有一个东西叫GIL,叫做全局解释器锁。这个解释器锁是有必要的。因力Cpython解释器的内存管理不是线程安全的。当然除了CPytn解释器,还有其他的解释器,有些解释器是没有GIL锁的,见下面:1、Jython:用Java实现的Python解释器。不存在GIL锁。更多详情请见:https:/zh.wikipedi.org/wiki/Jython IronPython:用.net实现的 Python解器。不存在GIL锁。更多详情请见:htes/zh. wikipedia.org/ wiki/lronpython PyPy:用 Python实现的Python解释器。存在GIL锁。更多详情请见:htps//zh. wikipedia.org/wki/PyPyGIL虽然足是一个假的多线程.但是在处理一些IO操作(比如文件读写和网请求)还是可以在很大程度上提高效率的。在IO操作上建议使用多线程提高效率。在一些CPU计算操作上不建议使用多线程,建议使用多进程。","categories":[],"tags":[]},{"title":"了解三元运算符以及三元运算符的使用","slug":"My-blog-15","date":"2016-05-19T14:13:47.000Z","updated":"2019-10-22T11:41:38.807Z","comments":true,"path":"2016/05/19/My-blog-15/","link":"","permalink":"http://yoursite.com/2016/05/19/My-blog-15/","excerpt":"","text":"1.格式 数据类型 变量名 = 布尔类型表达式？结果1：结果2; 例如： int c = a &gt; b ? a : b;2.三元运算符计算方式布尔类型表达式结果是true，三元运算符整体结果为结果1，赋值给变量。 布尔类型表达式结果是false，三元运算符整体结果为结果2，赋值给变量。 public static void main(String[] args){ int age = 19 ; // 在这里讲解一下字符串的类型是String。 String str = age &gt;= 18 ? “成年人” : “未成年人”； System.out.println(str);","categories":[],"tags":[]},{"title":"冒泡排序","slug":"My-blog-14","date":"2016-05-12T05:33:11.000Z","updated":"2019-10-22T11:39:55.365Z","comments":true,"path":"2016/05/12/My-blog-14/","link":"","permalink":"http://yoursite.com/2016/05/12/My-blog-14/","excerpt":"","text":"冒泡排序：它重复地走访过要排序的数列，一次比较两个元素，如果他们的顺序错误就把他们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成 时间复杂度：O(n²) 空间复杂度：O(1) 稳定性：稳定 12345678910def bubble_sort(blist): count = len(blist) for i in range(0, count): for j in range(i + 1, count): if blist[i] &gt; blist[j]: blist[i], blist[j] = blist[j], blist[i] return blistblist = bubble_sort([4,5,6,7,3,2,6,9,8])print(blist)","categories":[],"tags":[]},{"title":"session,cookie,sessionStorage,localStorage的区别及应用场景","slug":"My-blog-13","date":"2016-04-19T02:46:37.000Z","updated":"2019-10-22T11:38:43.693Z","comments":true,"path":"2016/04/19/My-blog-13/","link":"","permalink":"http://yoursite.com/2016/04/19/My-blog-13/","excerpt":"","text":"客户端状态保持是一个老生常谈的问题了，归根结底追踪浏览器的用户身份及其相关数据无非就是以下四种方式：session,cookie,sessionStorage,localStorage ​ 首先cookie和session: ​ Cookie机制：如果不在浏览器中设置过期时间，cookie被保存在内存中，生命周期随浏览器的关闭而结束，这种cookie简称会话cookie。如果在浏览器中设置了cookie的过期时间，cookie被保存在硬盘中，关闭浏览器后，cookie数据仍然存在，直到过期时间结束才消失。 ​ Cookie是服务器发给客户端的特殊信息，cookie是以文本的方式保存在客户端，每次请求时都带上它 ​ Session机制：当服务器收到请求需要创建session对象时，首先会检查客户端请求中是否包含sessionid。如果有sessionid，服务器将根据该id返回对应session对象。如果客户端请求中没有sessionid，服务器会创建新的session对象，并把sessionid在本次响应中返回给客户端。通常使用cookie方式存储sessionid到客户端，在交互中浏览器按照规则将sessionid发送给服务器。如果用户禁用cookie，则要使用URL重写，可以通过response.encodeURL(url) 进行实现；API对encodeURL的结束为，当浏览器支持Cookie时，url不做任何处理；当浏览器不支持Cookie的时候，将会重写URL将SessionID拼接到访问地址后。 ​ 3、存储内容：cookie只能保存字符串类型，以文本的方式；session通过类似与Hashtable的数据结构来保存，能支持任何类型的对象(session中可含有多个对象) ​ 4、存储的大小：cookie：单个cookie保存的数据不能超过4kb；session大小没有限制。 ​ 5、安全性：cookie：针对cookie所存在的攻击：Cookie欺骗，Cookie截获；session的安全性大于cookie。 ​ 原因如下： ​ （1）sessionID存储在cookie中，若要攻破session首先要攻破cookie； ​ （2）sessionID是要有人登录，或者启动session_start才会有，所以攻破cookie也不一定能得到sessionID； ​ （3）第二次启动session_start后，前一次的sessionID就是失效了，session过期后，sessionID也随之失效。 ​ （4）sessionID是加密的 ​ （5）综上所述，攻击者必须在短时间内攻破加密的sessionID，并非易事。 ​ 6、应用场景： ​ cookie： ​ （1）判断用户是否登陆过网站，以便下次登录时能够实现自动登录（或者记住密码）。如果我们删除cookie，则每次登录必须从新填写登录的相关信息。 ​ （2）保存上次登录的时间等信息。 ​ （3）保存上次查看的页面 ​ （4）浏览计数 ​ session：Session用于保存每个用户的专用信息，变量的值保存在服务器端，通过SessionID来区分不同的客户。 ​ （1）网上商城中的购物车 ​ （2）保存用户登录信息 ​ （3）将某些数据放入session中，供同一用户的不同页面使用 ​ （4）防止用户非法登录 ​ 7、缺点：cookie： ​ （1）大小受限 ​ （2）用户可以操作（禁用）cookie，使功能受限 ​ （3）安全性较低 ​ （4）有些状态不可能保存在客户端。 ​ （5）每次访问都要传送cookie给服务器，浪费带宽。 ​ （6）cookie数据有路径（path）的概念，可以限制cookie只属于某个路径下。 ​ session： ​ （1）Session保存的东西越多，就越占用服务器内存，对于用户在线人数较多的网站，服务器的内存压力会比较大。 ​ （2）依赖于cookie（sessionID保存在cookie），如果禁用cookie，则要使用URL重写，不安全 ​ （3）创建Session变量有很大的随意性，可随时调用，不需要开发者做精确地处理，所以，过度使用session变量将会导致代码不可读而且不好维护。 ​ 说白了，这两种状态保持方式都差强人意，于是webStroage应运而生 ​ WebStorage的目的是克服由cookie所带来的一些限制，当数据需要被严格控制在客户端时，不需要持续的将数据发回服务器。 ​ WebStorage两个主要目标：（1）提供一种在cookie之外存储会话数据的路径。（2）提供一种存储大量可以跨会话存在的数据的机制。 ​ HTML5的WebStorage提供了两种API：localStorage（本地存储）和sessionStorage（会话存储）。 ​ 1、生命周期：localStorage:localStorage的生命周期是永久的，关闭页面或浏览器之后localStorage中的数据也不会消失。localStorage除非主动删除数据，否则数据永远不会消失。 ​ sessionStorage的生命周期是在仅在当前会话下有效。sessionStorage引入了一个“浏览器窗口”的概念，sessionStorage是在同源的窗口中始终存在的数据。只要这个浏览器窗口没有关闭，即使刷新页面或者进入同源另一个页面，数据依然存在。但是sessionStorage在关闭了浏览器窗口后就会被销毁。同时独立的打开同一个窗口同一个页面，sessionStorage也是不一样的。 ​ 2、存储大小：localStorage和sessionStorage的存储数据大小一般都是：5MB ​ 3、存储位置：localStorage和sessionStorage都保存在客户端，不与服务器进行交互通信。 ​ 4、存储内容类型：localStorage和sessionStorage只能存储字符串类型，对于复杂的对象可以使用ECMAScript提供的JSON对象的stringify和parse来处理 ​ 5、获取方式：localStorage：window.localStorage;；sessionStorage：window.sessionStorage;。 ​ 6、应用场景：localStoragese：常用于长期登录（+判断用户是否已登录），适合长期保存在本地的数据（令牌）。sessionStorage：敏感账号一次性登录； ​ WebStorage的优点： ​ （1）存储空间更大：cookie为4KB，而WebStorage是5MB； ​ （2）节省网络流量：WebStorage不会传送到服务器，存储在本地的数据可以直接获取，也不会像cookie一样美词请求都会传送到服务器，所以减少了客户端和服务器端的交互，节省了网络流量； ​ （3）对于那种只需要在用户浏览一组页面期间保存而关闭浏览器后就可以丢弃的数据，sessionStorage会非常方便； ​ （4）快速显示：有的数据存储在WebStorage上，再加上浏览器本身的缓存。获取数据时可以从本地获取会比从服务器端获取快得多，所以速度更快； ​ （5）安全性：WebStorage不会随着HTTP header发送到服务器端，所以安全性相对于cookie来说比较高一些，不会担心截获，但是仍然存在伪造问题； ​ （6）WebStorage提供了一些方法，数据操作比cookie方便； ​ setItem (key, value) —— 保存数据，以键值对的方式储存信息。","categories":[],"tags":[]},{"title":"python垃圾回收机制以及循环引用问题","slug":"My-blog-12","date":"2016-04-02T10:41:56.000Z","updated":"2019-10-22T11:37:19.612Z","comments":true,"path":"2016/04/02/My-blog-12/","link":"","permalink":"http://yoursite.com/2016/04/02/My-blog-12/","excerpt":"","text":"几次涉及到垃圾回收机制,这里做一个总结 引用计数是一种垃圾回收机制,而且也是一种最直观,最简单的垃圾收集技术.当一个对象呗创建或者被引用时,该对象的引用计数就会加1,当对象被销毁时相应的引用计数就会减1,一旦引用计数减为0时,表示该对象已经没有被使用.可以将其所占用的内存资源释放掉. 1 a =[]#对象A的引用计数为 12 b =[]#对象B的引用计数为 13 a[1] = b #B的引用计数增14 b[1] = a #A的引用计数增15 del a #A的引用减 1，最后A对象的引用为 16 del b #B的引用减 1, 最后B对象的引用为 1优点: 实时性:任何内存,一旦没有 指向他的引用.就会立即被回收.而其他的垃圾回收技术必须在某种图书条件下才能进行无效的内存回收.缺点:交差引用.如果两个对象的引用计数都为 1，但是仅仅存在他们之间的循环引用，那么这两个对象都是需要被回收的，也就是说，它们的引用计数虽然表现为非0，但实际上有效的引用计数为 0。我们必须先将循环引用摘掉，那么这两个对象的有效计数就现身了。假设两个对象为 A、B，我们从A出发，因为它有一个对 B的引用，则将B的引用计数减1；然后顺着引用达到 B，因为B有一个对A的引用，同样将A的引用减1，这样，就完成了循环引用对象间环摘除 但是这样就有一个问题，假设对象A有一个对象引用C，而C没有引用 A，如果将C计数引用减1， 而最后A并没有被回收，显然，我们错误的将 C的引用计数减 1，这将导致在未来的某个时刻出现一个对 C 的悬空引用。这就要求我们必须在 A没有被删除的情况下复原 C 的引用计数，如果采用这样的方案，那么维护引用计数的复杂度将成倍增加 分代回收分代回收是一种以空间换时间的操作方式，Python将内存根据对象的存活时间划分为不同的集合，每个集合称为一个代，Python将内存分为了3“代”，分别为年轻代（第0代）、中年代（第1代）、老年代（第2代），他们对应的是3个链表，它们的垃圾收集频率与对象的存活时间的增大而减小。新创建的对象都会分配在年轻代，年轻代链表的总数达到上限时，Python垃圾收集机制就会被触发，把那些可以被回收的对象回收掉，而那些不会回收的对象就会被移到中年代去，依此类推，老年代中的对象是存活时间最久的对象，甚至是存活于整个系统的生命周期内。同时，分代回收是建立在标记清除技术基础之上。分代回收同样作为Python的辅助垃圾收集技术处理那些容器对象内存池Python中有分为大内存和小内存：（256K为界限分大小内存）1、大内存使用malloc进行分配2、小内存使用内存池进行分配python内存池:第3层：最上层，用户对Python对象的直接操作 第1层和第2层：内存池，有Python的接口函数PyMem_Malloc实现—–若请求分配的内存在1~256字节之间就使用内存池管理系统进行分配，调用malloc函数分配内存，但是每次只会分配一块大小为256K的大块内存，不会调用free函数释放内存，将该内存块留在内存池中以便下次使用。 第0层：大内存—–若请求分配的内存大于256K，malloc函数分配内存，free函数释放内存。 第-1，-2层：操作系统进行操作内存泄漏:指由于疏忽或错误造成程序未能释放已经不再使用的内存的情况。内存泄漏并非指内存在物理上的消失，而是应用程序分配某段内存后，由于设计错误，失去了对该段内存的控制，因而造成了内存的浪费。导致程序运行速度减慢甚至系统崩溃等严重后果。有 del() 函数的对象间的循环引用是导致内存泄漏的主凶。不使用一个对象时使用:del object 来删除一个对象的引用计数就可以有效防止内存泄漏问题。通过 Python 扩展模块 gc 来查看不能回收的对象的详细信息。可以通过 sys.getrefcount(obj) 来获取对象的引用计数，并根据返回值是否为 0 来判断是否内存泄漏","categories":[],"tags":[]},{"title":"HTTP、 HTTP1.1、 HTTP/2的区别","slug":"My-blog-11","date":"2016-03-28T07:19:15.000Z","updated":"2019-10-22T11:33:38.031Z","comments":true,"path":"2016/03/28/My-blog-11/","link":"","permalink":"http://yoursite.com/2016/03/28/My-blog-11/","excerpt":"","text":"1、什么是HTTP/2？HTTP/2 是 HTTP 协议的第二个主要版本，为什么HTTP/2不叫HTTP2.0，是因为标准委员会不在打算发布子版本了，下一个版本将是HTTP/3了。该版本注重性能，致力于减少延迟时间，以提高页面的响应速度为准则。HTTP/2 相比HTTP1.X更简单,高效,强大,在传输层解决了之前使用HTTP1.x中的问题，HTTP/2 是基于 SPDY 协议的，可以打开一个TCP连接并且重复使用，响应多路复用，而且也支持压缩头部减少头部的体积，添加请求优先级，服务端推送。在HTTP/2中为了支持这些特性，增加了大量的头部字段来支持。HTTP/2并没有更改HTTP的语法，而是通过客户端和服务端传输的数据格式，通过新的二进制帧层控制整个过程。 2、HTTP1.x对比HTTP/2，主要区别有哪些？HTTP/2新的二级制格式而非文本格式HTTP/2是完全多路复用，只需要一个TCP连接就可以实现并行，而非有序并阻塞的使用header压缩，HTTP/2降低了请求成本服务端推送，服务端可以主动把响应主动push到客户端3、HTTP/2新特性总结头部压缩每个HTTP传输都包括了请求的一系列资源和描述以及属性的标题。在HTTP/2之前是使用纯文本的形式发送，每次的开销要增加500-800字节，如果使用了HTTP Cookie会增加上千字节，在HTTP/2中，使用了HPACK压缩Header以及响应数据。压缩元数据：它允许通过静态霍夫曼编码对传输的头部字段进行编码，从而减少它们各自的传输大小。它要求客户端和服务器都维护和更新先前看到的标题字段的索引列表（即，建立共享压缩上下文），然后将其用作参考以高效编码先前传输的值。示意图如下图 二进制帧层HTTP1.x是基于解析文本，基于文本的协议格式存在天然的缺陷，表现的形式比较多样化，要做到很健全的场景很难，但是二进制就不同。HTTP/2性能提升的核心在于二进制的帧层，它是指HTTP的消息怎么在客户端以及服务端如何进行封装及传输。它是接口传输过程中的一种更加轻便、更改的编码机制，新的API提供给我们了应用。HTTP1.x采用的是换行分隔符的形式，HTTP/2是采用的分割消息以及frame，然后每个消息以及frame进行编码。客户端和服务端进行统一规则，统一的编码解码，从而一直的处理这些消息的发送及接收，要注意的的是HTTP1.x和HTTP/2是不同通信的。请求和响应多路复用连接共享，在HTTP1.x，用户想要多个请求并行必须使用多个TCP连接，而且在一个时间点只能有一个请求发送出去，使得队头阻塞、低效，只能一个过程完成才能发送下一个。在HTTP/2中，新的帧层接触了对头阻塞限制使得所有请求和响应多路复用，允许客户端和服务端把消息分解成独立的帧，交错传输，然后在另个一个端组装。过程中，每一个请求都做连接机制，一个请求对应一个id，这样一个TCP连接上可以有多个请求，这些请求可以混杂在一起，接收方根据id进行分类归属到不同的服务端的请求中。服务端推送HTTP/2的另一个强大的新功能是服务器为单个客户端请求发送多个响应的能力。也就是说，除了对原始请求的响应之外，服务器还可以向客户端推送额外的资源，而不需要客户端明确请求每一个资源！ HTTP / 2脱离了严格的请求 - 响应语义，并支持一对多和服务器启动的推送工作流程，在浏览器内部和外部打开全新的交互可能性。这是一个启动功能，对于我们如何考虑协议以及在何处以及如何使用协议，都会产生重要的长期影响。服务端推送的优势 推送的资源可以在不同的页面上使用推送的资源可以复用，和其他资源一起使用推送的资源根据用户的意愿，可以拒绝推送的资源可以在客户端缓存推送的资源服务端优先减少重复请求，极大提升响应速度4、多路复用和长连接复用的区别？HTTP1.x 请求&lt;==&gt;响应，每次都要建立一个连接，过程完成自动关闭，比较浪费资源。HTTP/2多个请求可以在一个连接上完成，每个请求都有一个id作为唯一的标识，其中某个请求比较耗时的情况下，不会影响后续请求过程。HTTP1.1若干个请求是串行的单线程处理，后面的请求需要等待前面的请求完成以后才可以继续执行，一旦其中被阻塞，那么就会阻塞后面的请求。","categories":[],"tags":[]},{"title":"Python中的比较运算符和逻辑运算符","slug":"My-blog-10","date":"2016-03-11T04:28:48.000Z","updated":"2019-10-22T08:12:08.432Z","comments":true,"path":"2016/03/11/My-blog-10/","link":"","permalink":"http://yoursite.com/2016/03/11/My-blog-10/","excerpt":"","text":"1 比较(即关系)运算符python中的比较运算符如下表 运算符 描述 示例 == 检查两个操作数的值是否相等，如果是则条件变为真。 如a=3,b=3则（a == b) 为 True. != 检查两个操作数的值是否相等，如果值不相等，则条件变为真。 如a=1,b=3则(a != b) 为 True. &lt;&gt; 检查两个操作数的值是否相等，如果值不相等，则条件变为真。 如a=1,b=3则(a &lt;&gt; b) 为 True。这个类似于 != 运算符 检查左操作数的值是否大于右操作数的值，如果是，则条件成立。 如a=7,b=3则(a &gt; b) 为 True. &lt; 检查左操作数的值是否小于右操作数的值，如果是，则条件成立。 如a=7,b=3则(a &lt; b) 为 False. = 检查左操作数的值是否大于或等于右操作数的值，如果是，则条件成立。 如a=3,b=3则(a &gt;= b) 为 True. &lt;= 检查左操作数的值是否小于或等于右操作数的值，如果是，则条件成立。 如a=3,b=3则(a &lt;= b) 为 True. 2 逻辑运算符运算符 逻辑表达式 描述 实例 and x and y 布尔”与” - 如果 x 为False，x and y 返回 False，否则它返回 y 的计算值。 (aand b) 返回 20。 or x or y 布尔”或” - 如果 x 是 True，它返回 True，否则它返回 y 的计算值。 (a or b) 返回 10。 not notx 布尔”非” - 如果 x为 True，返回 False 。如果 x 为 False，它返回 True。 not(a and b) 返回 False 完工吃饭去了","categories":[],"tags":[]},{"title":"匿名函数","slug":"My-blog-09","date":"2016-02-26T03:16:12.000Z","updated":"2019-10-22T08:09:49.277Z","comments":true,"path":"2016/02/26/My-blog-09/","link":"","permalink":"http://yoursite.com/2016/02/26/My-blog-09/","excerpt":"","text":"匿名函数没有函数名字的函数 单独的匿名函数是无法运行的 可以把匿名函数赋值给变量 通过表达式自我执行，语法：（匿名函数）（） 匿名函数传递参数，语法：（匿名函数）（参数） 单独的匿名函数 function(){ console.log(111); }//单独匿名函数不能运行,控制台无打印内容1234匿名函数赋值给变量 var test=function(){ console.log(111); }test();//变量运行 控制台打印内容111123456789函数自我运行 (function(){ console.log(111); })() //函数自己运行 控制台打印内容11112345678匿名函数传递参数 //第一种情况var a=222;(function(i){ console.log(i);})(a)控制台打印内容222 //第二种情况var a=222;var test=function(i){ console.log(i);}(a)","categories":[],"tags":[]},{"title":"python之selenium三种等待方法","slug":"My-blog-08","date":"2016-02-02T12:06:56.000Z","updated":"2019-10-22T08:09:34.835Z","comments":true,"path":"2016/02/02/My-blog-08/","link":"","permalink":"http://yoursite.com/2016/02/02/My-blog-08/","excerpt":"","text":"python之selenium三种等待方法前提： 我们在做Web自动化时，有的时候要等待元素加载出来，才能操作，不然会报错 1.强制等待 2.隐式等待 3.显示等待 内容： 一，强制等待 这个比较简单，就是利用time模块的sleep的方法来实现 例子： 123456789101112 1 # coding = utf-8 2 from time import sleep 3 from selenium import webdriver 4 # 启动浏览器 5 driver = webdriver.Chrome() 6 # 打开百度首页 7 driver.get(r&apos;https://www.baidu.com/&apos;) 8 # 等待3秒 9 sleep(3)10 driver.find_element_by_css_selector(&quot;#kw&quot;).send_keys(&quot;selenium&quot;)11 # 退出12 driver.quit() 这个就是强制等待3秒，不管你怎么样，就是给我停3秒，强制性的 二，隐式等待 隐式等待就是浏览器会在内部执行等待，但是我们可能看不出来他等待了 例子： 123456789101112131415# coding = utf-8from selenium import webdriver# 启动浏览器driver = webdriver.Chrome()# 打开百度首页driver.get(r&apos;https://www.baidu.com/&apos;)driver.find_element_by_css_selector(&quot;#kw&quot;).send_keys(&quot;selenium&quot;)driver.find_element_by_css_selector(&quot;#su&quot;).click()# 隐式等待30秒driver.implicitly_wait(30)result = driver.find_elements_by_css_selector(&quot;h3.t&gt;a&quot;)for i in result: print(i.text)# 退出driver.quit() 三，显示等待 因为隐式等待有缺陷，比如你想要元素加载出来就执行下面的事情，怎么办，这个要用显示等待了 显示等待要用到WebDriverWait，配合该类的until()和until_not()方法，就能够根据判断条件而进行灵活地等待了。它主要的意思就是：程序每隔xx检查一次，如果条件成立了，则执行下一步，否则继续等待，直到超过设置的最长时间，然后抛出TimeoutException 例子： 1234567891011121314# coding = utf-8from selenium import webdriverfrom selenium.webdriver.support.wait import WebDriverWait# 启动浏览器driver = webdriver.Chrome()# 打开百度首页driver.get(r&apos;https://www.baidu.com/&apos;)driver.find_element_by_css_selector(&quot;#kw&quot;).send_keys(&quot;selenium&quot;)driver.find_element_by_css_selector(&quot;#su&quot;).click()# 超时时间为30秒，每0.2秒检查1次，直到class=&quot;tt&quot;的元素出现text = WebDriverWait(driver,30,0.2).until(lambda x:x.find_element_by_css_selector(&quot;.tt&quot;)).textprint(text)# 退出driver.quit()","categories":[],"tags":[]},{"title":"Python的循环引用解决方案","slug":"My-blog-07","date":"2016-01-18T14:47:45.000Z","updated":"2019-09-19T00:40:35.167Z","comments":true,"path":"2016/01/18/My-blog-07/","link":"","permalink":"http://yoursite.com/2016/01/18/My-blog-07/","excerpt":"","text":"最近项目使用django重构，由于使用了代理类继承models中的类，为了可以使用代理类中的方法，models中的类的外键又是指向代理类的，毫无疑问这样发生了循环引用。 方案一（临时解决方案---不推荐）：在代理类以及models中建立外键所需要引用时，from...import...全部采用局部引用。 此方案要写很多很多重复的from...import...显然不够优雅 方案二（ 我们最终选择的方案---推荐）：models中的外键所引用的类用引号括起来，这样在第一次加载的时候就算此类没有被加载也没有关系，只要你真正调用之前加载了这个类就不会出错，然后将这个文件中的所有的加载其他类的from...import语句放到文件的最下面。这样就是先加载上面全部的类，然后再去加载代理类。代理类中的from...import...也不需要局部引用，写在文件头部就没问题了。 方案三（改动太大，放弃）：将“代理类继承model”的模式改成“model继承外部类”，改变继承方向。我觉得用一个写满属性的类去继承一个写满方法的类在设计上就怪怪的，也没有进行验证是否可行，遂放弃。方案四（实验未成功）：网上有的解决方案是使用import代替from，实验没成功 总结： 1.如果有能力就在架构设计之初尽量避免循环引用问题。特别是使用了继承、工厂、代理类的话要考虑清楚 2.user = models.ForeignKey(User)是可以写成user = models.ForeignKey(‘User’)，这样在加载时可以忽略引用顺序问题，但是编译器就没办法提醒你这个类到底有没有引用，不过是小问题。 3.model中只要外键用的是字符串，from…import…是可以放在文件末尾的，不过编译器会提醒你from语句应该放在文件头，小问题。 注：最近加班加成了沙雕，晚些时候测试一下方案三和方案四，并把示例代码贴上来","categories":[],"tags":[]},{"title":"Python的内存管理(毛毛随笔)","slug":"My-blog-05","date":"2015-12-23T07:24:02.000Z","updated":"2019-10-22T08:00:33.508Z","comments":true,"path":"2015/12/23/My-blog-05/","link":"","permalink":"http://yoursite.com/2015/12/23/My-blog-05/","excerpt":"","text":"Python对内存的管理从三个方面来说：1.对象的引用计数机制、2.垃圾回收机制、 3.内存池机制 一 对象的引用机制Pyhton的内部使用引用计数，来保持内存中的对象，所有对象都有引用计数。 (1) 引用计数增加： 一个对象分配一个新名称 将其放入一个容器中(列表、元素或字典) (2) 引用减少的情况 使用del语句将对象的别名显式的销毁 引用被重新赋值获取应用对象：通过sys.getrefcount( )函数获取某个引用的引用数，函数参数实际上创建了一个临时的引用。因此，getrefcount( )所得到的结果，会比期望多1。 二.垃圾回收 当一个对象的引用计数归零时，它将被垃圾回收机制处理掉 python的自动垃圾回收：当分配对象的次数和取消分配对象的次数的差值高于某个阈值时，垃圾回收才会启动。 分代回收 python分代回收基本策略：存活时间越久的对象，越不可能在后面的程序中变成垃圾。(在垃圾回收的过程中，减少“长寿”对象的扫描频率)。\\1. Python将所有的对象分为0, 1, 2三代。所有新建的对象都是0代对象。2. 当某一代对象经历过垃圾回收，依然存活，那么它就被归入下一代对象。\\3. 每次垃圾回收启动时，一定会扫描所有0代对象。如果0代对象经过一定次数 垃圾回收，那么就会启动0代和1代的扫描清理。当1代经历一定次数的垃圾回收后，那么会启动对0，1，2，即对所有对象进行扫描。\\4. 对于函数set_threshold()，返回(700, 10, 10)，700为分配对象和取消分配对象的差值，当差值大于700时，启动垃圾回收；每10次0代垃圾回收，会配合1次1代的垃圾回收；每10次1代垃圾回收，会配合1次2代的垃圾回收。 三. 内存池机制Python的内存垃圾回收机制，将不用的内存放到内存池而不是返回给操作系统。\\1. Pymalloc机制。为了加速Python的执行效率，Python引入内存池机制，用于管理对小块内存的管理和释放。\\2. 对于所有小于256个字节的对象都使用pymalloc实现的分配器；而大于这个长度的对象则使用系统的malloc。\\3. 对于Python对象，如整数、浮点数和List，都有其独立的私有内存池，对象间不共享他们的内存池。如果你分配又释放了大量的整数，用于缓存这些整数的内存不能再分配给浮点数。 好了 ,我要去准备圣诞礼物了 哈哈","categories":[],"tags":[]},{"title":"Python列表和元组的详细区别","slug":"My-blog-04","date":"2015-12-08T14:14:31.000Z","updated":"2019-09-12T02:57:43.293Z","comments":true,"path":"2015/12/08/My-blog-04/","link":"","permalink":"http://yoursite.com/2015/12/08/My-blog-04/","excerpt":"","text":"一、列表和元组的区别列表是动态数组，它们不可变且可以重设长度（改变其内部元素的个数）。元组是静态数组，它们不可变，且其内部数据一旦创建便无法改变。元组缓存于Python运行时环境，这意味着我们每次使用元组时无须访问内核去分配内存。这些区别结实率两者在设计哲学上的不同： 列表可被用于保存多个互相独立对象的数据集合元组用于描述一个不会改不安的事务的多个属性二、列表（List）列表一旦创建了我们就可以根据自己的需要随意改变它的内容： 12345&gt;&gt;&gt; L = [1, 2, 3, 4, 5, 6]&gt;&gt;&gt; L[0] = l[2] * l[3]&gt;&gt;&gt; L[12, 2, 3, 4, 5, 6] 另外我们可以给列边添加新的数据来增加其大小： 1234567&gt;&gt;&gt; len(l)6&gt;&gt;&gt; l.append(7)&gt;&gt;&gt; l[12, 2, 3, 4, 5, 6, 7]&gt;&gt;&gt; len(l)7 这是因为动态数组支持resize操作，可以增加数组的容量。当一个大小为N的列表第一次需要添加数据时，Python会创建一个新的列表，足够放原来的N个元素以及额外添加的元素。不过，实际分配的并不是N+1个元素，而是M个元素，M &gt; N, 这是为了给未来的添加预留空间。然后旧列表的数据被复制到新列表中，旧列表则会被销毁。从设计理念上来说，第一次添加可能是后续多次添加的开始，通过预留空间的做法，我们就可以减少这一分配空间的操作次数以及内存复制的次数。这点非常重要，因为内存复制可能非常的昂贵，特别是当列表大小开始增长以后。 三、元组（Tuple）元组是固定且不可改变的。这意味着一旦元组被创建，和列表不同，它的内容无法被修改或它的大小也无法被改变。 12345&gt;&gt;&gt; t = (1, 2, 3, 4)&gt;&gt;&gt; t[0] = 5Traceback (most recent call last): File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;TypeError: &apos;tuple&apos; object does not support item assignment 虽然它们不支持改变大小，但是我们可以将两个元组合并成一个新元组。这一操作类似列表的resize操作，但我们不需要为新生的元组分配任何额外的空间： 1234&gt;&gt;&gt; t1 = (1, 2, 3, 4)&gt;&gt;&gt; t2 = (5, 6, 7, 8)&gt;&gt;&gt; t1 + t2(1, 2, 3, 4, 5, 6, 7, 8) 如果我们将其与列表的append操作比较，我们会看到它的复杂度是O(n)而不是列表的O(1)。这是因为对元组每添加一个新元素都会有分配和复制操作，而不是像列表那样仅在额外的空间耗尽时发生。所以元组并没有提供一个类似append的自增操作，任意两个元组相加始终返回一个新分配的元组。 该网站为Python Tutor,一个能够对python运行内存可视化的网站，非常适合初学者研究，在运行过程中内存发生了什么。 元组的静态特性的另一个好处体现在一些会在Python后台发生的事情：资源缓存。 Python是一门垃圾收集语言，这意味着当一个变量不再被使用时，Python会将该变量使用的内存释放回操作系统，以供其他程序（变量）使用。然而，对于长度为1~20的元组，即使它们不在被使用，它们的空间也不会立刻还给系统，而是留待未来使用。这意味着当未来需要一个同样大小的新的元组时，我们不再需要向操作系统申请一块内存来存放数据，因为我们已经有了预留的空间。 这看上去可能是个细微的好处，但是实际上是元组一个很神奇的地方：它们可以被轻松快速地创建，因为它们可以避免跟操作系统频繁的打交道，而后者会花很长的时间。 下面举个例子会非常直观的说明问题: 12345In [1]: %timeit l = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]93.7 ns ± 3.33 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)In [2]: %timeit t = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)18.5 ns ± 1.19 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each) 上面的示例中显示了初始化一个列表比初始化一个元组慢了5.1倍——如果这是在一个循环中，这点差别会很快的累加起来。","categories":[],"tags":[]},{"title":"RabbitMQ底层原理分析","slug":"My-blog-03","date":"2015-11-19T00:39:42.000Z","updated":"2019-09-11T11:47:02.882Z","comments":true,"path":"2015/11/19/My-blog-03/","link":"","permalink":"http://yoursite.com/2015/11/19/My-blog-03/","excerpt":"","text":"RabbitMq消息中间件介绍&amp;为什么要使用消息中间件&amp;什么时候使用消息中间件我们用java来举例子， 打个比方 我们客户端发送一个下单请求给订单系统（order）订单系统发送了 一个请求给我们的库存系统告诉他需要更改库存了， 我已经下单了， 这里， 每一个请求我们都可以看作一条消息， 但是 我们客户端需要等待订单系统告诉我这条消息的处理结果（我到底有没有下单成功） 但是 订单系统不需要知道库存系统这条消息的处理情况 因为无论你库存有没有改动成功， 我订单还是下了， 因为是先下完了订单（下成功了） 才去更改库存， 库存如果更改出BUG了 那是库存系统的问题， 这个BUG不会影响订单系统。如果这里你能理解的话， 那么我们就能发现 我们用户发送的这条消息（下订单）， 是需要同步的（我需要知道结果）， 订单发送给库存的消息，是可以异步的（我不想知道你库存到底改了没， 我只是通知你我这边成功下了一个订单） 那么如果我们还按原来的方式去实现这个需求的话， 那么结果会是这样： 那可能有同学说了， 我们订单系统开辟线程去访问库存系统不就好了吗？ ## 使用线程池解决 确实可以， 但是也有他的缺点， 那么 到底怎么来完美解决这个问题呢？ 如果这张图能理解的话， 那么 这个消息系统， 就是我们的消息中间件。 RabbitMq介绍&amp;AMQP介绍导语:我们刚刚介绍了什么是消息中间件， 那么RabbitMq就是对于消息中间件的一种实现，市面上还有很多很多实现， 比如RabbitMq、ActiveMq、ZeroMq、kafka，以及阿里开源的RocketMQ等等 我们这节主要讲RabbitMq AMQP 这里引用百度的一句话 再加以我的理解： AMQP 其实和Http一样 都是一种协议， 只不过 Http是针对网络传输的， 而AMQP是基于消息队列的 AMQP 协议中的基本概念： ​ • Broker: 接收和分发消息的应用，我们在介绍消息中间件的时候所说的消息系统就是Message Broker。 ​ • Virtual host: 出于多租户和安全因素设计的，把AMQP的基本组件划分到一个虚拟的分组中，类似于网络中的namespace概念。当多个不同的用户使用同一个RabbitMQ server提供的服务时，可以划分出多个vhost，每个用户在自己的vhost创建exchange／queue等。 ​ • Connection: publisher／consumer和broker之间的TCP连接。断开连接的操作只会在client端进行，Broker不会断开连接，除非出现网络故障或broker服务出现问题。 ​ • Channel: 如果每一次访问RabbitMQ都建立一个Connection，在消息量大的时候建立TCP Connection的开销将是巨大的，效率也较低。Channel是在connection内部建立的逻辑连接，如果应用程序支持多线程，通常每个thread创建单独的channel进行通讯，AMQP method包含了channel id帮助客户端和message broker识别channel，所以channel之间是完全隔离的。Channel作为轻量级的Connection极大减少了操作系统建立TCP connection的开销。 ​ • Exchange: message到达broker的第一站，根据分发规则，匹配查询表中的routing key，分发消息到queue中去。常用的类型有：direct (point-to-point), topic (publish-subscribe) and fanout (multicast)。 ​ • Queue: 消息最终被送到这里等待consumer取走。一个message可以被同时拷贝到多个queue中。 ​ • Binding: exchange和queue之间的虚拟连接，binding中可以包含routing key。Binding信息被保存到exchange中的查询表中，用于message的分发依据。 Exchange的类型:direct : 这种类型的交换机的路由规则是根据一个routingKey的标识，交换机通过一个routingKey与队列绑定 ，在生产者生产消息的时候 指定一个routingKey 当绑定的队列的routingKey 与生产者发送的一样 那么交换机会吧这个消息发送给对应的队列。 fanout: 这种类型的交换机路由规则很简单，只要与他绑定了的队列， 他就会吧消息发送给对应队列（与routingKey没关系） topic:(因为*在这个笔记软件里面是关键字，所以下面就用星替换掉了)这种类型的交换机路由规则也是和routingKey有关 只不过 topic他可以根据:星,#（ 星号代表过滤一单词，#代表过滤后面所有单词， 用.隔开）来识别routingKey 我打个比方 假设 我绑定的routingKey 有队列A和B A的routingKey是：星.user B的routingKey是: #.user 那么我生产一条消息routingKey 为： error.user 那么此时 2个队列都能接受到， 如果改为 topic.error.user 那么这时候 只有B能接受到了 headers:这个类型的交换机很少用到，他的路由规则 与routingKey无关 而是通过判断header参数来识别的， 基本上没有应用场景，因为上面的三种类型已经能应付了。RabbitMQ MQ： message Queue 顾名思义 消息队列， 队列大家都知道， 存放内容的一个东西， 存放的内容先进先出， 消息队列， 只是里面存放的内容是消息而已。 RabbitMq 是一个开源的 基于AMQP协议实现的一个完整的企业级消息中间件，服务端语言由Erlang（面向并发编程）语言编写 对于高并发的处理有着天然的优势，客户端支持非常多的语言： • Python • Java • Ruby • PHP • C# • JavaScript • Go • Elixir • Objective-C • Swift RabbitMQ服务端部署在介绍消息中间件的时候所提到的“消息系统” 便是我们这节的主题：RabbitMq 如同redis一样 他也是采用c/s架构 由服务端 与客户端组成， 我们现在我们计算机上部署他的服务端 由于我们刚刚介绍过了RabbitMQ服务端是由Erlang语言编写所以我们这里先下载Erlang语言的环境 注意：如果是在官网下的RabbitMQ服务端的话 Erlang语言的版本不能太低， 不然要卸载掉旧的去装新的， 我们这里下载OTP21.0版本直接从外网下载会很慢， 我这里直接贴上百度网盘的地址(因为这个东西还是有点大的) https://pan.baidu.com/s/1pZJ8l2f3omrgnuCm9a8DVA 我们再去官网下载 他的服务端安装包 http://www.rabbitmq.com/download.html 根据自己的系统选择下载即可 注意！ 需要先下载Erlang再下载安装包安装， 不然安装RabbitMQ服务端的时候会提示你本地没有Erlang环境 安装的话， 基本上就是默认的选项不用改 如何看RabbitMq安装完成了？ 在系统-服务中找到如下即可： 包括启动 停止 重启 服务等 RabbitMQ安装会附带一个管理工具（方便我们能直观的查看整个RabbitMQ的运行状态和详细数据等，有点像Navicat 对应Mysql的关系） 值得一提的是， 管理工具和RabbitMQ是两码事 希望同学们不要混稀了。 管理工具启动方式： 到你们安装的 RabbitMQ Server\\rabbitmq_server-3.7.12\\sbin 目录下面 执行一条cmd命令： rabbitmq-plugins enable rabbitmq_management 直接复制这条命令即可 ， 当然 嫌每次都要去目录中去执行的麻烦的话， 可以配置一个环境变量 或者在我们的开始菜单栏中找到这个： 输入完启动命令后 稍微等一下会有结果返回 然后可以打开浏览器 输入 http://127.0.0.1:15672 访问管理页面： 默认账号密码都是 guest 即 username ：guest password：guest 登录进去之后会看到如下界面（因为我不小心装了2次RabbitMq 所以这里能看到都重复了， 你们自己那不会重复，然后我们刚刚说了 管理工具和rabbitmq 是两码事 所以端口也就不一样） 这个页面在笔记里面介绍起来可能比较复杂， 就不一一介绍了， 我这里讲个重点， 就是线上环境下一定要吧guest用户（当然 guest这个用户只能本机才能登陆）删掉并且新加一个用户， 这里就演示一下这个功能 首先 点击admin页签， 在下面找到Add User 然后输入账号 密码 确认密码 这个Tags其实是一个用户权限标签， 关于他的介绍可以看官方介绍（点旁边那个小问号就好了，我这里直接翻译他的介绍） 填写完之后点击AddUser 就可以添加一个用户了， 添加完用户之后还要给这个用户添加对应的权限（注：Targ不等于权限） 比如说 我刚刚添加了一个jojo角色 点击这个jojo可以进去给他添加权限 这个权限可以是 Virtual host 级别的 也可以是交换机级别的 甚至是细化到某一个读写操作 我这里就给他添加一个Virtual host权限 这里 我们给了他 testhost这个Virtual host的权限 正则匹配都是* 也就是所有权限 然后点击set添加完毕 那么管理页面 我们就讲到这里 RabbitMq快速开始因为我们这里是用java来作为客户端， 我们首先引入maven依赖： com.rabbitmq amqp-client 5.1.2 （注意的是， 我这里引入的是5.x的rabbitmq客户端版本， 那么我们jdk的版本最好在8以上，反之， 这里就建议使用4.x的版本，这里仅仅讨论jdk8 其他的版本不做讨论） 首先 我们编写一个连接的工具类： packagecom.luban.util; importcom.rabbitmq.client.Connection; importcom.rabbitmq.client.ConnectionFactory; importjava.util.ArrayList; importjava.util.HashMap; importjava.util.List; importjava.util.Map; /** * 需要咨询java高级VIP课程的同学可以木兰老师的QQ：2746251334 * 需要往期视频的同学可以加加安其拉老师的QQ：3164703201 *author：鲁班学院-商鞅老师 */ public classConnectionUtil { public static final StringQUEUE_NAME = “testQueue”; public static final StringEXCHANGE_NAME = “exchange”; public static Connection getConnection() throws Exception{ //创建一个连接工厂 ConnectionFactory connectionFactory = new ConnectionFactory(); //设置rabbitmq 服务端所在地址 我这里在本地就是本地 connectionFactory.setHost(“127.0.0.1”); //设置端口号，连接用户名，虚拟地址等 connectionFactory.setPort(5672); connectionFactory.setUsername(“jojo”); connectionFactory.setPassword(“jojo”); connectionFactory.setVirtualHost(“testhost”); returnconnectionFactory.newConnection(); } } 然后我们编写一个消费者（producer），和一个生产者（consumer）： 生产者： public classConsumer { public static void sendByExchange(Stringmessage) throws Exception{ Connectionconnection = ConnectionUtil.getConnection(); Channelchannel = connection.createChannel(); //声明队列 channel.queueDeclare(ConnectionUtil.QUEUE_NAME,true,false,false,null); // 声明exchange channel.exchangeDeclare(ConnectionUtil.EXCHANGE_NAME, “fanout”); //交换机和队列绑定 channel.queueBind(ConnectionUtil.QUEUE_NAME, ConnectionUtil.EXCHANGE_NAME, “”); channel.basicPublish(ConnectionUtil.EXCHANGE_NAME, “”, null, message.getBytes()); System.out.println(“发送的信息为:”+ message); channel.close(); connection.close(); } } 消费者： public classProducer { public static void getMessage() throws Exception{ Connectionconnection = ConnectionUtil.getConnection(); Channelchannel = connection.createChannel(); // channel.queueDeclare(ConnectionUtil.QUEUE_NAME,true,false,false,null); DefaultConsumer deliverCallback = new DefaultConsumer(channel) { @Override public void handleDelivery(StringconsumerTag, Envelope envelope, AMQP.BasicPropertiesproperties, byte[] body) throws IOException{ System.out.println(new String(body, “UTF-8”)); } }; channel.basicConsume(ConnectionUtil.QUEUE_NAME, deliverCallback); } } 这里， 我们演示绑定fanout的类型的交换机， 所以不需要routingKey 就可以路由只需要绑定即可 （可能有同学要问了， 如果没有绑定交换机怎么办呢？ 没有绑定交换机的话， 消息会发给rabbitmq默认的交换机里面 默认的交换机隐式的绑定了所有的队列，默认的交换机类型是direct 路由建就是队列的名字） 基本上这样子的话就已经进行一个快速入门了， 由于我们现在做项目基本上都是用spring boot（就算没用spring boot也用spring 吧） 所以后面我们直接基于spring boot来讲解（rabbitmq的特性，实战等）","categories":[],"tags":[]},{"title":"Redis和Memcached的区别","slug":"My-blog-02","date":"2015-11-03T03:49:13.000Z","updated":"2019-09-11T11:43:59.099Z","comments":true,"path":"2015/11/03/My-blog-02/","link":"","permalink":"http://yoursite.com/2015/11/03/My-blog-02/","excerpt":"","text":"Redis Memcached 网络IO模型 单线程的IO复用的网络模型 多线程的非阻塞IO复用的网络模型 数据支持类型 key-value数据类型 , 还支持list,set,zset,hash等数据结构的存储 还支持list,set,zset,hash等数据结构的存储 内存管理机制 基于zmalloc.h和zmalloc.c两个文件对mallc/free包装来实现管理内存 , 使用现场申请内存的方式 基于Slab Allocation机制管理内存 , 使用预分配的内存池的方式 数据存储及持久化 支持存储 除以in-memory的形式存储另外两种：快照（snapshotting)，只追加文件（append-only file， AOF) 不支持存储 只以in-memory的形式存储 数据一致性 提供了事务的功能，保证命令的原子性，中间不会被任何操作打断 提供了cas命令，保证多个并发访问操作同一份数据的一致性 集群管理 服务器端构建分布式存储 只能采用客户端实现分布式存储 性能 单核 数据量&lt;100k 高 数据量&gt;100k 低 多核 数据量&lt;100k 低 数据量&gt;100k 高 内存使用效率 采用key-value 存储结构 利用率低 采用hash结构来做key-value存储(组合式的压缩方式) 利用率高 采用key-value 存储结构 利用率高 Redis和Memcached这种基于内存的数据库系统来说，内存管理的效率高低是影响系统性能的关键因素。Redis和Memcached最大存储量是根据机器内存大小而定。Redis 是一种键值数据库，处于关系数据库和键值数据库之间。 Redis采用的是包装的mallc/free方式来实现内存管理 Slab Allocation机制的原理：它首先申请一大块内存，并将其分割成各种尺寸的块Chunk，并把尺寸相同的块分成组Slab Class。其中，Chunk就是用来存储key-value数据的最小单位。每个Slab Class的大小，可以在Memcached启动的时候通过制定Growth Factor来控制。假定图中Growth Factor的取值为1.25，如果第一组Chunk的大小为88个字节，第二组Chunk的大小就为112个字节，依此类推。 Memcached使用预分配的内存池的方式，使用slab和大小不同的chunk来管理内存，Item根据大小选择合适的chunk存储，内存池的方式可以省去申请/释放内存的开销，并且能减小内存碎片产生，但这种方式也会带来一定程度上的空间浪费，并且在内存仍然有很大空间时，新的数据也可能会被剔除。 Redis使用现场申请内存的方式来存储数据，并且很少使用free-list等方式来优化内存分配，会在一定程度上存在内存碎片，Redis跟据存储命令参数，会把带过期时间的数据单独存放在一起，并把它们称为临时数据，非临时数据是永远不会被剔除的，即便物理内存不够，导致swap也不会剔除任何非临时数据（但会尝试剔除部分临时数据），这点上Redis更适合作为存储而不是cache。 redis提供的两种不同的持久化方法来存储数据到硬盘里面：①快照（snapshotting)，它可以将存在于某一时刻的所有数据都写入硬盘里面。②只追加文件（append-only file， AOF),它会在执行写命令时，将被执行的写命令复制到硬盘里面。","categories":[],"tags":[]},{"title":"面向对象的四大基本特征和五大基本原则","slug":"My-blog-01","date":"2015-10-28T07:06:09.000Z","updated":"2019-09-11T07:11:50.804Z","comments":true,"path":"2015/10/28/My-blog-01/","link":"","permalink":"http://yoursite.com/2015/10/28/My-blog-01/","excerpt":"","text":"面向对象主要有四大特性 · 抽象 o 忽略一个主题中与当前目标无关的东西,专注的注意与当前目标有关的方面.(就是把现实世界中的某一类东西,提取出来,用程序代码表示,抽象出来的一般叫做类或者接口).抽象并不打算了解全部问题,而是选择其中的一部分,暂时不用部分细节.抽象包括两个方面,一个数据抽象,而是过程抽象. o 数据抽象 –&gt;表示世界中一类事物的特征,就是对象的属性.比如鸟有翅膀,羽毛等(类的属性) o 过程抽象 –&gt;表示世界中一类事物的行为,就是对象的行为.比如鸟会飞,会叫(类的方法) · 封装 o 封装就是把过程和数据包围起来,对数据的访问只能通过特定的界面.如私有变量,用set,get方法获取 · 继承 o 一种联结类的层次模型,并且允许和鼓励类的重用,提供一种明确表达共性的方法.对象的一个新类可以从现有的类中派生,这个过程称为类继承.新类继承了原始类的特性,新类称为原始类的派生类(子类),原始类称为新类的基类(父类).派生类可以从它的父类哪里继承方法和实例变量,并且类可以修改或增加新的方法使之更适合特殊的需要.因此可以说,继承为了重用父类代码,同时为实现多态性作准备. · 多态 o 多态是指允许不同类的对象对同一消息做出响应.多态性包括参数化多态性和包含多态性.多态性语言具有灵活/抽象/行为共享/代码共享的优势,很好的解决了应用程序函数同名问题.总的来说,方法的重写,重载与动态链接构成多态性.java引入多态的概念原因之一就是弥补类的单继承带来的功能不足. o 动态链接 –&gt;对于父类中定义的方法,如果子类中重写了该方法,那么父类类型的引用将调用子类中的这个方法,这就是动态链接. 注意继承与重载:子类与父类的关系,重载方法的调用问题 子类对象可以直接当成父类对象用,但是反过来就不行.比如:人是父类,学生是人的子类,所以学生对象一定具备人对象的属性,但是人对象就未必具有学生对象的特性.但是子类对象当做父类来用,将失去子类所有特性,只保留与父类同名的属性和方法,此时可以对父类方法进行重写. 一个类中如果定义了重载的方法,则系统在调用方法时,会根据参数的类型自动选择调用合适的方法 五大基本原则 · 单一职责原则SRP(Single Responsibility Principle) o 是指一个类的功能要单一，不能包罗万象。如同一个人一样，分配的工作不能太多，否则一天到晚虽然忙忙碌碌的，但效率却高不起来。 · 开放封闭原则OCP(Open－Close Principle) o 一个模块在扩展性方面应该是开放的而在更改性方面应该是封闭的。比如：一个网络模块，原来只服务端功能，而现在要加入客户端功能， o 那么应当在不用修改服务端功能代码的前提下，就能够增加客户端功能的实现代码，这要求在设计之初，就应当将服务端和客户端分开，公共部分抽象出来。 · 替换原则(the Liskov Substitution Principle LSP) o 子类应当可以替换父类并出现在父类能够出现的任何地方。比如：公司搞年度晚会，所有员工可以参加抽奖，那么不管是老员工还是新员工， o 也不管是总部员工还是外派员工，都应当可以参加抽奖，否则这公司就不和谐了。 · 依赖原则(the Dependency Inversion Principle DIP) o 具体依赖抽象，上层依赖下层。假设B是较A低的模块，但B需要使用到A的功能， o 这个时候，B不应当直接使用A中的具体类：而应当由B定义一抽象接口，并由A来实现这个抽象接口，B只使用这个抽象接口：这样就达到 o 了依赖倒置的目的，B也解除了对A的依赖，反过来是A依赖于B定义的抽象接口。通过上层模块难以避免依赖下层模块，假如B也直接依赖A的实现，那么就可能造成循环依赖。一个常见的问题就是编译A模块时需要直接包含到B模块的cpp文件，而编译B时同样要直接包含到A的cpp文件。 · 接口分离原则(the Interface Segregation Principle ISP) o 模块间要通过抽象接口隔离开，而不是通过具体的类强耦合起来","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2015-05-02T12:45:31.000Z","updated":"2019-09-11T06:32:00.660Z","comments":true,"path":"2015/05/02/hello-world/","link":"","permalink":"http://yoursite.com/2015/05/02/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}